[
  {
    "objectID": "posts/Workflow/index.html",
    "href": "posts/Workflow/index.html",
    "title": " Facing the Abyss - WorkFLow",
    "section": "",
    "text": "So you have your shiny new R skills and you’ve successfully loaded a cool dataframe into R… Now what?\nThe best charts come from understanding your data, asking good questions from it, and displaying the answers to those questions as clearly as possible."
  },
  {
    "objectID": "posts/Workflow/index.html#a-data-analytics-process",
    "href": "posts/Workflow/index.html#a-data-analytics-process",
    "title": " Facing the Abyss - WorkFLow",
    "section": "",
    "text": "So you have your shiny new R skills and you’ve successfully loaded a cool dataframe into R… Now what?\nThe best charts come from understanding your data, asking good questions from it, and displaying the answers to those questions as clearly as possible."
  },
  {
    "objectID": "posts/Workflow/index.html#setting-up-r-packages",
    "href": "posts/Workflow/index.html#setting-up-r-packages",
    "title": " Facing the Abyss - WorkFLow",
    "section": "{{< iconify noto-v1 package >}} Setting up R Packages",
    "text": "{{&lt; iconify noto-v1 package &gt;}} Setting up R Packages\n\nInstall packages using install.packages() in your Console.\nLoad up your libraries in a setup chunk:\n\n\nknitr::opts_chunk$set(dev = \"ragg_png\")\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(palmerpenguins)\nlibrary(ggformula)\nlibrary(ggridges)\nlibrary(skimr)\n##\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\n\nGo to https://fonts.google.com/ and choose some professional looking, or funky looking, fonts.\n\nlibrary(extrafont)\n\nRegistering fonts with R\n\nextrafont::loadfonts(quiet = TRUE)\n##\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\n\n\nAttaching package: 'showtextdb'\n\n\nThe following object is masked from 'package:extrafont':\n\n    font_install\n\n## Loading Google fonts (https://fonts.google.com/)\nfont_add_google(name = \"Fira Sans Condensed\", family = \"fira\")\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Montserrat Alternates\", \"montserrat\")\nfont_add_google(\"Roboto Condensed\", \"roboto\")\n### Automatically use showtext to render text\nshowtext_auto()"
  },
  {
    "objectID": "posts/Workflow/index.html#read-data",
    "href": "posts/Workflow/index.html#read-data",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Read Data",
    "text": "Read Data\n\nUse readr::read_csv()"
  },
  {
    "objectID": "posts/Workflow/index.html#examine-data",
    "href": "posts/Workflow/index.html#examine-data",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Examine Data",
    "text": "Examine Data\n\nUse dplyr::glimpse()\nUse mosaic::inspect() or skimr::skim()\nUse dplyr::summarise() and crosstable::crosstable()\nFormat your tables with knitr::kable()\nHighlight any interesting summary stats or data imbalances"
  },
  {
    "objectID": "posts/Workflow/index.html#data-dictionary-and-experiment-description",
    "href": "posts/Workflow/index.html#data-dictionary-and-experiment-description",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Data Dictionary and Experiment Description",
    "text": "Data Dictionary and Experiment Description\n\nA table containing the variable names, their interpretation, and their nature(Qual/Quant/Ord…)\nIf there are wrongly coded variables in the original data, state them in their correct form, so you can munge the in the next step\nDeclare what might be target and predictor variables, based on available information of the experiment, or a description of the data"
  },
  {
    "objectID": "posts/Workflow/index.html#data-munging",
    "href": "posts/Workflow/index.html#data-munging",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Data Munging",
    "text": "Data Munging\n\nConvert variables to factors as needed\nReformat / Rename other variables as needed\nClean badly formatted columns (e.g. text + numbers) using tidyr::separate_**_**()\nSave the data as a modified file\nDo not mess up the original data file"
  },
  {
    "objectID": "posts/Workflow/index.html#form-hypotheses",
    "href": "posts/Workflow/index.html#form-hypotheses",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Form Hypotheses",
    "text": "Form Hypotheses\n\nQuestion-1\n\nState the Question or Hypothesis\n(Temporarily) Drop variables using dplyr::select()\nCreate new variables if needed with dplyr::mutate()\nFilter the data set using dplyr::filter()\nReformat data if needed with tidyr::pivot_longer() or tidyr::pivot_wider()\nAnswer the Question with a Table, a Chart, a Test, using an appropriate Model for Statistical Inference\nUse title, subtitle, legend and scales appropriately in your chart\nPrefer ggformula unless you are using a chart that is not yet supported therein (eg. ggbump() or plot_likert())\n\n\n## Set graph theme\n## Idotic that we have to repeat this every chunk\n## Open issue in Quarto\n\npenguins %&gt;% \n  drop_na() %&gt;% \n  gf_point(body_mass_g ~ flipper_length_mm, \n           colour = ~ species) %&gt;% \n  gf_labs(title = \"My First Penguins Plot\",\n          subtitle = \"Using ggformula with fonts\",\n          x = \"Flipper Length mm\", y = \"Body Mass gms\",\n          caption = \"I love penguins, and R\") %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n  gf_theme(theme(\n      panel.grid.minor = element_blank(),\n      ###\n      text = element_text(family = \"fira\", size = 14),\n      ###\n      plot.title = element_text(\n      family = \"roboto\",\n      face = \"bold\",\n      size = 28, hjust = 0\n    ),\n    plot.subtitle = element_text(\n      family = \"montserrat\",\n      face = \"bold\",\n      size = 18, hjust = 0),\n    \n    plot.margin = margin(2,2,2,2, unit = \"pt\"),\n    \n    axis.title = element_text(size = 20),\n    \n    plot.caption = element_text(family = \"gochi\", size = 14),\n    \n    legend.title = element_text(\n      family = \"bell\",\n      face = \"bold\",\n      size = 20\n    ),\n    legend.text = element_text(family = \"fira\",\n                               size = 12),\n    \n    legend.background = element_rect(fill = \"cornsilk\",\n                                     colour = \"black\"),\n    legend.margin = margin(\n      t = 2,\n      r = 2,\n      b = 2,\n      l = 2,\n      unit = \"pt\"\n    )\n    ))\n\n\n\n\n\n\n\n\n\n\nInference-1\n. . . .\n\n\nQuestion-n\n….\n\n\nInference-n\n…."
  },
  {
    "objectID": "posts/Workflow/index.html#one-most-interesting-graph",
    "href": "posts/Workflow/index.html#one-most-interesting-graph",
    "title": " Facing the Abyss - WorkFLow",
    "section": "One Most Interesting Graph",
    "text": "One Most Interesting Graph"
  },
  {
    "objectID": "posts/Workflow/index.html#conclusion",
    "href": "posts/Workflow/index.html#conclusion",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Conclusion",
    "text": "Conclusion\nDescribe what the graph shows and why it so interesting. What could be done next?"
  },
  {
    "objectID": "posts/Workflow/index.html#references",
    "href": "posts/Workflow/index.html#references",
    "title": " Facing the Abyss - WorkFLow",
    "section": "References",
    "text": "References\n\nhttps://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/"
  },
  {
    "objectID": "posts/day-6/index.html",
    "href": "posts/day-6/index.html",
    "title": "Day 6",
    "section": "",
    "text": "library(tidyverse) # Data Processing in R\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic) # Our workhorse for stats, sampling\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr) # Good to Examine data\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula) # Formula interface for graphs\n\n# load the NHANES data library\nlibrary(NHANES)\nlibrary(infer)\n\n\nAttaching package: 'infer'\n\nThe following objects are masked from 'package:mosaic':\n\n    prop_test, t_test\n\n\n\ndata(\"NHANES\")\nglimpse(NHANES)\n\nRows: 10,000\nColumns: 76\n$ ID               &lt;int&gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 5164…\n$ SurveyYr         &lt;fct&gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,…\n$ Gender           &lt;fct&gt; male, male, male, male, female, male, male, female, f…\n$ Age              &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, …\n$ AgeDecade        &lt;fct&gt;  30-39,  30-39,  30-39,  0-9,  40-49,  0-9,  0-9,  40…\n$ AgeMonths        &lt;int&gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795,…\n$ Race1            &lt;fct&gt; White, White, White, Other, White, White, White, Whit…\n$ Race3            &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education        &lt;fct&gt; High School, High School, High School, NA, Some Colle…\n$ MaritalStatus    &lt;fct&gt; Married, Married, Married, NA, LivePartner, NA, NA, M…\n$ HHIncome         &lt;fct&gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, 3…\n$ HHIncomeMid      &lt;int&gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 8750…\n$ Poverty          &lt;dbl&gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00,…\n$ HomeRooms        &lt;int&gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3,…\n$ HomeOwn          &lt;fct&gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, O…\n$ Work             &lt;fct&gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, N…\n$ Weight           &lt;dbl&gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7,…\n$ Length           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HeadCirc         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Height           &lt;dbl&gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.…\n$ BMI              &lt;dbl&gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.2…\n$ BMICatUnder20yrs &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BMI_WHO          &lt;fct&gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plus…\n$ Pulse            &lt;int&gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 8…\n$ BPSysAve         &lt;int&gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111, …\n$ BPDiaAve         &lt;int&gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, 6…\n$ BPSys1           &lt;int&gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124, …\n$ BPDia1           &lt;int&gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, 6…\n$ BPSys2           &lt;int&gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108, …\n$ BPDia2           &lt;int&gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, 6…\n$ BPSys3           &lt;int&gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114, …\n$ BPDia3           &lt;int&gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, 7…\n$ Testosterone     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DirectChol       &lt;dbl&gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, 2…\n$ TotChol          &lt;dbl&gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, 5…\n$ UrineVol1        &lt;int&gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113, …\n$ UrineFlow1       &lt;dbl&gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1.…\n$ UrineVol2        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ UrineFlow2       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Diabetes         &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ DiabetesAge      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HealthGen        &lt;fct&gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vgo…\n$ DaysPhysHlthBad  &lt;int&gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0,…\n$ DaysMentHlthBad  &lt;int&gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA,…\n$ LittleInterest   &lt;fct&gt; Most, Most, Most, NA, Several, NA, NA, None, None, No…\n$ Depressed        &lt;fct&gt; Several, Several, Several, NA, Several, NA, NA, None,…\n$ nPregnancies     &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, N…\n$ nBabies          &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Age1stBaby       &lt;int&gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SleepHrsNight    &lt;int&gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, N…\n$ SleepTrouble     &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, Y…\n$ PhysActive       &lt;fct&gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes, …\n$ PhysActiveDays   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2, …\n$ TVHrsDay         &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CompHrsDay       &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ TVHrsDayChild    &lt;int&gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, N…\n$ CompHrsDayChild  &lt;int&gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, N…\n$ Alcohol12PlusYr  &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, Y…\n$ AlcoholDay       &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA, …\n$ AlcoholYear      &lt;int&gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, N…\n$ SmokeNow         &lt;fct&gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA, …\n$ Smoke100         &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No, …\n$ Smoke100n        &lt;fct&gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smoke…\n$ SmokeAge         &lt;int&gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, N…\n$ Marijuana        &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Ye…\n$ AgeFirstMarij    &lt;int&gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, N…\n$ RegularMarij     &lt;fct&gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes,…\n$ AgeRegMarij      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, N…\n$ HardDrugs        &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes, …\n$ SexEver          &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, Y…\n$ SexAge           &lt;int&gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, N…\n$ SexNumPartnLife  &lt;int&gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA, …\n$ SexNumPartYear   &lt;int&gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1,…\n$ SameSex          &lt;fct&gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, N…\n$ SexOrientation   &lt;fct&gt; Heterosexual, Heterosexual, Heterosexual, NA, Heteros…\n$ PregnantNow      &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nNHANES_adult &lt;-\n  NHANES %&gt;%\n  distinct(ID, .keep_all = TRUE) %&gt;%\n  filter(Age &gt;= 18) %&gt;%\n  select(Height) %&gt;%\n  drop_na(Height)\nNHANES_adult\n\n# A tibble: 4,790 × 1\n   Height\n    &lt;dbl&gt;\n 1   165.\n 2   168.\n 3   167.\n 4   170.\n 5   182.\n 6   169.\n 7   148.\n 8   178.\n 9   181.\n10   170.\n# ℹ 4,780 more rows\n\n\n\npop_mean &lt;- mosaic::mean(~Height, data = NHANES_adult)\n\npop_sd &lt;- mosaic::sd(~Height, data = NHANES_adult)\n\npop_mean\n\n[1] 168.3497\n\npop_sd \n\n[1] 10.15705\n\n\n\nsample_50 &lt;- mosaic::sample(NHANES_adult, size = 50) %&gt;%\n  select(Height)\nsample_50\n\n# A tibble: 50 × 1\n   Height\n    &lt;dbl&gt;\n 1   144.\n 2   158.\n 3   176.\n 4   155.\n 5   163 \n 6   167.\n 7   153.\n 8   151.\n 9   167.\n10   156.\n# ℹ 40 more rows\n\nsample_mean_50 &lt;- mean(~Height, data = sample_50)\nsample_mean_50\n\n[1] 166.84\n\n\n\nsample_50 %&gt;%\n  gf_histogram(~Height, bins = 10) %&gt;%\n  gf_vline(\n    xintercept = ~sample_mean_50,\n    color = \"purple\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_mean,\n    colour = \"black\"\n  ) %&gt;%\n  gf_label(7 ~ (pop_mean + 8),\n    label = \"Population Mean\",\n    color = \"black\"\n  ) %&gt;%\n  gf_label(7 ~ (sample_mean_50 - 8),\n    label = \"Sample Mean\", color = \"purple\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Distribution and Mean of a Single Sample\",\n    subtitle = \"Sample Size = 50\"\n  )\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 50 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 50 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\nsample_50_500 &lt;- do(500) * {\n  sample(NHANES_adult, size = 50) %&gt;%\n    select(Height) %&gt;% # drop sampling related column \"orig.id\"\n    summarise(\n      sample_mean = mean(Height),\n      sample_sd = sd(Height),\n      sample_min = min(Height),\n      sample_max = max(Height)\n    )\n}\nsample_50_500\n\n# A tibble: 500 × 6\n   sample_mean sample_sd sample_min sample_max  .row .index\n         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1        168.      9.73       151.       193.     1      1\n 2        167.      9.95       149        189.     1      2\n 3        168.      9.86       152.       190.     1      3\n 4        169.      9.85       145        196.     1      4\n 5        167.      9.64       149.       191.     1      5\n 6        170.     10.3        146.       193.     1      6\n 7        167.      9.58       152.       189.     1      7\n 8        168.     10.2        146.       186.     1      8\n 9        169.     10.2        149.       190.     1      9\n10        168.     11.3        146.       191.     1     10\n# ℹ 490 more rows\n\ndim(sample_50_500)\n\n[1] 500   6\n\n\n\nsample_50_500 %&gt;%\n  gf_point(.index ~ sample_mean,\n    color = \"purple\",\n    title = \"Sample Means are close to the Population Mean\",\n    subtitle = \"Sample Means are Random!\",\n    caption = \"Grey lines represent our 500 samples\"\n  ) %&gt;%\n  gf_segment(\n    .index + .index ~ sample_min + sample_max,\n    color = \"grey\",\n    linewidth = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_mean,\n    color = \"black\"\n  ) %&gt;%\n  gf_label(-25 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"black\"\n  )\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nIt can be more of less, therefore fore it is fair. i can trust the mean. Therefore a sample mean is a good point to estimate the data.\n\nsample_50_500 %&gt;%\n  gf_point(.index ~ sample_sd,\n    color = \"purple\",\n    title = \"Sample SDs are close to the Population Sd\",\n    subtitle = \"Sample SDs are Random!\",\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_sd,\n    color = \"black\"\n  ) %&gt;%\n  gf_label(-25 ~ pop_sd,\n    label = \"Population SD\",\n    color = \"black\"\n  ) %&gt;%\n  gf_refine(lims(x = c(4, 16)))\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\nsample_50_500 %&gt;%\n  gf_dhistogram(~sample_mean, bins = 30, xlab = \"Height\") %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(0.01 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Sampling Mean Distribution\",\n    subtitle = \"500 means\"\n  )\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n# How does this **distribution of sample-means** compare with the\n# overall distribution of the population?\n#\nsample_50_500 %&gt;%\n  gf_dhistogram(~sample_mean, bins = 30, xlab = \"Height\") %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(0.01 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  ## Add the population histogram\n  gf_histogram(~Height,\n    data = NHANES_adult,\n    alpha = 0.2, fill = \"blue\",\n    bins = 30\n  ) %&gt;%\n  gf_label(0.025 ~ (pop_mean + 20),\n    label = \"Population Distribution\", color = \"blue\"\n  ) %&gt;%\n  gf_labs(title = \"Sampling Mean Distribution\", subtitle = \"Original Population overlay\")\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\nDeriving the Central Limit Theorem (CLT)\nAs sample lenght increases, the density graph becomes narrower.\nlet’s test this out:\n\nsamples_08_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 08))\n\nsamples_16_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 16))\n\nsamples_32_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\n\nsamples_64_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\n\n# samples_128_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\nI will receive 1000 values for each sample size\nThe plotting of the 1000 values each seperately\n\n# Let us overlay their individual histograms to compare them:\np5 &lt;- gf_dhistogram(~mean,\n  data = samples_08_1000,\n  color = \"grey\",\n  fill = \"dodgerblue\", title = \"N = 8\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean, inherit = FALSE,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-0.025 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n##\np6 &lt;- gf_dhistogram(~mean,\n  data = samples_16_1000,\n  color = \"grey\",\n  fill = \"sienna\", title = \"N = 16\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n##\np7 &lt;- gf_dhistogram(~mean,\n  data = samples_32_1000,\n  na.rm = TRUE,\n  color = \"grey\",\n  fill = \"palegreen\", title = \"N = 32\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\", color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n\np8 &lt;- gf_dhistogram(~mean,\n  data = samples_64_1000,\n  na.rm = TRUE,\n  color = \"grey\",\n  fill = \"violetred\", title = \"N = 64\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\", color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n\n# patchwork::wrap_plots(p5,p6,p7,p8)\np5\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\np6\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\np7\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\np8\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThe range in the x axis, reduces as the sample size reduces."
  },
  {
    "objectID": "posts/day-5/index.html",
    "href": "posts/day-5/index.html",
    "title": "Day 5",
    "section": "",
    "text": "Variables1: Quantitative, Variable 2: Qualitative\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(palmerpenguins)\n\n\nwages &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/stevedata/gss_wages.csv\")\n\nRows: 61697 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): occrecode, wrkstat, gender, educcat, maritalcat\ndbl (7): rownames, year, realrinc, age, occ10, prestg10, childs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwages\n\n# A tibble: 61,697 × 12\n   rownames  year realrinc   age occ10 occrecode  prestg10 childs wrkstat gender\n      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; \n 1        1  1974     4935    21  5620 Office an…       25      0 School  Male  \n 2        2  1974    43178    41  2040 Professio…       66      3 Full-T… Male  \n 3        3  1974       NA    83    NA &lt;NA&gt;             NA      2 Housek… Female\n 4        4  1974       NA    69    NA &lt;NA&gt;             NA      2 Housek… Female\n 5        5  1974    18505    58  5820 Office an…       37      0 Full-T… Female\n 6        6  1974    22206    30   910 Business/…       45      0 School  Male  \n 7        7  1974    55515    48   230 Business/…       59      2 Full-T… Male  \n 8        8  1974       NA    67  6355 Construct…       49      1 Retired Male  \n 9        9  1974       NA    51  4720 Sales            28      2 Housek… Female\n10       10  1974     4935    54  3940 Service          38      2 Full-T… Female\n# ℹ 61,687 more rows\n# ℹ 2 more variables: educcat &lt;chr&gt;, maritalcat &lt;chr&gt;\n\n\n\nglimpse(wages)\n\nRows: 61,697\nColumns: 12\n$ rownames   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ year       &lt;dbl&gt; 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974,…\n$ realrinc   &lt;dbl&gt; 4935, 43178, NA, NA, 18505, 22206, 55515, NA, NA, 4935, NA,…\n$ age        &lt;dbl&gt; 21, 41, 83, 69, 58, 30, 48, 67, 51, 54, 89, 71, 27, 30, 22,…\n$ occ10      &lt;dbl&gt; 5620, 2040, NA, NA, 5820, 910, 230, 6355, 4720, 3940, 4810,…\n$ occrecode  &lt;chr&gt; \"Office and Administrative Support\", \"Professional\", NA, NA…\n$ prestg10   &lt;dbl&gt; 25, 66, NA, NA, 37, 45, 59, 49, 28, 38, 47, 45, 50, 29, 33,…\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 0, 2, 1, 2, 2, 3, 1, 4, 3, 0, 1, 2, 3, 4, 8,…\n$ wrkstat    &lt;chr&gt; \"School\", \"Full-Time\", \"Housekeeper\", \"Housekeeper\", \"Full-…\n$ gender     &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male…\n$ educcat    &lt;chr&gt; \"High School\", \"Bachelor\", \"Less Than High School\", \"Less T…\n$ maritalcat &lt;chr&gt; \"Married\", \"Married\", \"Widowed\", \"Widowed\", \"Never Married\"…\n\n\n\nwages %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n61697\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\noccrecode\n3561\n0.94\n5\n37\n0\n11\n0\n\n\nwrkstat\n21\n1.00\n5\n23\n0\n8\n0\n\n\ngender\n0\n1.00\n4\n6\n0\n2\n0\n\n\neduccat\n135\n1.00\n8\n21\n0\n5\n0\n\n\nmaritalcat\n27\n1.00\n7\n13\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrownames\n0\n1.00\n30849.00\n17810.53\n1\n15425\n30849\n46273\n61697.0\n▇▇▇▇▇\n\n\nyear\n0\n1.00\n1996.07\n12.79\n1974\n1985\n1996\n2006\n2018.0\n▆▇▇▇▇\n\n\nrealrinc\n23810\n0.61\n22326.36\n28581.79\n227\n8156\n16563\n27171\n480144.5\n▇▁▁▁▁\n\n\nage\n219\n1.00\n46.18\n17.56\n18\n32\n44\n59\n89.0\n▇▇▆▅▂\n\n\nocc10\n3561\n0.94\n4695.77\n2627.72\n10\n2710\n4720\n6230\n9997.0\n▃▅▇▂▃\n\n\nprestg10\n4186\n0.93\n43.06\n12.99\n16\n33\n42\n50\n80.0\n▃▇▇▃▁\n\n\nchilds\n189\n1.00\n1.92\n1.76\n0\n0\n2\n3\n8.0\n▇▇▂▁▁\n\n\n\n\n\n\ninspect(wages)\n\n\ncategorical variables:  \n        name     class levels     n missing\n1  occrecode character     11 58136    3561\n2    wrkstat character      8 61676      21\n3     gender character      2 61697       0\n4    educcat character      5 61562     135\n5 maritalcat character      5 61670      27\n                                   distribution\n1 Professional (19%), Service (16.9%) ...      \n2 Full-Time (49.4%), Housekeeper (15.1%) ...   \n3 Female (56.1%), Male (43.9%)                 \n4 High School (51.5%) ...                      \n5 Married (51.7%), Never Married (21.8%) ...   \n\nquantitative variables:  \n      name   class  min    Q1 median    Q3      max         mean           sd\n1 rownames numeric    1 15425  30849 46273  61697.0 30849.000000 17810.534116\n2     year numeric 1974  1985   1996  2006   2018.0  1996.073715    12.794470\n3 realrinc numeric  227  8156  16563 27171 480144.5 22326.359234 28581.794499\n4      age numeric   18    32     44    59     89.0    46.176177    17.561065\n5    occ10 numeric   10  2710   4720  6230   9997.0  4695.774081  2627.724076\n6 prestg10 numeric   16    33     42    50     80.0    43.060701    12.987526\n7   childs numeric    0     0      2     3      8.0     1.923457     1.763569\n      n missing\n1 61697       0\n2 61697       0\n3 37887   23810\n4 61478     219\n5 58136    3561\n6 57511    4186\n7 61508     189\n\n\n\nyear (&lt;dbl&gt;):\n\nThe year in which the survey was conducted. This is a numeric (double) variable representing the survey’s year (e.g., 2010, 2015, etc.).\n\nrealrinc (&lt;dbl&gt;):\n\nThis likely stands for real income. It’s the respondent’s income adjusted for inflation, typically measured in constant dollars from a specific base year. This allows comparisons of income across different years by accounting for changes in the cost of living.\n\nage (&lt;dbl&gt;):\n\nThe age of the respondent in years. It’s a numeric value.\n\nocc10 (&lt;dbl&gt;):\n\nThis is likely a standard occupation code, using the 2010 Occupational Classification System. It’s a numeric code representing the respondent’s occupation according to a standardized system used by labor statistics agencies.\n\noccrecode (&lt;chr&gt;):\n\nThis likely refers to a recoded occupation variable. The occupation might have been simplified or recoded into broader categories to facilitate analysis. This is stored as a character string (text).\n\nprestg10 (&lt;dbl&gt;):\n\nThis likely stands for occupational prestige score (2010). It is a numeric value that represents the societal prestige or status of the respondent’s occupation, based on surveys or expert ratings of different jobs.\n\nchilds (&lt;dbl&gt;):\n\nThe number of children the respondent has. This is a numeric variable representing the count of children.\n\nwrkstat (&lt;chr&gt;):\n\nLikely short for work status. It indicates the respondent’s current employment situation (e.g., “Employed full-time,” “Unemployed,” “Retired”). This is a categorical variable stored as a character string.\n\ngender (&lt;chr&gt;):\n\nThe respondent’s gender, typically recorded as “Male” or “Female.” This is a categorical (character) variable.\n\neduccat (&lt;chr&gt;):\n\n\nThis refers to education category, which classifies respondents into broad educational attainment groups (e.g., “Less than High School,” “High School Graduate,” “Some College,” “College Graduate”). This is a categorical variable.\n\n\nmaritalcat (&lt;chr&gt;):\n\n\nThis is a marital status category, which categorizes respondents into groups based on their marital status (e.g., “Married,” “Single,” “Divorced,” “Widowed”). It’s a categorical variable stored as a character string.\n\nSince there are so many missing data in the target variable realinc and there is still enough data leftover, let us remove the rows containing missing data in that variable.\n\nwages_clean &lt;-\n  wages %&gt;%\n  tidyr::drop_na(realrinc) # choose column or leave blank to choose all\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(realrinc ~ \"Income\") %&gt;% ## There is nothing really to put on the x. \n  gf_labs(\n    title = \"Plot 1A: Income has a skewed distribution\",\n    subtitle = \"Many outliers on the high side\"\n  )\n\n\n\n\n\n\n\n\nIncome is a very skewed distribution. There is a large population of people with low incomes and we observe a couple of outliers in higher incomes.\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 2A: Income by Gender\")\n\n\n\n\n\n\n\n\nTo understand this better, we take the log10 of the Realinc and we add colour to the 2 groups we are seperating it by\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc, fill = ~gender) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(title = \"Plot 2C: Income filled by Gender, log scale\")\n\n\n\n\n\n\n\n\nThe IQR for males is smaller than the IQR for females. There is less variation in the middle ranges of realrinc for men. log10 transformation helps to view and understand the regions of low realrinc. There median of both genders suggests that there may be a disparity in pay. We can only confirm this by looking at the mean value.\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 3A: Income by Education Category\")\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ log10(realrinc)) %&gt;%\n  gf_labs(title = \"Plot 3B: Log(Income) by Education Category\")\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(\n    reorder(educcat, realrinc, FUN = median) ~ log(realrinc),\n    fill = ~educcat,\n    alpha = 0.3\n  ) %&gt;%\n  gf_labs(title = \"Plot 3C: Log(Income) by Education Category, sorted\") %&gt;%\n  gf_labs(\n    x = \"Log Income\",\n    y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\nreorder(educcat, realrinc, FUN = median): Reorders the educcat (education category) based on the median of realrinc (income). This means the education categories will be ordered in the plot from lowest to highest median income.\n\nwages_clean %&gt;%\n  gf_boxplot(reorder(educcat, realrinc, FUN = median) ~ realrinc,\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Income by Education Category, sorted\",\n    subtitle = \"Log Income\"\n  ) %&gt;%\n  gf_labs(\n    x = \"Income\",\n    y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\nThe second code uses gf_refine(scale_x_log10()), which transforms the x-axis scale into a logarithmic scale (base 10) after the plot is created. This means the log transformation is purely for the axis labels and the scaling of the plot, not for the underlying data in the plot.\nConclutions?\nThere an increase in the median of income as the qualifications increase as expected. There are people with very low and very high income in all categories of education.\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(educcat, realrinc) ~ log10(realrinc),\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(childs)) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Log Income by Education Category and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\nFamily size does not seem to play a role in income while the education does.\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(childs = as_factor(childs)) %&gt;%\n  gf_boxplot(childs ~ log10(realrinc),\n    group = ~childs,\n    fill = ~childs,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~gender) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  gf_boxplot(gender ~ log10(realrinc),\n    group = ~gender,\n    fill = ~gender,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~childs) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  gf_boxplot(gender ~ log10(realrinc),\n    group = ~gender,\n    fill = ~gender,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~educcat) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Qualification\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )"
  },
  {
    "objectID": "posts/day-5/index.html#introduction",
    "href": "posts/day-5/index.html#introduction",
    "title": "Day 5",
    "section": "",
    "text": "Variables1: Quantitative, Variable 2: Qualitative\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(palmerpenguins)\n\n\nwages &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/stevedata/gss_wages.csv\")\n\nRows: 61697 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): occrecode, wrkstat, gender, educcat, maritalcat\ndbl (7): rownames, year, realrinc, age, occ10, prestg10, childs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwages\n\n# A tibble: 61,697 × 12\n   rownames  year realrinc   age occ10 occrecode  prestg10 childs wrkstat gender\n      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; \n 1        1  1974     4935    21  5620 Office an…       25      0 School  Male  \n 2        2  1974    43178    41  2040 Professio…       66      3 Full-T… Male  \n 3        3  1974       NA    83    NA &lt;NA&gt;             NA      2 Housek… Female\n 4        4  1974       NA    69    NA &lt;NA&gt;             NA      2 Housek… Female\n 5        5  1974    18505    58  5820 Office an…       37      0 Full-T… Female\n 6        6  1974    22206    30   910 Business/…       45      0 School  Male  \n 7        7  1974    55515    48   230 Business/…       59      2 Full-T… Male  \n 8        8  1974       NA    67  6355 Construct…       49      1 Retired Male  \n 9        9  1974       NA    51  4720 Sales            28      2 Housek… Female\n10       10  1974     4935    54  3940 Service          38      2 Full-T… Female\n# ℹ 61,687 more rows\n# ℹ 2 more variables: educcat &lt;chr&gt;, maritalcat &lt;chr&gt;\n\n\n\nglimpse(wages)\n\nRows: 61,697\nColumns: 12\n$ rownames   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ year       &lt;dbl&gt; 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974,…\n$ realrinc   &lt;dbl&gt; 4935, 43178, NA, NA, 18505, 22206, 55515, NA, NA, 4935, NA,…\n$ age        &lt;dbl&gt; 21, 41, 83, 69, 58, 30, 48, 67, 51, 54, 89, 71, 27, 30, 22,…\n$ occ10      &lt;dbl&gt; 5620, 2040, NA, NA, 5820, 910, 230, 6355, 4720, 3940, 4810,…\n$ occrecode  &lt;chr&gt; \"Office and Administrative Support\", \"Professional\", NA, NA…\n$ prestg10   &lt;dbl&gt; 25, 66, NA, NA, 37, 45, 59, 49, 28, 38, 47, 45, 50, 29, 33,…\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 0, 2, 1, 2, 2, 3, 1, 4, 3, 0, 1, 2, 3, 4, 8,…\n$ wrkstat    &lt;chr&gt; \"School\", \"Full-Time\", \"Housekeeper\", \"Housekeeper\", \"Full-…\n$ gender     &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male…\n$ educcat    &lt;chr&gt; \"High School\", \"Bachelor\", \"Less Than High School\", \"Less T…\n$ maritalcat &lt;chr&gt; \"Married\", \"Married\", \"Widowed\", \"Widowed\", \"Never Married\"…\n\n\n\nwages %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n61697\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\noccrecode\n3561\n0.94\n5\n37\n0\n11\n0\n\n\nwrkstat\n21\n1.00\n5\n23\n0\n8\n0\n\n\ngender\n0\n1.00\n4\n6\n0\n2\n0\n\n\neduccat\n135\n1.00\n8\n21\n0\n5\n0\n\n\nmaritalcat\n27\n1.00\n7\n13\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrownames\n0\n1.00\n30849.00\n17810.53\n1\n15425\n30849\n46273\n61697.0\n▇▇▇▇▇\n\n\nyear\n0\n1.00\n1996.07\n12.79\n1974\n1985\n1996\n2006\n2018.0\n▆▇▇▇▇\n\n\nrealrinc\n23810\n0.61\n22326.36\n28581.79\n227\n8156\n16563\n27171\n480144.5\n▇▁▁▁▁\n\n\nage\n219\n1.00\n46.18\n17.56\n18\n32\n44\n59\n89.0\n▇▇▆▅▂\n\n\nocc10\n3561\n0.94\n4695.77\n2627.72\n10\n2710\n4720\n6230\n9997.0\n▃▅▇▂▃\n\n\nprestg10\n4186\n0.93\n43.06\n12.99\n16\n33\n42\n50\n80.0\n▃▇▇▃▁\n\n\nchilds\n189\n1.00\n1.92\n1.76\n0\n0\n2\n3\n8.0\n▇▇▂▁▁\n\n\n\n\n\n\ninspect(wages)\n\n\ncategorical variables:  \n        name     class levels     n missing\n1  occrecode character     11 58136    3561\n2    wrkstat character      8 61676      21\n3     gender character      2 61697       0\n4    educcat character      5 61562     135\n5 maritalcat character      5 61670      27\n                                   distribution\n1 Professional (19%), Service (16.9%) ...      \n2 Full-Time (49.4%), Housekeeper (15.1%) ...   \n3 Female (56.1%), Male (43.9%)                 \n4 High School (51.5%) ...                      \n5 Married (51.7%), Never Married (21.8%) ...   \n\nquantitative variables:  \n      name   class  min    Q1 median    Q3      max         mean           sd\n1 rownames numeric    1 15425  30849 46273  61697.0 30849.000000 17810.534116\n2     year numeric 1974  1985   1996  2006   2018.0  1996.073715    12.794470\n3 realrinc numeric  227  8156  16563 27171 480144.5 22326.359234 28581.794499\n4      age numeric   18    32     44    59     89.0    46.176177    17.561065\n5    occ10 numeric   10  2710   4720  6230   9997.0  4695.774081  2627.724076\n6 prestg10 numeric   16    33     42    50     80.0    43.060701    12.987526\n7   childs numeric    0     0      2     3      8.0     1.923457     1.763569\n      n missing\n1 61697       0\n2 61697       0\n3 37887   23810\n4 61478     219\n5 58136    3561\n6 57511    4186\n7 61508     189\n\n\n\nyear (&lt;dbl&gt;):\n\nThe year in which the survey was conducted. This is a numeric (double) variable representing the survey’s year (e.g., 2010, 2015, etc.).\n\nrealrinc (&lt;dbl&gt;):\n\nThis likely stands for real income. It’s the respondent’s income adjusted for inflation, typically measured in constant dollars from a specific base year. This allows comparisons of income across different years by accounting for changes in the cost of living.\n\nage (&lt;dbl&gt;):\n\nThe age of the respondent in years. It’s a numeric value.\n\nocc10 (&lt;dbl&gt;):\n\nThis is likely a standard occupation code, using the 2010 Occupational Classification System. It’s a numeric code representing the respondent’s occupation according to a standardized system used by labor statistics agencies.\n\noccrecode (&lt;chr&gt;):\n\nThis likely refers to a recoded occupation variable. The occupation might have been simplified or recoded into broader categories to facilitate analysis. This is stored as a character string (text).\n\nprestg10 (&lt;dbl&gt;):\n\nThis likely stands for occupational prestige score (2010). It is a numeric value that represents the societal prestige or status of the respondent’s occupation, based on surveys or expert ratings of different jobs.\n\nchilds (&lt;dbl&gt;):\n\nThe number of children the respondent has. This is a numeric variable representing the count of children.\n\nwrkstat (&lt;chr&gt;):\n\nLikely short for work status. It indicates the respondent’s current employment situation (e.g., “Employed full-time,” “Unemployed,” “Retired”). This is a categorical variable stored as a character string.\n\ngender (&lt;chr&gt;):\n\nThe respondent’s gender, typically recorded as “Male” or “Female.” This is a categorical (character) variable.\n\neduccat (&lt;chr&gt;):\n\n\nThis refers to education category, which classifies respondents into broad educational attainment groups (e.g., “Less than High School,” “High School Graduate,” “Some College,” “College Graduate”). This is a categorical variable.\n\n\nmaritalcat (&lt;chr&gt;):\n\n\nThis is a marital status category, which categorizes respondents into groups based on their marital status (e.g., “Married,” “Single,” “Divorced,” “Widowed”). It’s a categorical variable stored as a character string.\n\nSince there are so many missing data in the target variable realinc and there is still enough data leftover, let us remove the rows containing missing data in that variable.\n\nwages_clean &lt;-\n  wages %&gt;%\n  tidyr::drop_na(realrinc) # choose column or leave blank to choose all\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(realrinc ~ \"Income\") %&gt;% ## There is nothing really to put on the x. \n  gf_labs(\n    title = \"Plot 1A: Income has a skewed distribution\",\n    subtitle = \"Many outliers on the high side\"\n  )\n\n\n\n\n\n\n\n\nIncome is a very skewed distribution. There is a large population of people with low incomes and we observe a couple of outliers in higher incomes.\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 2A: Income by Gender\")\n\n\n\n\n\n\n\n\nTo understand this better, we take the log10 of the Realinc and we add colour to the 2 groups we are seperating it by\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc, fill = ~gender) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(title = \"Plot 2C: Income filled by Gender, log scale\")\n\n\n\n\n\n\n\n\nThe IQR for males is smaller than the IQR for females. There is less variation in the middle ranges of realrinc for men. log10 transformation helps to view and understand the regions of low realrinc. There median of both genders suggests that there may be a disparity in pay. We can only confirm this by looking at the mean value.\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 3A: Income by Education Category\")\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ log10(realrinc)) %&gt;%\n  gf_labs(title = \"Plot 3B: Log(Income) by Education Category\")\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(\n    reorder(educcat, realrinc, FUN = median) ~ log(realrinc),\n    fill = ~educcat,\n    alpha = 0.3\n  ) %&gt;%\n  gf_labs(title = \"Plot 3C: Log(Income) by Education Category, sorted\") %&gt;%\n  gf_labs(\n    x = \"Log Income\",\n    y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\nreorder(educcat, realrinc, FUN = median): Reorders the educcat (education category) based on the median of realrinc (income). This means the education categories will be ordered in the plot from lowest to highest median income.\n\nwages_clean %&gt;%\n  gf_boxplot(reorder(educcat, realrinc, FUN = median) ~ realrinc,\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Income by Education Category, sorted\",\n    subtitle = \"Log Income\"\n  ) %&gt;%\n  gf_labs(\n    x = \"Income\",\n    y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\nThe second code uses gf_refine(scale_x_log10()), which transforms the x-axis scale into a logarithmic scale (base 10) after the plot is created. This means the log transformation is purely for the axis labels and the scaling of the plot, not for the underlying data in the plot.\nConclutions?\nThere an increase in the median of income as the qualifications increase as expected. There are people with very low and very high income in all categories of education.\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(educcat, realrinc) ~ log10(realrinc),\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(childs)) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Log Income by Education Category and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\nFamily size does not seem to play a role in income while the education does.\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(childs = as_factor(childs)) %&gt;%\n  gf_boxplot(childs ~ log10(realrinc),\n    group = ~childs,\n    fill = ~childs,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~gender) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  gf_boxplot(gender ~ log10(realrinc),\n    group = ~gender,\n    fill = ~gender,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~childs) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  gf_boxplot(gender ~ log10(realrinc),\n    group = ~gender,\n    fill = ~gender,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~educcat) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Qualification\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )"
  },
  {
    "objectID": "posts/day-3(2)/index.html",
    "href": "posts/day-3(2)/index.html",
    "title": "Day 3",
    "section": "",
    "text": "Today we worked on count of qualitative data. We also looked in creation of bar graph - data in lengths.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing"
  },
  {
    "objectID": "posts/day-3(2)/index.html#introduction",
    "href": "posts/day-3(2)/index.html#introduction",
    "title": "Day 3",
    "section": "",
    "text": "Today we worked on count of qualitative data. We also looked in creation of bar graph - data in lengths.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing"
  },
  {
    "objectID": "posts/day-3(2)/index.html#taxi-data-set",
    "href": "posts/day-3(2)/index.html#taxi-data-set",
    "title": "Day 3",
    "section": "Taxi data-set",
    "text": "Taxi data-set\n\ntaxi &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/taxi.csv\")\n\nRows: 10000 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): tip, company, local, dow, month\ndbl (3): rownames, distance, hour\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntaxi\n\n# A tibble: 10,000 × 8\n   rownames tip   distance company                      local dow   month  hour\n      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1        1 yes      17.2  Chicago Independents         no    Thu   Feb      16\n 2        2 yes       0.88 City Service                 yes   Thu   Mar       8\n 3        3 yes      18.1  other                        no    Mon   Feb      18\n 4        4 yes      20.7  Chicago Independents         no    Mon   Apr       8\n 5        5 yes      12.2  Chicago Independents         no    Sun   Mar      21\n 6        6 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n 7        7 yes      17.5  Flash Cab                    no    Fri   Mar      12\n 8        8 yes      17.7  other                        no    Sun   Jan       6\n 9        9 yes       1.85 Taxicab Insurance Agency Llc no    Fri   Apr      12\n10       10 yes       1.47 City Service                 no    Tue   Mar      14\n# ℹ 9,990 more rows\n\n\nWe first glimpse it to identify what variables need to be converted to factors.\n\ntaxi %&gt;% dplyr::glimpse()\n\nRows: 10,000\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ tip      &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\"…\n$ distance &lt;dbl&gt; 17.19, 0.88, 18.11, 20.70, 12.23, 0.94, 17.47, 17.67, 1.85, 1…\n$ company  &lt;chr&gt; \"Chicago Independents\", \"City Service\", \"other\", \"Chicago Ind…\n$ local    &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\",…\n$ dow      &lt;chr&gt; \"Thu\", \"Thu\", \"Mon\", \"Mon\", \"Sun\", \"Sat\", \"Fri\", \"Sun\", \"Fri\"…\n$ month    &lt;chr&gt; \"Feb\", \"Mar\", \"Feb\", \"Apr\", \"Mar\", \"Apr\", \"Mar\", \"Jan\", \"Apr\"…\n$ hour     &lt;dbl&gt; 16, 8, 18, 8, 21, 23, 12, 6, 12, 14, 18, 11, 12, 19, 17, 13, …\n\n\nBefore we do mutate stuff into factors, it helps to know the number of levels of each variable, just to make sure. For do inspect and skim.\n\ntaxi %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n     name     class levels     n missing\n1     tip character      2 10000       0\n2 company character      7 10000       0\n3   local character      2 10000       0\n4     dow character      7 10000       0\n5   month character      4 10000       0\n                                   distribution\n1 yes (92.1%), no (7.9%)                       \n2 other (27.1%) ...                            \n3 no (81.2%), yes (18.8%)                      \n4 Thu (19.6%), Wed (17.5%), Tue (16.3%) ...    \n5 Apr (31.8%), Mar (31.4%), Feb (20.4%) ...    \n\nquantitative variables:  \n      name   class min      Q1  median        Q3     max        mean\n1 rownames numeric   1 2500.75 5000.50 7500.2500 10000.0 5000.500000\n2 distance numeric   0    0.94    1.78   15.5625    42.3    6.224144\n3     hour numeric   0   11.00   15.00   18.0000    23.0   14.177300\n           sd     n missing\n1 2886.895680 10000       0\n2    7.381397 10000       0\n3    4.359904 10000       0\n\n\n\ntaxi %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntip\n0\n1\n2\n3\n0\n2\n0\n\n\ncompany\n0\n1\n5\n28\n0\n7\n0\n\n\nlocal\n0\n1\n2\n3\n0\n2\n0\n\n\ndow\n0\n1\n3\n3\n0\n7\n0\n\n\nmonth\n0\n1\n3\n3\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrownames\n0\n1\n5000.50\n2886.90\n1\n2500.75\n5000.50\n7500.25\n10000.0\n▇▇▇▇▇\n\n\ndistance\n0\n1\n6.22\n7.38\n0\n0.94\n1.78\n15.56\n42.3\n▇▁▂▁▁\n\n\nhour\n0\n1\n14.18\n4.36\n0\n11.00\n15.00\n18.00\n23.0\n▁▃▅▇▃\n\n\n\n\n\n\nTable of Variables\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\ntip\nRepresents if the tip was given or not\nQualitative\n\n\ncompany\nThis variable indicates the taxi company or vendor that operates the vehicle.\nQualitative\n\n\nlocal\nThis variable may refer to whether the trip occurred within a local area or not\nQualitative\n\n\ndow\nhis variable represents the day of the week when the taxi ride took place\nQualitative\n\n\nmonth\nThis variable indicates the month during which the taxi ride occurred.\nQualitative\n\n\ndistance\nThis variable measures the total distance traveled during the taxi ride, usually expressed in miles\nQuantitative\n\n\nhour\nThis variable indicates the hour of the day when the trip started\nQuantitative\n\n\n\nTarget Variable: tip\nPredictor Variables: company, dow, local, month, hour\n\nAnd then we mutate. I feel like i scientist when i use the word mutate. We glimpse it again, cause why not.\n\n\ntaxi_modified &lt;- taxi %&gt;%\n  dplyr::mutate(\n    tip = factor(tip,\n      levels = c(\"no\", \"yes\"),\n      labels = c(\"no\", \"yes\"),\n      ordered = TRUE\n    ),\n    company = as_factor(company),\n    ##\n    dow = factor(dow,\n      levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"),\n      labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"),\n      ordered = TRUE\n    ),\n    ##\n    local = factor(local,\n      levels = c(\"no\", \"yes\"),\n      labels = c(\"no\", \"yes\"),\n      ordered = TRUE\n    ),\n    ##\n    month = factor(month,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n      labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n      ordered = TRUE\n    ),\n    hour = as_factor(hour)\n  )\nglimpse(taxi_modified)\n\nRows: 10,000\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ tip      &lt;ord&gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, y…\n$ distance &lt;dbl&gt; 17.19, 0.88, 18.11, 20.70, 12.23, 0.94, 17.47, 17.67, 1.85, 1…\n$ company  &lt;fct&gt; Chicago Independents, City Service, other, Chicago Independen…\n$ local    &lt;ord&gt; no, yes, no, no, no, yes, no, no, no, no, no, no, no, yes, no…\n$ dow      &lt;ord&gt; Thu, Thu, Mon, Mon, Sun, Sat, Fri, Sun, Fri, Tue, Tue, Sun, W…\n$ month    &lt;ord&gt; Feb, Mar, Feb, Apr, Mar, Apr, Mar, Jan, Apr, Mar, Mar, Apr, A…\n$ hour     &lt;fct&gt; 16, 8, 18, 8, 21, 23, 12, 6, 12, 14, 18, 11, 12, 19, 17, 13, …\n\n\n\nq1. Do more people tip than not?\n\ngf_bar(~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips\")\n\n\n\n\n\n\n\n\nLook that that! A vast majority of the lot, are people who tip!\n\n\nq2. Does the tip depend upon whether the trip is local or not?\nIn a dodged bar chart one variable is in the x- axis, but the y axis has the count of another variable. In this way we acquire counts of different levels of a qualitative data broken down by levels of another qualitative variable side by side, making it easy to compare.\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"dodge\"\n  ) %&gt;%\n  gf_labs(title = \"Dodged Bar Chart\" ,\n    subtitle = \"Proof that i know how to write subtitles\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nA stacked bar looks very similar to a dodged chart, but counts of both levels are on top of each other. I like this one better, it looks neater, but in my opinion if the counts of multiple levels of one paramter are closeby values, it would be more practical to use a dodged bar.\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Stacked Bar Chart\",\n    subtitle = \"A sandwiched version of the last one!\"\n  )%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nLooking at these, while we can spot the number of tips and lack of tips the locals and non-locals contributed- there are a lot more non local tips than local ones and in quantity, they do get most of their tips from non-locals.\n\nBut we can’t definitely say that non-locals tend to tip more certainly, it could only appear so beacuse the non-local trips are so much more. To find this, we will have to find the difference in ratios by using position = fill.\n\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"fill\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  )%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nWe can now observe that locals tend to tip lesser than the non-locals.\n\n\nq3. Do some cab companies get more tips than others?\n\ntaxi_modified %&gt;%\n  gf_bar(~company, fill = ~tip, position = \"dodge\") %&gt;%\n  gf_labs(title = \"Dodged Bar Chart\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1)))%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nOther gets the most count of trips but do they get the most number of tips in ratio?\n\ntaxi_modified %&gt;%\n  gf_props(~company, fill = ~tip, position = \"fill\") %&gt;%\n  gf_labs(\n    title = \"Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1)))%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nIt appears so that flash cabs is the company that gets the least amount of tips. While Chicago gets the most number of tips per-group proportion.\n\n\nq4. Does a tip depend upon the hour of the day? At which are are the tips highest and lowest?\n\ngf_bar(~hour, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Hour\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nThe count of tips are highest are the 17th hour while lowest is at the 4th hour.\n\ngf_bar(~hour, fill = ~tip, position = \"fill\", data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Hour in proportion\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nPeople are most likely to tip at 2nd and 4th hour and least at 8th, 15th and 23rd hour.\n\n\nq5. Does a tip depend upon the day of the week? At which day are the tips highest and lowest?\n\ngf_bar(~dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Day of Week\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nThe highest count of tips are on Thursdays and lowest is on Sundays. The count of tips thend to be the highest in weekdays and lesser in weekends\n\ngf_bar(~dow, fill = ~tip, data = taxi_modified, position = \"fill\") %&gt;%\n  gf_labs(title = \"Counts of Tips by Day of Week in proportion\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nPeople are most likely to tip on Sundays and least likely to tip on Fridays.\n\n\nq6. At which month are the tips highest and lowest among the 4 recorded?\n\ngf_bar(~month, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Month\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nThe highest count of tips is on April and lowest is on January.\n\ngf_bar(~month, fill = ~tip, data = taxi_modified, position = \"fill\") %&gt;%\n  gf_labs(title = \"Counts of Tips by Month in proportion\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nPeople are least likely to tip in April and most likely to tip in January.\n\n\nCounts of Tips by Day of Week and Month\n\nHere, we are obtaining a total 7 graphs, each for a day week. Each graph has 4 bars each, with month as the x axis indicating- for each day, in what month, the count of tips is observed.\n\n\ngf_bar(~ month | dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Day of Week and Month\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\n\n\nCounts of Tips by Hour and Day of Week\n\ngf_bar(~ hour | dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(\n    title = \"Counts of Tips by Hour and Day of Week\"\n  )%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))"
  },
  {
    "objectID": "posts/day-3(2)/index.html#addiction-healthcare-data-set",
    "href": "posts/day-3(2)/index.html#addiction-healthcare-data-set",
    "title": "Day 3",
    "section": "Addiction healthcare data-set",
    "text": "Addiction healthcare data-set\n\ndata(\"HELPrct\")\n\n\nObtaining the count of of each substance\n\nHELPrct %&gt;% gf_bar(~substance)\n\n\n\n\n\n\n\n\nBased on this data, more people are addicted to alcohol than cocaine or heroin.\n\n\nPlotting the count of substance based on gender\n\nHELPrct %&gt;% gf_bar(~substance,fill = ~sex)%&gt;% \n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nFor all 3 substances, more men in count seem to be addicted than women."
  },
  {
    "objectID": "posts/day-3(2)/index.html#banned-books-data-set-trying-it-myself",
    "href": "posts/day-3(2)/index.html#banned-books-data-set-trying-it-myself",
    "title": "Day 3",
    "section": "Banned Books data-set (Trying it myself)",
    "text": "Banned Books data-set (Trying it myself)\n\nbanned &lt;- read_csv(\"../../data/banned-author-title.csv\")\n\nRows: 1586 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): Author, Title, Type of Ban, State, District, Date of Challenge/Remo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbanned\n\n# A tibble: 1,586 × 7\n   Author              Title `Type of Ban` State District Date of Challenge/Re…¹\n   &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;                 \n 1 Àbíké-Íyímídé, Far… Ace … Banned from … Flor… Indian … Nov-21                \n 2 Acevedo, Elizabeth  Clap… Banned from … Penn… Central… Sep-21                \n 3 Acevedo, Elizabeth  The … Banned from … Flor… Indian … Nov-21                \n 4 Acevedo, Elizabeth  The … Banned from … New … Marlbor… Feb-22                \n 5 Acevedo, Elizabeth  The … Banned Pendi… Texas Frederi… Mar-22                \n 6 Acevedo, Elizabeth  The … Banned Pendi… Virg… New Ken… Oct-21                \n 7 Aciman, André       Call… Banned Pendi… Virg… Spotsly… Nov-21                \n 8 Acito, Marc         How … Banned Pendi… Flor… Indian … Nov-21                \n 9 Adeyoha, Koja       47,0… Banned from … Penn… Central… Sep-21                \n10 Adichie, Chimamand… Half… Banned from … Mich… Hudsonv… Jan-22                \n# ℹ 1,576 more rows\n# ℹ abbreviated name: ¹​`Date of Challenge/Removal`\n# ℹ 1 more variable: `Origin of Challenge` &lt;chr&gt;\n\n\n\nbanned %&gt;% dplyr::glimpse()\n\nRows: 1,586\nColumns: 7\n$ Author                      &lt;chr&gt; \"Àbíké-Íyímídé, Faridah\", \"Acevedo, Elizab…\n$ Title                       &lt;chr&gt; \"Ace of Spades\", \"Clap When You Land\", \"Th…\n$ `Type of Ban`               &lt;chr&gt; \"Banned from Libraries and Classrooms\", \"B…\n$ State                       &lt;chr&gt; \"Florida\", \"Pennsylvania\", \"Florida\", \"New…\n$ District                    &lt;chr&gt; \"Indian River County School\", \"Central Yor…\n$ `Date of Challenge/Removal` &lt;chr&gt; \"Nov-21\", \"Sep-21\", \"Nov-21\", \"Feb-22\", \"M…\n$ `Origin of Challenge`       &lt;chr&gt; \"Administrator\", \"Administrator\", \"Adminis…\n\n\n\n banned %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n                       name     class levels    n missing\n1                    Author character    797 1586       0\n2                     Title character   1145 1586       0\n3               Type of Ban character      4 1586       0\n4                     State character     26 1586       0\n5                  District character     86 1586       0\n6 Date of Challenge/Removal character     12 1586       0\n7       Origin of Challenge character      2 1586       0\n                                   distribution\n1 Kobabe, Maia (1.9%) ...                      \n2 Gender Queer: A Memoir (1.9%) ...            \n3 Banned Pending Investigation (46.1%) ...     \n4 Texas (45%), Pennsylvania (28.8%) ...        \n5 Central York (27.8%) ...                     \n6 Sep-21 (28.8%), Dec-21 (28.3%) ...           \n7 Administrator (95.6%) ...                    \n\n\n\nTable of Variables\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nAuthor\nRepresents the name of the author of the book that has been challenged or banned\nQualitative\n\n\nTitle\nThis variable indicates the title of the book that has faced challenges or bans.\nQualitative\n\n\nType of Ban\nDescribes the context in which the book is banned\nQualitative\n\n\nState\nIndicates the U.S. state where the challenge or ban occurred.\nQualitative\n\n\nDistrict\nThis variable specifies the school district or library district involved in the challenge or ban.\nQualitative\n\n\nDate of Challenge/Removal\nThis variable records the date when the challenge was made or when the book was removed from circulation.\nQualitative\n\n\nOrigin of Challenge\nIndicates who initiated the challenge\nQualitative\n\n\n\n\nType of ban, State, District, Date of challenge and origin of challenge can be declared as factors.\n\n\nWhat could the target variable be?\nI guess it could be type of ban - understanding it could help predict if a book will be fully banned, temporarily banned, or restricted or with State/district, we could understand which states or districts are more likely to challenge books.\nWhen it comes to predictor variables, everything except title, author and district (there way too many levels) could be predictor variables.\n\nbanned_modified &lt;- banned %&gt;%\n  dplyr::mutate(\n    `Type of Ban` = as_factor(`Type of Ban`),\n    State = as_factor(State),\n    District = as_factor(District),\n    `Date of Challenge/Removal` = as_factor(`Date of Challenge/Removal`),\n    `Origin of Challenge` = as_factor(`Origin of Challenge`),\n  )\nglimpse(banned_modified)\n\nRows: 1,586\nColumns: 7\n$ Author                      &lt;chr&gt; \"Àbíké-Íyímídé, Faridah\", \"Acevedo, Elizab…\n$ Title                       &lt;chr&gt; \"Ace of Spades\", \"Clap When You Land\", \"Th…\n$ `Type of Ban`               &lt;fct&gt; Banned from Libraries and Classrooms, Bann…\n$ State                       &lt;fct&gt; Florida, Pennsylvania, Florida, New York, …\n$ District                    &lt;fct&gt; Indian River County School, Central York, …\n$ `Date of Challenge/Removal` &lt;fct&gt; Nov-21, Sep-21, Nov-21, Feb-22, Mar-22, Oc…\n$ `Origin of Challenge`       &lt;fct&gt; Administrator, Administrator, Administrato…\n\n\n\nbanned_modified %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n                       name     class levels    n missing\n1                    Author character    797 1586       0\n2                     Title character   1145 1586       0\n3               Type of Ban    factor      4 1586       0\n4                     State    factor     26 1586       0\n5                  District    factor     86 1586       0\n6 Date of Challenge/Removal    factor     12 1586       0\n7       Origin of Challenge    factor      2 1586       0\n                                   distribution\n1 Kobabe, Maia (1.9%) ...                      \n2 Gender Queer: A Memoir (1.9%) ...            \n3 Banned Pending Investigation (46.1%) ...     \n4 Texas (45%), Pennsylvania (28.8%) ...        \n5 Central York (27.8%) ...                     \n6 Sep-21 (28.8%), Dec-21 (28.3%) ...           \n7 Administrator (95.6%) ...                    \n\n\n\nbanned_modified %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nAuthor\n0\n1\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1\n2\n155\n0\n1145\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nType of Ban\n0\n1\nFALSE\n4\nBan: 731, Ban: 474, Ban: 197, Ban: 184\n\n\nState\n0\n1\nFALSE\n26\nTex: 713, Pen: 456, Flo: 204, Okl: 43\n\n\nDistrict\n0\n1\nFALSE\n86\nCen: 441, Nor: 435, Ind: 161, Gra: 131\n\n\nDate of Challenge/Removal\n0\n1\nFALSE\n12\nSep: 456, Dec: 449, Nov: 227, Jan: 178\n\n\nOrigin of Challenge\n0\n1\nFALSE\n2\nAdm: 1517, For: 69\n\n\n\n\n\n\n\n\nTARGET VARIABLE: State\n\nq1. Which state has the most amount of banned books?\n\ngf_bar(~State, data = banned_modified) %&gt;%\n  gf_labs(title = \"Counts of States\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nAs expected, Texas leads the nation in the number of banned books, with Pennsylvania following in second place.\n\nSince there is data on so many states, like we did on day 4, i want to filter it to be only 10 states with the most amount of banned books.\n\n\ntop_states &lt;- banned_modified %&gt;%\n  group_by(State) %&gt;%                   # Group by state\n  summarize(n = n()) %&gt;%                # Count banned books per state\n  slice_max(n, n = 10) %&gt;%                # Select the top 5 states\n  pull(State)                           # Extract the state names\n\n\ntop_10_states_data &lt;- banned_modified %&gt;%\n  filter(State %in% top_states)\n\n\ntop_10_states_data\n\n# A tibble: 1,524 × 7\n   Author              Title `Type of Ban` State District Date of Challenge/Re…¹\n   &lt;chr&gt;               &lt;chr&gt; &lt;fct&gt;         &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt;                 \n 1 Àbíké-Íyímídé, Far… Ace … Banned from … Flor… Indian … Nov-21                \n 2 Acevedo, Elizabeth  Clap… Banned from … Penn… Central… Sep-21                \n 3 Acevedo, Elizabeth  The … Banned from … Flor… Indian … Nov-21                \n 4 Acevedo, Elizabeth  The … Banned Pendi… Texas Frederi… Mar-22                \n 5 Acevedo, Elizabeth  The … Banned Pendi… Virg… New Ken… Oct-21                \n 6 Aciman, André       Call… Banned Pendi… Virg… Spotsly… Nov-21                \n 7 Acito, Marc         How … Banned Pendi… Flor… Indian … Nov-21                \n 8 Adeyoha, Koja       47,0… Banned from … Penn… Central… Sep-21                \n 9 Agell, Charlotte    The … Banned from … Texas North E… Dec-21                \n10 Ahmadi, Arvin       How … Banned Pendi… Texas North E… Dec-21                \n# ℹ 1,514 more rows\n# ℹ abbreviated name: ¹​`Date of Challenge/Removal`\n# ℹ 1 more variable: `Origin of Challenge` &lt;fct&gt;\n\n\nTrying kskin again with this updated data-set to see if the amount of districts are small enough to be a facor\n\ntop_10_states_data %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1524\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nAuthor\n0\n1\n7\n29\n0\n789\n0\n\n\nTitle\n0\n1\n2\n155\n0\n1133\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nType of Ban\n0\n1\nFALSE\n4\nBan: 701, Ban: 464, Ban: 189, Ban: 170\n\n\nState\n0\n1\nFALSE\n10\nTex: 713, Pen: 456, Flo: 204, Okl: 43\n\n\nDistrict\n0\n1\nFALSE\n57\nCen: 441, Nor: 435, Ind: 161, Gra: 131\n\n\nDate of Challenge/Removal\n0\n1\nFALSE\n12\nSep: 451, Dec: 447, Nov: 224, Jan: 161\n\n\nOrigin of Challenge\n0\n1\nFALSE\n2\nAdm: 1479, For: 45\n\n\n\n\n\n\nNope, still too much. i guess i could try it tho.\n\n\n\nq2. What is the correlation between type of ban and the state it is challenged in?\n\ntop_10_states_data %&gt;%\n  gf_bar(~State,\n    fill = ~`Type of Ban`,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(title = \"Type of ban by state\" ,\n    subtitle = \"Stacked Bar Chart\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nIt seems that almost all books banned in Pennsylvania are banned in Classrooms and in Texas, while most of them are banned from a pending investigation, there is a significant section of it banned from libraries and classrooms\n\n\nq3. Does the ban depend on the Origin of Challenge?\n\ntop_10_states_data %&gt;%\n  gf_bar(~State,\n    fill = ~`Origin of Challenge`,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(title = \"Origin of Challenge by state\" ,\n    subtitle = \"Stacked Bar Chart\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nMost bans everywhere were raised by the administration. Interestingly, all the books banned in Pennsylvania have been challenged by the administrator. In Texas, a large majority has been challenged by Administrator but there is still a portion of it challenged formally. All banned books in Missouri were challenged formally, how surprising!\n\n\nq4. Is there a correlation between the type of ban and the Origin of challenge?\n\ngf_bar(~`Origin of Challenge` | `Type of Ban`, fill = ~State, data = top_10_states_data) %&gt;%\n  gf_labs(title = \"Counts of Ban by type of ban and origin in states\")\n\n\n\n\n\n\n\n\n\ngf_bar(~`Origin of Challenge` | State, fill = ~`Type of Ban`, data = top_10_states_data) %&gt;%\n  gf_labs(title = \"Counts of Types of Ban by State and Origin\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nYou would expect that only all books whose origin was formal would be the ones banned in classrooms (over protective parents exist all over the world) but surprising, From what i can see, in Texas, most books formally banned are banned from libraries (a few also banned from classrooms and libraries) and the ones banned in Missouri are mostly banned pending investigation.\n\n\nq5. In which month of which year were most bans challenged?\n\ngf_bar(~`Date of Challenge/Removal`, data = banned_modified) %&gt;%\n  gf_labs(title = \"Count of bans every month\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nMost books were banned during September and December.\n\ntop_10_states_data %&gt;%\n  gf_bar(~State,\n    fill = ~`Date of Challenge/Removal`,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(title = \"Date of Challenge/Removal by State\" ,\n    subtitle = \"Stacked bar chart\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nFrom this bar, it is clear that every state has one month where most of there bans are imposed. For Texas it’s mostly in December, some in January, fewer in March, fall and November. For Pennsylvania, it’s moslty all in September. For Florida it’s mostly in November.\n\n\nq6. Is there a correlation between the type of ban and when it was challenged?\n\ngf_bar(~`Type of Ban` | `Date of Challenge/Removal`, fill = ~State, data = top_10_states_data) %&gt;%\n  gf_labs(title = \"Counts of Types of Ban by State and Date of Challenge\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nTo no surprise, there does not seem to be any correlation between type of ban and the month it was challenged in.\n\n\nq7. Is there a correlation between the origin of ban and when it was challenged?\nI know it’s a long shot.\n\ngf_bar(~`Origin of Challenge` | State, fill = ~`Date of Challenge/Removal`, data = top_10_states_data) %&gt;%\n  gf_labs(title = \"Origin of challenge of State and Date of Challenge/Removal\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nThere doesn’t seem to be any clear correlation between origin of ban and when it was challenged except in Tennesse where most if not all administrator origins came at Winter 2021 and all formal challenges came of September 2021.\n\n\nq8. Which author has the honor of being the most banned?\nI know this doesn’t really come under the bar graph theme we were going with in this post but i really wanted to know and when i did yry and make it a bar graph, i wasn;t getting one with all the same counts.\n\ntop_author &lt;- banned_modified %&gt;%\n  count(Author) %&gt;%\n  slice_max(n, n = 10)  \n\ntop_author\n\n# A tibble: 10 × 2\n   Author                 n\n   &lt;chr&gt;              &lt;int&gt;\n 1 Kobabe, Maia          30\n 2 Hopkins, Ellen        27\n 3 Johnson, George M.    21\n 4 Do, Anh               17\n 5 Evison, Jonathan      16\n 6 Faruqi, Saadia        16\n 7 Jules, Jacqueline     16\n 8 Morrison, Toni        16\n 9 Myracle, Lauren       16\n10 Pérez, Ashley Hope    16\n\n\nKobabe, Maia is the author with the most number of banned books, so proud of him or her!\n\n\nq9. Which District of Texas has raised the most challenges?\n\ntop_1_state &lt;- banned_modified %&gt;%\n  group_by(State) %&gt;%                   \n  summarize(n = n()) %&gt;%                \n  slice_max(n, n = 1) %&gt;%                \n  pull(State)\n\ntop_1_states_data &lt;- banned_modified %&gt;%\n  filter(State %in% top_1_state)\n\n\ngf_bar(~District, data = top_1_states_data, fill = \"lightblue\") %&gt;%\n  gf_labs(title = \"Counts of Districts in Texas\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nThe North East District of Texas has the most number of challenges/ bans\n\n\n\nThe End of my A1."
  },
  {
    "objectID": "posts/cs3/index.html",
    "href": "posts/cs3/index.html",
    "title": "Case Study 3",
    "section": "",
    "text": "This data-set pertains to the extent of Antarctic Sea Ice over time that is monitored by the National Snow and Ice Data Center\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/cs3/index.html#introduction",
    "href": "posts/cs3/index.html#introduction",
    "title": "Case Study 3",
    "section": "",
    "text": "This data-set pertains to the extent of Antarctic Sea Ice over time that is monitored by the National Snow and Ice Data Center\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/cs3/index.html#playing-with-and-understanding-the-data",
    "href": "posts/cs3/index.html#playing-with-and-understanding-the-data",
    "title": "Case Study 3",
    "section": "Playing with and understanding the data",
    "text": "Playing with and understanding the data\n\nAcquiring the data:\n\nice &lt;- read_excel(\"../../data/ice.xlsx\", , sheet = \"SH-Daily-Extent\")\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...50`\n\nice\n\n# A tibble: 366 × 52\n   ...1     ...2 `1978` `1979` `1980` `1981` `1982` `1983` `1984` `1985` `1986`\n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 January     1     NA  NA      5.97   6.32  NA      6.51  NA     NA      7.72\n 2 &lt;NA&gt;        2     NA   6.94  NA     NA      7.04  NA      6.94   6.53  NA   \n 3 &lt;NA&gt;        3     NA  NA      5.67   5.79  NA      6.17  NA     NA      7.57\n 4 &lt;NA&gt;        4     NA   6.84  NA     NA      6.69  NA      6.65   6.06  NA   \n 5 &lt;NA&gt;        5     NA  NA      5.58   5.35  NA      5.87  NA     NA      7.24\n 6 &lt;NA&gt;        6     NA   6.64  NA     NA      6.39  NA      6.30   5.66  NA   \n 7 &lt;NA&gt;        7     NA  NA      5.33   5.19  NA      5.66  NA     NA      6.81\n 8 &lt;NA&gt;        8     NA   6.27  NA     NA      6.08  NA      5.94   5.31  NA   \n 9 &lt;NA&gt;        9     NA  NA      5      4.78  NA      5.30  NA     NA      6.28\n10 &lt;NA&gt;       10     NA   6.14  NA     NA      5.86  NA      5.63   4.93  NA   \n# ℹ 356 more rows\n# ℹ 41 more variables: `1987` &lt;dbl&gt;, `1988` &lt;dbl&gt;, `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;,\n#   `1991` &lt;dbl&gt;, `1992` &lt;dbl&gt;, `1993` &lt;dbl&gt;, `1994` &lt;dbl&gt;, `1995` &lt;dbl&gt;,\n#   `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;, `1999` &lt;dbl&gt;, `2000` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2005` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, …\n\n\n\nObservations (list of imperfections):\n\nThere is no data collected for 1978. For the next 9 years (till 1987), data has been entered every alternate day. Fom the mid of 1987, data has been entered every day but there by end ( from december 3rd) there is missing data till the end of that year.\nOn 1988- there is no data for the first few days of the week- first 12 days of January.\nWhat am i supposed to do with the data on Feb 29 only every 4 years?\nThe month was only entered for the 1st day of every month, Making the value of every other data entry of each month NA\n\n\n\n\nData Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\n..1\nRefers to the month the data was collected in\nQualitative\n\n\n..2\nRefers to date of the month the data was collected in\nQualitative\n\n\nYear (1978, 1979 … 2027)\nSea Ice Extent - the total area covered by sea ice, often measured in square kilometers based on the year, month and day\nQuantitative\n\n\n1981-2010 mean\nThis variable represents the average value for sea ice extent calculated over the 30-year period from 1981 to 2010.\nQuantitative\n\n\n1981-2010 median\nThis variable represents the median value for sea ice extent calculated over the 30-year period from 1981 to 2010.\nQuantitative\n\n\n\n\n\nTarget and Predictor Variables:\nTarget variable could be daily sea ice levels across multiple years.\nPredictor Variables:\n\nYear: Trends over time can have a major impact on sea ice due to climate change.\nMonth: Sea ice varies seasonally, so the month is a key predictor.\nDay: In daily data, day-to-day variations can be tracked."
  },
  {
    "objectID": "posts/cs3/index.html#defining-the-research-experiment",
    "href": "posts/cs3/index.html#defining-the-research-experiment",
    "title": "Case Study 3",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nIt could have been aimed at monitoring, measuring, and understanding changes in Antarctic sea ice over time. In order to do this, extensive data on sea ice coverage and its fluctuations have been collected over the span of many years.This would be used to identify trends in sea ice extent over the long-term (year-to-year trends) and short-term (daily or seasonal fluctuations).\nUse: Understanding the trend in Antarctic sea ice can provide critical insight into global climate change and its effects on the polar regions.\nQuestions:\n\nHow has the Antarctic sea ice extent changed from 1978 to the present day?\nAre there significant seasonal differences in sea ice over decades?\nHas the extent of Antarctic sea ice significantly reduced over the past 40 years due to rising global temperatures?"
  },
  {
    "objectID": "posts/cs3/index.html#graph",
    "href": "posts/cs3/index.html#graph",
    "title": "Case Study 3",
    "section": "Graph:",
    "text": "Graph:\n\nType of graph: A box plot.\n\nDefining the question:\n\nWhat happened to Antarctic sea ice extent in 2023 compared to historical data from 1980 to 2022?\nHow did the Antarctic ice extent in 2023 differ from the historical norms?\n\n\n\nAnalyzing the data:\nTypes of Variables: quantitative variables representing the ice extent that are stored unter mutiple columns seperated by the year the time they were collected in. Similar to my last case study, i need all the quantiative data i should plot in one column and so i creat a new data-set where a new row called series is created where the year is stored and the ice extent value under is stored in well… values.\n\nice %&gt;%\n  select(\"month\" = ...1, \"day\" = ...2, c(4:49)) %&gt;%\n  tidyr::fill(month) %&gt;%\n  pivot_longer(\n    cols = -c(month, day),\n    names_to = \"series\",\n    values_to = \"values\"\n  ) %&gt;%\n  mutate(\n    series = as.integer(series),\n    month = factor(month,\n      levels = month.name,\n      labels = month.name,\n      ordered = TRUE\n    ),\n  ) -&gt; ice_prepared\n\nice_prepared\n\n# A tibble: 16,836 × 4\n   month     day series values\n   &lt;ord&gt;   &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;\n 1 January     1   1979  NA   \n 2 January     1   1980   5.97\n 3 January     1   1981   6.32\n 4 January     1   1982  NA   \n 5 January     1   1983   6.51\n 6 January     1   1984  NA   \n 7 January     1   1985  NA   \n 8 January     1   1986   7.72\n 9 January     1   1987  NA   \n10 January     1   1988  NA   \n# ℹ 16,826 more rows\n\n\n\n\nReplicating the graph:\n\nice_prepared %&gt;%\n  filter(is.finite(values)) %&gt;%\n  gf_boxplot(values ~ month, color = \"darkgrey\") %&gt;%  \n  gf_point(values ~ month, data = subset(ice_prepared, series == 2023), color = \"salmon\", size = 3, shape = 15) %&gt;%  \n  gf_labs(title = \"Antarctic Ice Area over the years 1980 to 2023\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, all values of the year 2023 are being plotted across the month they are inputted in. Can i put only the average value of ice extent each month?\n\nice_avg_2023 &lt;- ice_prepared %&gt;%\n  filter(series == 2023) %&gt;%\n  group_by(month) %&gt;%                    \n  summarise(avg_value = mean(values, na.rm = TRUE))  \nice_avg_2023\n\n# A tibble: 12 × 2\n   month     avg_value\n   &lt;ord&gt;         &lt;dbl&gt;\n 1 January        3.23\n 2 February       1.91\n 3 March          2.80\n 4 April          5.49\n 5 May            8.36\n 6 June          11.0 \n 7 July          13.5 \n 8 August        15.5 \n 9 September     16.8 \n10 October       16.2 \n11 November      14.3 \n12 December       8.67\n\n\n\nice_prepared %&gt;%\n  filter(is.finite(values)) %&gt;%\n  gf_boxplot(values ~ month, color = \"darkgrey\") %&gt;%  \n  gf_point(data = ice_avg_2023, avg_value ~ month, color = \"salmon\", size = 3, shape = 15) %&gt;%  \n  gf_labs(title = \"Antarctic Ice Area over the years 1980 to 2023\", subtitle = \"What happend in 2023?\", x = \"Month\", y = \"Ice Evtent in million square km\", caption = \"Data: National Snow and Ice data center\")\n\n\n\n\n\n\n\n\n\nObservations:\n\nThere is a seasonal pattern in the Antarctic ice extent, with ice reaching its maximum around September and its minimum around February . This pattern seems to be consistent with the natural annual cycle of ice melting and formation in the region.\nFor all months, the ice extent in 2023 is lower than the historical median. June through October seems to have it being far more lower than the median compared to the other months."
  },
  {
    "objectID": "posts/cs3/index.html#my-story",
    "href": "posts/cs3/index.html#my-story",
    "title": "Case Study 3",
    "section": "My Story:",
    "text": "My Story:"
  },
  {
    "objectID": "posts/CS1/index.html",
    "href": "posts/CS1/index.html",
    "title": "Case Study 1",
    "section": "",
    "text": "This is a dataset pertaining to movies and genres, modified for ease of analysis and plotting."
  },
  {
    "objectID": "posts/CS1/index.html#introduction",
    "href": "posts/CS1/index.html#introduction",
    "title": "Case Study 1",
    "section": "",
    "text": "This is a dataset pertaining to movies and genres, modified for ease of analysis and plotting."
  },
  {
    "objectID": "posts/CS1/index.html#playing-with-and-understanding-the-data",
    "href": "posts/CS1/index.html#playing-with-and-understanding-the-data",
    "title": "Case Study 1",
    "section": "Playing with and understanding the data",
    "text": "Playing with and understanding the data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\n\n\nAcquiring the data:\n\nmovie_profit &lt;- read_delim(\"../../data/movie_profit.csv\", delim=\";\")\n\nRows: 3310 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (4): movie, distributor, mpaa_rating, genre\ndbl  (4): production_budget, domestic_gross, worldwide_gross, decade\nnum  (1): profit_ratio\ndate (1): release_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmovie_profit\n\n# A tibble: 3,310 × 10\n   release_date movie           production_budget domestic_gross worldwide_gross\n   &lt;date&gt;       &lt;chr&gt;                       &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n 1 2005-07-22   November                   250000         191862          191862\n 2 1998-08-28   I Married a St…            250000         203134          203134\n 3 1997-03-28   Love and Other…            250000         212285          743216\n 4 2000-07-14   Chuck&Buck                 250000        1055671         1157672\n 5 2011-10-28   Like Crazy                 250000        3395391         3728400\n 6 2003-04-11   Better Luck To…            250000        3802390         3809226\n 7 2017-04-28   Sleight                    250000        3930990         3934450\n 8 2002-06-28   Lovely and Ama…            250000        4210379         4613482\n 9 2012-08-17   Compliance                 270000         319285          830700\n10 2005-05-06   Fighting Tommy…            300000          10514           10514\n# ℹ 3,300 more rows\n# ℹ 5 more variables: distributor &lt;chr&gt;, mpaa_rating &lt;chr&gt;, genre &lt;chr&gt;,\n#   profit_ratio &lt;dbl&gt;, decade &lt;dbl&gt;\n\n\n\nglimpse(movie_profit)\n\nRows: 3,310\nColumns: 10\n$ release_date      &lt;date&gt; 2005-07-22, 1998-08-28, 1997-03-28, 2000-07-14, 201…\n$ movie             &lt;chr&gt; \"November\", \"I Married a Strange Person\", \"Love and …\n$ production_budget &lt;dbl&gt; 250000, 250000, 250000, 250000, 250000, 250000, 2500…\n$ domestic_gross    &lt;dbl&gt; 191862, 203134, 212285, 1055671, 3395391, 3802390, 3…\n$ worldwide_gross   &lt;dbl&gt; 191862, 203134, 743216, 1157672, 3728400, 3809226, 3…\n$ distributor       &lt;chr&gt; \"Other\", \"Other\", \"Other\", \"Other\", \"Paramount Pictu…\n$ mpaa_rating       &lt;chr&gt; \"R\", NA, \"R\", \"R\", \"PG-13\", \"R\", \"R\", \"R\", \"R\", \"R\",…\n$ genre             &lt;chr&gt; \"Drama\", \"Comedy\", \"Comedy\", \"Drama\", \"Drama\", \"Dram…\n$ profit_ratio      &lt;dbl&gt; 7.674480e+13, 8.125360e+13, 2.972864e+14, 4.630688e+…\n$ decade            &lt;dbl&gt; 2000, 1990, 1990, 2000, 2010, 2000, 2010, 2000, 2010…\n\n\n\nskim(movie_profit)\n\n\nData summary\n\n\nName\nmovie_profit\n\n\nNumber of rows\n3310\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nDate\n1\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmovie\n0\n1.00\n1\n35\n0\n3310\n0\n\n\ndistributor\n42\n0.99\n5\n18\n0\n6\n0\n\n\nmpaa_rating\n130\n0.96\n1\n5\n0\n4\n0\n\n\ngenre\n0\n1.00\n5\n9\n0\n5\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nrelease_date\n0\n1\n1936-02-05\n2017-12-22\n2005-06-30\n1723\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nproduction_budget\n0\n1\n3.326794e+07\n3.460741e+07\n2.50e+05\n9.500000e+06\n2.000000e+07\n4.500000e+07\n1.750000e+08\n▇▂▁▁▁\n\n\ndomestic_gross\n0\n1\n4.551509e+07\n5.852794e+07\n0.00e+00\n6.530094e+06\n2.558731e+07\n6.046695e+07\n4.745447e+08\n▇▁▁▁▁\n\n\nworldwide_gross\n0\n1\n9.384123e+07\n1.389514e+08\n4.23e+02\n1.086144e+07\n4.040903e+07\n1.184703e+08\n1.162782e+09\n▇▁▁▁▁\n\n\nprofit_ratio\n0\n1\n4.319388e+14\n1.501736e+15\n1.38e+10\n7.861269e+13\n1.962499e+14\n3.942158e+14\n4.315179e+16\n▇▁▁▁▁\n\n\ndecade\n0\n1\n1.998790e+03\n1.061000e+01\n1.93e+03\n1.990000e+03\n2.000000e+03\n2.010000e+03\n2.010000e+03\n▁▁▁▃▇\n\n\n\n\n\n\n\nData Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nmovie\nThe title of the movie\nQualitative\n\n\ndistributor\nThe company responsible for distributing the movie (e.g., Warner Bros., Disney)\nQualitative\n\n\nmpaa_rating\nThe movie’s MPAA rating (e.g., G, PG, PG-13, R), which indicates the age-appropriateness of the content.\nQualitative\n\n\ngenre\nThe type or category of the movie (e.g., Action, Comedy, Drama)\nQualitative\n\n\nrelease_date\nThe date when the movie was first released in theaters\nQualitative\n\n\nproduction_budget\nThe amount of money spent on producing the movie\nQuantitative\n\n\ndomestic_gross\nThe total earnings of the movie from domestic (usually U.S.) markets\nQuantitative\n\n\nworldwide_gross\nThe total earnings of the movie from all markets worldwide\nQuantitative\n\n\nprofit_ratio\nThe ratio of profit earned relative to the production budget\nQuantitative\n\n\ndecade\nThe decade during which the movie was released\nQualitative\n\n\n\n\nObservations:\n\nThe variables mpaa_rating, genre, and distributor are qualitative (categorical) data with a low enough number of unique levels that they could be treated as factors.\nDecade is quantitative data but could also be treated as a factor.\nThere are missing values in the dataset for mpaa_rating and distributor.\n\n\n\n\nTransforming the data\n\nmovie_modified &lt;- movie_profit %&gt;%\n  dplyr::mutate(\n    mpaa_rating = as_factor(mpaa_rating),\n    genre = as_factor(genre),\n    distributor = as_factor(distributor),\n    decade = as_factor(decade)\n  )\nmovie_modified\n\n# A tibble: 3,310 × 10\n   release_date movie           production_budget domestic_gross worldwide_gross\n   &lt;date&gt;       &lt;chr&gt;                       &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n 1 2005-07-22   November                   250000         191862          191862\n 2 1998-08-28   I Married a St…            250000         203134          203134\n 3 1997-03-28   Love and Other…            250000         212285          743216\n 4 2000-07-14   Chuck&Buck                 250000        1055671         1157672\n 5 2011-10-28   Like Crazy                 250000        3395391         3728400\n 6 2003-04-11   Better Luck To…            250000        3802390         3809226\n 7 2017-04-28   Sleight                    250000        3930990         3934450\n 8 2002-06-28   Lovely and Ama…            250000        4210379         4613482\n 9 2012-08-17   Compliance                 270000         319285          830700\n10 2005-05-06   Fighting Tommy…            300000          10514           10514\n# ℹ 3,300 more rows\n# ℹ 5 more variables: distributor &lt;fct&gt;, mpaa_rating &lt;fct&gt;, genre &lt;fct&gt;,\n#   profit_ratio &lt;dbl&gt;, decade &lt;fct&gt;\n\n\n\nglimpse(movie_modified)\n\nRows: 3,310\nColumns: 10\n$ release_date      &lt;date&gt; 2005-07-22, 1998-08-28, 1997-03-28, 2000-07-14, 201…\n$ movie             &lt;chr&gt; \"November\", \"I Married a Strange Person\", \"Love and …\n$ production_budget &lt;dbl&gt; 250000, 250000, 250000, 250000, 250000, 250000, 2500…\n$ domestic_gross    &lt;dbl&gt; 191862, 203134, 212285, 1055671, 3395391, 3802390, 3…\n$ worldwide_gross   &lt;dbl&gt; 191862, 203134, 743216, 1157672, 3728400, 3809226, 3…\n$ distributor       &lt;fct&gt; Other, Other, Other, Other, Paramount Pictures, Para…\n$ mpaa_rating       &lt;fct&gt; R, NA, R, R, PG-13, R, R, R, R, R, R, R, PG-13, NA, …\n$ genre             &lt;fct&gt; Drama, Comedy, Comedy, Drama, Drama, Drama, Action, …\n$ profit_ratio      &lt;dbl&gt; 7.674480e+13, 8.125360e+13, 2.972864e+14, 4.630688e+…\n$ decade            &lt;fct&gt; 2000, 1990, 1990, 2000, 2010, 2000, 2010, 2000, 2010…\n\n\n\nskim(movie_modified)\n\n\nData summary\n\n\nName\nmovie_modified\n\n\nNumber of rows\n3310\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nfactor\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmovie\n0\n1\n1\n35\n0\n3310\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nrelease_date\n0\n1\n1936-02-05\n2017-12-22\n2005-06-30\n1723\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndistributor\n42\n0.99\nFALSE\n6\nOth: 1737, War: 360, Son: 332, Uni: 299\n\n\nmpaa_rating\n130\n0.96\nFALSE\n4\nR: 1477, PG-: 1066, PG: 552, G: 85\n\n\ngenre\n0\n1.00\nFALSE\n5\nDra: 1209, Com: 798, Act: 547, Adv: 467\n\n\ndecade\n0\n1.00\nFALSE\n9\n200: 1387, 201: 994, 199: 607, 198: 228\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nproduction_budget\n0\n1\n3.326794e+07\n3.460741e+07\n2.50e+05\n9.500000e+06\n2.000000e+07\n4.500000e+07\n1.750000e+08\n▇▂▁▁▁\n\n\ndomestic_gross\n0\n1\n4.551509e+07\n5.852794e+07\n0.00e+00\n6.530094e+06\n2.558731e+07\n6.046695e+07\n4.745447e+08\n▇▁▁▁▁\n\n\nworldwide_gross\n0\n1\n9.384123e+07\n1.389514e+08\n4.23e+02\n1.086144e+07\n4.040903e+07\n1.184703e+08\n1.162782e+09\n▇▁▁▁▁\n\n\nprofit_ratio\n0\n1\n4.319388e+14\n1.501736e+15\n1.38e+10\n7.861269e+13\n1.962499e+14\n3.942158e+14\n4.315179e+16\n▇▁▁▁▁\n\n\n\n\n\n\n\nTarget and Predictor Variables:\nIn the context of your movie_profit data-set, a potential target variable could be profit_ratio (the name of the data-set does suggest so anyway). Leaving the rest of the variables- distributor, mpaa_rating, genre, decade, release_date, production_budget, domestic_gross and worldwide_gross to be predictor variables."
  },
  {
    "objectID": "posts/CS1/index.html#defining-the-research-experiment",
    "href": "posts/CS1/index.html#defining-the-research-experiment",
    "title": "Case Study 1",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nTo analyze factors that might influence the profitability of movies. Using statistical techniques, researchers might examine the relationships between the predictor variables and the target variable (profit ratio) to determine what factors contribute to higher profitability in films.\nUse: The study’s findings could inform studios, distributors, and investors about what factors are most likely to lead to a movie being profitable, potentially influencing future decisions in movie production.\n\nAnalyzing the Predictor Variables:\n\nDistributors:\n\nmovie_modified %&gt;% count(distributor)\n\n# A tibble: 7 × 2\n  distributor            n\n  &lt;fct&gt;              &lt;int&gt;\n1 Other               1737\n2 Paramount Pictures   261\n3 Universal            299\n4 20th Century Fox     279\n5 Sony Pictures        332\n6 Warner Bros.         360\n7 &lt;NA&gt;                  42\n\n\n\nObservations:\n\n“Other” Distributors: Largest group with 1,737 entries, suggesting many smaller or less-known distributors.\nTop Distributors include Warner Bros. (360), Sony Pictures (332), Universal (299), 20th Century Fox (279), and Paramount Pictures (261).\n\n\n\n\nMpaa Rating:\n\nmovie_modified %&gt;% count(mpaa_rating)\n\n# A tibble: 5 × 2\n  mpaa_rating     n\n  &lt;fct&gt;       &lt;int&gt;\n1 R            1477\n2 PG-13        1066\n3 PG            552\n4 G              85\n5 &lt;NA&gt;          130\n\n\n\nObservations:\nThis indicates a higher prevalence of R-rated and PG-13 movies in the data-set, with fewer G-rated films.\n\n\n\nGenre:\n\nmovie_modified %&gt;% count(genre)\n\n# A tibble: 5 × 2\n  genre         n\n  &lt;fct&gt;     &lt;int&gt;\n1 Drama      1209\n2 Comedy      798\n3 Action      547\n4 Horror      289\n5 Adventure   467\n\n\n\nObservations:\nThis distribution shows a preference for Drama and Comedy in the dataset, while Horror appears less often.\n\n\n\nDecade:\n\nmovie_modified %&gt;% count(decade)\n\n# A tibble: 9 × 2\n  decade     n\n  &lt;fct&gt;  &lt;int&gt;\n1 1930       2\n2 1940       4\n3 1950       6\n4 1960      19\n5 1970      63\n6 1980     228\n7 1990     607\n8 2000    1387\n9 2010     994\n\n\n\nObservations:\nThe distribution indicates a clear trend of increasing movie releases from the 1930s to the 2000s, with a noticeable decline in earlier decades.\n\n\n\nProduction Budget:\n\nmovie_modified %&gt;%\n  gf_histogram(~production_budget)\n\n\n\n\n\n\n\n\n\n\nDomestic Gross:\n\nmovie_modified %&gt;%\n  gf_histogram(~domestic_gross)\n\n\n\n\n\n\n\n\n\n\nWorldwide Gross:\n\nmovie_modified %&gt;%\n  gf_histogram(~worldwide_gross)\n\n\n\n\n\n\n\n\n\nObservations:\nThe histograms of all the quantitative predictor variables are similarly skewed to the right. A rightward skew indicates that a majority of the data points are concentrated on the lower end of the scale, with fewer high values. This could suggest that most movies in my data-set have lower production budgets or grosses, while only a few have significantly higher values.The uniform skewness across multiple variables suggests a consistent pattern in the data-set, which can indicate underlying trends or characteristics shared among the movies"
  },
  {
    "objectID": "posts/CS1/index.html#define-the-question",
    "href": "posts/CS1/index.html#define-the-question",
    "title": "Case Study 1",
    "section": "Define the Question:",
    "text": "Define the Question:\nGraph:\n\nIdentifying the type of plot: A bar plot where a quantitative variable is plotted rather than a qualitative one.\nQuestions:\n\nHow do the distributor and genre affect a movie’s profitability?\nWhich genre is most likely to being the most profit for a distributor.\nIf i have a horror movie i want to bring to the theaters, which distributor would be my best bet?\n\n\nGraph Replication:\nI examined the variables plotted- We are plotting a quantitative variable against 2 qualitative variables. Has the profit ratio been transformed in any way to be used as a bar graph- one where the count of qualitative variables are plotted? A histogram is used to plot continuous quantitative data that could be faceted by groups that are defined by levels of a qualitative data. But is it possible to plot it against 2 qulaitative variables? No, You cannot meaningfully divide categories into bins, which is the core concept behind a histogram.\nMaybe i could use groups to understand this data? The x axis of the graph does say median profit ratio. Using box-plots, i do find the different medians.\n\nmovie_modified_again &lt;- movie_modified %&gt;%\n  dplyr::mutate(\n    genre = factor(genre,\n      levels = c(\"Action\", \"Adventure\", \"Comedy\", \"Drama\", \"Horror\" ),\n      labels = c(\"Action\", \"Adventure\", \"Comedy\", \"Drama\", \"Horror\"),\n      ordered = TRUE\n    )\n  )\n\n\nmovie_modified_again %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(genre, profit_ratio) ~ profit_ratio,\n    fill = ~genre,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(distributor)) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Profits made by film distributors\",\n    subtitle = \"Ratio of profits to budgets\",\n    x = \"Median profit ratio\",\n    y = \"genre\"\n  )\n\n\n\n\n\n\n\n\n\nmovie_modified_again %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(genre, profit_ratio) ~ log10(profit_ratio),\n    fill = ~genre,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(distributor)) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Profits made by film distributors\",\n    subtitle = \"Ratio of profits to budgets\",\n    x = \"Median profit ratio\",\n    y = \"genre\"\n  )\n\n\n\n\n\n\n\n\nAlthough bar graphs are commonly used to represent the count or frequency of qualitative (categorical) data. They can also be used to represent quantitative data if you summarize the data in some way (e.g., using means, sums, or other aggregates).\n\nggplot(movie_modified_again, aes(x = genre, y = profit_ratio)) +\n  facet_wrap(vars(distributor))+\n  stat_summary(fun = \"median\", geom = \"bar\") +\n  labs(title = \"Profits made by film distributors\", subtitle = \"Ratio of profits to budgets\", x = \"Genre\", y = \"Median Profit Ratio\")+  \n  coord_flip()\n\n\n\n\n\n\n\n\nI tried doing this with ggformula using stat = “summary” and fun = “median” but it didn’t work for some reason to i tried it out with ggplot2 and it did work.\nI know that I am only required to make this graph, but I’m curious about how to graph would look if i plot the mean instead of the median. Would it be easier to pull out inferences from the mean plot?\nfacet_wrap is used to obtain separate plots for each distributor.\n\nggplot(movie_modified_again, aes(x = genre, y = profit_ratio)) +\n  facet_wrap(vars(distributor))+\n  stat_summary(fun = \"mean\", geom = \"bar\") +\n  labs(title = \"Profits made by film distributors\", subtitle = \"Ratio of profits to budgets\", x = \"Genre\", y = \"Mean Profit Ratio\")+  \n  coord_flip()\n\n\n\n\n\n\n\n\nSo after a little bit of research, I realise, mean is more sensitive to outliers and extreme values, so it works well when the data is normally distributed and without significant skewness while Median is more robust in the presence of skewed data or outliers because it represents the middle value.\nWhen i observe the box plot, there are couple of outliers present of both sides making the data skewed, in which case, it could mislead the interpretation.Therefore, the median often provides a better sense of the “typical” value, making it easier to draw meaningful inferences.\n\n\nInferences:\n\nParamount pictures, Warner Bros and Sony Pictures seem to most likely gain the highest profits through horror movies.\nAlmost all distributors seem to make the lest amount of profits through drama movies (it’s a close second otherwise). This is interesting because if we looks at the count of all genres in the data-set, drama is the most. Is this a peculiarity of the data-set? If not, if it is most likely to bring in the least profit, why do they make it so much?\n\nThis makes me curious about the count of genre by distributors.\n\ngf_bar(~ genre | distributor, fill = ~genre, data = movie_modified) %&gt;%\n  gf_labs(title = \"Counts of genre by distributors\")\n\n\n\n\n\n\n\n\n\nThis is interesting because even though Paramount pictures, Warner Bros and Sony Pictures are most likely to gain highest profits through horror movies, these movies are of the least count in their data-set.\nFor Universal Studio and 20th Century Fox, adventure is the genre that’s most likely to bring in a high profit. Supporting this, 20th Century Fox does have a high count of adventure films, but Universal studio that has a higher median profit ratio for Adventure even compared to 20th Century fox, has a very low count (the second lowest they have) when it comes to adventure movies.\nWhile comedy brings in a good profit ratio for Universal Studio and and 20th Century Fox, not so much for any of the other distributors. What might that be about? The count of comedy films seem to be similar for all distributors despite this.\nIf I were to invest in movie production ventures, which are the two best genres that you might decide to invest in? It would be either horror or adventure. I’m most likely to gain profits with these 2 options."
  },
  {
    "objectID": "posts/CS1/index.html#my-story",
    "href": "posts/CS1/index.html#my-story",
    "title": "Case Study 1",
    "section": "My Story:",
    "text": "My Story:\nI have gone through the Case Studies provided in Arvind’s websites before the specifications of A2 came through. At that point we were two classes down and I remember feeling like everything was way out of my ballpark. I never looked at it again. So when we did get our A2 assignment, I panicked a little. When i did open case studies again, as i glimpsed through the preview images within each document, i jumped at clicking this movies profits one cause it looks like a bar plot. How hard could it possibly be?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Statistics and Data Analysis",
    "section": "",
    "text": "Experiment 4\n\n\n\n\n\n\nA3\n\n\nCartoons!\n\n\n\n\n\n\n\n\n\nOct 29, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 3\n\n\n\n\n\n\nA3\n\n\nGrades?\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 2\n\n\n\n\n\n\nA3\n\n\nTips?\n\n\n\n\n\n\n\n\n\nOct 25, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 1\n\n\n\n\n\n\nA3\n\n\nPocket Money\n\n\n\n\n\n\n\n\n\nOct 24, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 7\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 6 and 7\n\n\n\n\n\n\nStatistics\n\n\nCentral Limit Theorem\n\n\nrandamoisation\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study 3\n\n\n\n\n\n\nA2\n\n\nAntarctic-Sea-ice\n\n\nbox-plot\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study 2\n\n\n\n\n\n\nA2\n\n\nschool-scores\n\n\nscatter-plot\n\n\nbox-plot\n\n\n\n\n\n\n\n\n\nOct 12, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study 1\n\n\n\n\n\n\nA2\n\n\nmovie_profit\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5\n\n\n\n\n\n\ngroups\n\n\nbox-plot\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5 - Part 2\n\n\n\n\n\n\nchange\n\n\nscatter-plot\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4\n\n\n\n\n\n\nquantities\n\n\nhistogram\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3\n\n\n\n\n\n\ncounts\n\n\nBar-graph\n\n\ntaxi\n\n\naddiction\n\n\nbanned-books\n\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2\n\n\n\n\n\n\nsummaries\n\n\nmgp-data\n\n\nmath-anxiety-data\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\nSneha Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\n\n\n\nintro\n\n\nbabyname-analysis\n\n\nline-graph\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nSneha Jacob\n\n\n\n\n\n\n\n\n\n\n\n\n Facing the Abyss - WorkFLow\n\n\n\n\n\n\nEDA\n\n\nWorkflow\n\n\nDescriptive\n\n\n\nA complete EDA Workflow\n\n\n\n\n\nOct 21, 2023\n\n\nArvind V\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/cs2/index.html",
    "href": "posts/cs2/index.html",
    "title": "Case Study 2",
    "section": "",
    "text": "This dataset pertains to scores obtained by students in diverse subjects. Family Income is also part of this dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(easystats) \n\n# Attaching packages: easystats 0.7.3\n✔ bayestestR  0.14.0   ✔ correlation 0.8.5 \n✔ datawizard  0.13.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.5   ✔ modelbased  0.8.8 \n✔ performance 0.12.3   ✔ parameters  0.22.2\n✔ report      0.5.9    ✔ see         0.9.0 \n\nlibrary(correlation)"
  },
  {
    "objectID": "posts/cs2/index.html#introduction",
    "href": "posts/cs2/index.html#introduction",
    "title": "Case Study 2",
    "section": "",
    "text": "This dataset pertains to scores obtained by students in diverse subjects. Family Income is also part of this dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(easystats) \n\n# Attaching packages: easystats 0.7.3\n✔ bayestestR  0.14.0   ✔ correlation 0.8.5 \n✔ datawizard  0.13.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.5   ✔ modelbased  0.8.8 \n✔ performance 0.12.3   ✔ parameters  0.22.2\n✔ report      0.5.9    ✔ see         0.9.0 \n\nlibrary(correlation)"
  },
  {
    "objectID": "posts/cs2/index.html#playing-with-and-understanding-the-data",
    "href": "posts/cs2/index.html#playing-with-and-understanding-the-data",
    "title": "Case Study 2",
    "section": "Playing with and understanding the data",
    "text": "Playing with and understanding the data\n\nAcquiring the data:\n\nscores &lt;- read_csv(\"../../data/school-scores-data.csv\")\n\nRows: 577 Columns: 99\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): State.Code, State.Name\ndbl (97): Year, Total.Math, Total.Test-takers, Total.Verbal, Academic Subjec...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nscores\n\n# A tibble: 577 × 99\n    Year State.Code State.Name       Total.Math `Total.Test-takers` Total.Verbal\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1  2005 AL         Alabama                 559                3985          567\n 2  2005 AK         Alaska                  519                3996          523\n 3  2005 AZ         Arizona                 530               18184          526\n 4  2005 AR         Arkansas                552                1600          563\n 5  2005 CA         California              522              186552          504\n 6  2005 CO         Colorado                560               11990          560\n 7  2005 CT         Connecticut             517               34313          517\n 8  2005 DE         Delaware                502                6257          503\n 9  2005 DC         District Of Col…        478                3622          490\n10  2005 FL         Florida                 498               93505          498\n# ℹ 567 more rows\n# ℹ 93 more variables: `Academic Subjects.Arts/Music.Average GPA` &lt;dbl&gt;,\n#   `Academic Subjects.Arts/Music.Average Years` &lt;dbl&gt;,\n#   `Academic Subjects.English.Average GPA` &lt;dbl&gt;,\n#   `Academic Subjects.English.Average Years` &lt;dbl&gt;,\n#   `Academic Subjects.Foreign Languages.Average GPA` &lt;dbl&gt;,\n#   `Academic Subjects.Foreign Languages.Average Years` &lt;dbl&gt;, …\n\n\n\nglimpse(scores)\n\nRows: 577\nColumns: 99\n$ Year                                                      &lt;dbl&gt; 2005, 2005, …\n$ State.Code                                                &lt;chr&gt; \"AL\", \"AK\", …\n$ State.Name                                                &lt;chr&gt; \"Alabama\", \"…\n$ Total.Math                                                &lt;dbl&gt; 559, 519, 53…\n$ `Total.Test-takers`                                       &lt;dbl&gt; 3985, 3996, …\n$ Total.Verbal                                              &lt;dbl&gt; 567, 523, 52…\n$ `Academic Subjects.Arts/Music.Average GPA`                &lt;dbl&gt; 3.92, 3.76, …\n$ `Academic Subjects.Arts/Music.Average Years`              &lt;dbl&gt; 2.2, 1.9, 2.…\n$ `Academic Subjects.English.Average GPA`                   &lt;dbl&gt; 3.53, 3.35, …\n$ `Academic Subjects.English.Average Years`                 &lt;dbl&gt; 3.9, 3.9, 3.…\n$ `Academic Subjects.Foreign Languages.Average GPA`         &lt;dbl&gt; 3.54, 3.34, …\n$ `Academic Subjects.Foreign Languages.Average Years`       &lt;dbl&gt; 2.6, 2.1, 2.…\n$ `Academic Subjects.Mathematics.Average GPA`               &lt;dbl&gt; 3.41, 3.06, …\n$ `Academic Subjects.Mathematics.Average Years`             &lt;dbl&gt; 4.0, 3.5, 3.…\n$ `Academic Subjects.Natural Sciences.Average GPA`          &lt;dbl&gt; 3.52, 3.25, …\n$ `Academic Subjects.Natural Sciences.Average Years`        &lt;dbl&gt; 3.9, 3.2, 3.…\n$ `Academic Subjects.Social Sciences/History.Average GPA`   &lt;dbl&gt; 3.59, 3.39, …\n$ `Academic Subjects.Social Sciences/History.Average Years` &lt;dbl&gt; 3.9, 3.4, 3.…\n$ `Family Income.Between 20-40k.Math`                       &lt;dbl&gt; 513, 492, 49…\n$ `Family Income.Between 20-40k.Test-takers`                &lt;dbl&gt; 324, 401, 21…\n$ `Family Income.Between 20-40k.Verbal`                     &lt;dbl&gt; 527, 500, 49…\n$ `Family Income.Between 40-60k.Math`                       &lt;dbl&gt; 539, 517, 52…\n$ `Family Income.Between 40-60k.Test-takers`                &lt;dbl&gt; 442, 539, 22…\n$ `Family Income.Between 40-60k.Verbal`                     &lt;dbl&gt; 551, 522, 51…\n$ `Family Income.Between 60-80k.Math`                       &lt;dbl&gt; 550, 513, 52…\n$ `Family Income.Between 60-80k.Test-takers`                &lt;dbl&gt; 473, 603, 23…\n$ `Family Income.Between 60-80k.Verbal`                     &lt;dbl&gt; 564, 519, 52…\n$ `Family Income.Between 80-100k.Math`                      &lt;dbl&gt; 566, 528, 53…\n$ `Family Income.Between 80-100k.Test-takers`               &lt;dbl&gt; 475, 444, 18…\n$ `Family Income.Between 80-100k.Verbal`                    &lt;dbl&gt; 577, 534, 53…\n$ `Family Income.Less than 20k.Math`                        &lt;dbl&gt; 462, 464, 48…\n$ `Family Income.Less than 20k.Test-takers`                 &lt;dbl&gt; 175, 191, 89…\n$ `Family Income.Less than 20k.Verbal`                      &lt;dbl&gt; 474, 467, 47…\n$ `Family Income.More than 100k.Math`                       &lt;dbl&gt; 588, 541, 55…\n$ `Family Income.More than 100k.Test-takers`                &lt;dbl&gt; 980, 540, 30…\n$ `Family Income.More than 100k.Verbal`                     &lt;dbl&gt; 590, 544, 54…\n$ `GPA.A minus.Math`                                        &lt;dbl&gt; 569, 544, 54…\n$ `GPA.A minus.Test-takers`                                 &lt;dbl&gt; 724, 673, 33…\n$ `GPA.A minus.Verbal`                                      &lt;dbl&gt; 575, 546, 53…\n$ `GPA.A plus.Math`                                         &lt;dbl&gt; 622, 600, 60…\n$ `GPA.A plus.Test-takers`                                  &lt;dbl&gt; 563, 173, 16…\n$ `GPA.A plus.Verbal`                                       &lt;dbl&gt; 623, 604, 59…\n$ GPA.A.Math                                                &lt;dbl&gt; 600, 580, 57…\n$ `GPA.A.Test-takers`                                       &lt;dbl&gt; 1032, 671, 3…\n$ GPA.A.Verbal                                              &lt;dbl&gt; 608, 578, 56…\n$ GPA.B.Math                                                &lt;dbl&gt; 514, 492, 49…\n$ `GPA.B.Test-takers`                                       &lt;dbl&gt; 1253, 1622, …\n$ GPA.B.Verbal                                              &lt;dbl&gt; 525, 499, 49…\n$ GPA.C.Math                                                &lt;dbl&gt; 436, 466, 45…\n$ `GPA.C.Test-takers`                                       &lt;dbl&gt; 188, 418, 11…\n$ GPA.C.Verbal                                              &lt;dbl&gt; 451, 472, 46…\n$ `GPA.D or lower.Math`                                     &lt;dbl&gt; 0, 424, 439,…\n$ `GPA.D or lower.Test-takers`                              &lt;dbl&gt; 0, 12, 16, 0…\n$ `GPA.D or lower.Verbal`                                   &lt;dbl&gt; 0, 466, 435,…\n$ `GPA.No response.Math`                                    &lt;dbl&gt; 0, 0, 0, 0, …\n$ `GPA.No response.Test-takers`                             &lt;dbl&gt; 225, 427, 91…\n$ `GPA.No response.Verbal`                                  &lt;dbl&gt; 0, 0, 0, 0, …\n$ Gender.Female.Math                                        &lt;dbl&gt; 538, 505, 51…\n$ `Gender.Female.Test-takers`                               &lt;dbl&gt; 2072, 2161, …\n$ Gender.Female.Verbal                                      &lt;dbl&gt; 561, 521, 52…\n$ Gender.Male.Math                                          &lt;dbl&gt; 582, 535, 54…\n$ `Gender.Male.Test-takers`                                 &lt;dbl&gt; 1913, 1835, …\n$ Gender.Male.Verbal                                        &lt;dbl&gt; 574, 526, 53…\n$ `Score Ranges.Between 200 to 300.Math.Females`            &lt;dbl&gt; 22, 30, 119,…\n$ `Score Ranges.Between 200 to 300.Math.Males`              &lt;dbl&gt; 10, 20, 72, …\n$ `Score Ranges.Between 200 to 300.Math.Total`              &lt;dbl&gt; 32, 50, 191,…\n$ `Score Ranges.Between 200 to 300.Verbal.Females`          &lt;dbl&gt; 14, 26, 115,…\n$ `Score Ranges.Between 200 to 300.Verbal.Males`            &lt;dbl&gt; 17, 26, 86, …\n$ `Score Ranges.Between 200 to 300.Verbal.Total`            &lt;dbl&gt; 31, 52, 201,…\n$ `Score Ranges.Between 300 to 400.Math.Females`            &lt;dbl&gt; 173, 233, 88…\n$ `Score Ranges.Between 300 to 400.Math.Males`              &lt;dbl&gt; 93, 153, 450…\n$ `Score Ranges.Between 300 to 400.Math.Total`              &lt;dbl&gt; 266, 386, 13…\n$ `Score Ranges.Between 300 to 400.Verbal.Females`          &lt;dbl&gt; 123, 218, 73…\n$ `Score Ranges.Between 300 to 400.Verbal.Males`            &lt;dbl&gt; 84, 171, 613…\n$ `Score Ranges.Between 300 to 400.Verbal.Total`            &lt;dbl&gt; 207, 389, 13…\n$ `Score Ranges.Between 400 to 500.Math.Females`            &lt;dbl&gt; 514, 696, 32…\n$ `Score Ranges.Between 400 to 500.Math.Males`              &lt;dbl&gt; 293, 485, 19…\n$ `Score Ranges.Between 400 to 500.Math.Total`              &lt;dbl&gt; 807, 1181, 5…\n$ `Score Ranges.Between 400 to 500.Verbal.Females`          &lt;dbl&gt; 430, 656, 30…\n$ `Score Ranges.Between 400 to 500.Verbal.Males`            &lt;dbl&gt; 332, 552, 23…\n$ `Score Ranges.Between 400 to 500.Verbal.Total`            &lt;dbl&gt; 762, 1208, 5…\n$ `Score Ranges.Between 500 to 600.Math.Females`            &lt;dbl&gt; 722, 813, 35…\n$ `Score Ranges.Between 500 to 600.Math.Males`              &lt;dbl&gt; 614, 616, 31…\n$ `Score Ranges.Between 500 to 600.Math.Total`              &lt;dbl&gt; 1336, 1429, …\n$ `Score Ranges.Between 500 to 600.Verbal.Females`          &lt;dbl&gt; 690, 729, 36…\n$ `Score Ranges.Between 500 to 600.Verbal.Males`            &lt;dbl&gt; 617, 596, 31…\n$ `Score Ranges.Between 500 to 600.Verbal.Total`            &lt;dbl&gt; 1307, 1325, …\n$ `Score Ranges.Between 600 to 700.Math.Females`            &lt;dbl&gt; 485, 342, 16…\n$ `Score Ranges.Between 600 to 700.Math.Males`              &lt;dbl&gt; 611, 445, 21…\n$ `Score Ranges.Between 600 to 700.Math.Total`              &lt;dbl&gt; 1096, 787, 3…\n$ `Score Ranges.Between 600 to 700.Verbal.Females`          &lt;dbl&gt; 596, 423, 18…\n$ `Score Ranges.Between 600 to 700.Verbal.Males`            &lt;dbl&gt; 613, 375, 16…\n$ `Score Ranges.Between 600 to 700.Verbal.Total`            &lt;dbl&gt; 1209, 798, 3…\n$ `Score Ranges.Between 700 to 800.Math.Females`            &lt;dbl&gt; 156, 47, 327…\n$ `Score Ranges.Between 700 to 800.Math.Males`              &lt;dbl&gt; 292, 116, 63…\n$ `Score Ranges.Between 700 to 800.Math.Total`              &lt;dbl&gt; 448, 163, 95…\n$ `Score Ranges.Between 700 to 800.Verbal.Females`          &lt;dbl&gt; 219, 109, 41…\n$ `Score Ranges.Between 700 to 800.Verbal.Males`            &lt;dbl&gt; 250, 115, 50…\n$ `Score Ranges.Between 700 to 800.Verbal.Total`            &lt;dbl&gt; 469, 224, 91…\n\n\nThe data doesn’t seem right. How are there 99 columns?\n\nUse the janitor package here to clean up the variable names. Try to use the big_camel case name format for variables.\n\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following object is masked from 'package:insight':\n\n    clean_names\n\n\nThe following objects are masked from 'package:datawizard':\n\n    remove_empty, remove_empty_rows\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n\nscores_cleaned &lt;- clean_names(scores, case = \"big_camel\")\nscores_cleaned\n\n# A tibble: 577 × 99\n    Year StateCode StateName            TotalMath TotalTestTakers TotalVerbal\n   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                    &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1  2005 AL        Alabama                    559            3985         567\n 2  2005 AK        Alaska                     519            3996         523\n 3  2005 AZ        Arizona                    530           18184         526\n 4  2005 AR        Arkansas                   552            1600         563\n 5  2005 CA        California                 522          186552         504\n 6  2005 CO        Colorado                   560           11990         560\n 7  2005 CT        Connecticut                517           34313         517\n 8  2005 DE        Delaware                   502            6257         503\n 9  2005 DC        District Of Columbia       478            3622         490\n10  2005 FL        Florida                    498           93505         498\n# ℹ 567 more rows\n# ℹ 93 more variables: AcademicSubjectsArtsMusicAverageGpa &lt;dbl&gt;,\n#   AcademicSubjectsArtsMusicAverageYears &lt;dbl&gt;,\n#   AcademicSubjectsEnglishAverageGpa &lt;dbl&gt;,\n#   AcademicSubjectsEnglishAverageYears &lt;dbl&gt;,\n#   AcademicSubjectsForeignLanguagesAverageGpa &lt;dbl&gt;,\n#   AcademicSubjectsForeignLanguagesAverageYears &lt;dbl&gt;, …\n\n\n\nglimpse(scores_cleaned)\n\nRows: 577\nColumns: 99\n$ Year                                              &lt;dbl&gt; 2005, 2005, 2005, 20…\n$ StateCode                                         &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"A…\n$ StateName                                         &lt;chr&gt; \"Alabama\", \"Alaska\",…\n$ TotalMath                                         &lt;dbl&gt; 559, 519, 530, 552, …\n$ TotalTestTakers                                   &lt;dbl&gt; 3985, 3996, 18184, 1…\n$ TotalVerbal                                       &lt;dbl&gt; 567, 523, 526, 563, …\n$ AcademicSubjectsArtsMusicAverageGpa               &lt;dbl&gt; 3.92, 3.76, 3.85, 3.…\n$ AcademicSubjectsArtsMusicAverageYears             &lt;dbl&gt; 2.2, 1.9, 2.1, 2.2, …\n$ AcademicSubjectsEnglishAverageGpa                 &lt;dbl&gt; 3.53, 3.35, 3.45, 3.…\n$ AcademicSubjectsEnglishAverageYears               &lt;dbl&gt; 3.9, 3.9, 3.9, 4.0, …\n$ AcademicSubjectsForeignLanguagesAverageGpa        &lt;dbl&gt; 3.54, 3.34, 3.41, 3.…\n$ AcademicSubjectsForeignLanguagesAverageYears      &lt;dbl&gt; 2.6, 2.1, 2.6, 2.6, …\n$ AcademicSubjectsMathematicsAverageGpa             &lt;dbl&gt; 3.41, 3.06, 3.25, 3.…\n$ AcademicSubjectsMathematicsAverageYears           &lt;dbl&gt; 4.0, 3.5, 3.9, 4.1, …\n$ AcademicSubjectsNaturalSciencesAverageGpa         &lt;dbl&gt; 3.52, 3.25, 3.43, 3.…\n$ AcademicSubjectsNaturalSciencesAverageYears       &lt;dbl&gt; 3.9, 3.2, 3.4, 3.7, …\n$ AcademicSubjectsSocialSciencesHistoryAverageGpa   &lt;dbl&gt; 3.59, 3.39, 3.55, 3.…\n$ AcademicSubjectsSocialSciencesHistoryAverageYears &lt;dbl&gt; 3.9, 3.4, 3.3, 3.6, …\n$ FamilyIncomeBetween20_40KMath                     &lt;dbl&gt; 513, 492, 498, 513, …\n$ FamilyIncomeBetween20_40KTestTakers               &lt;dbl&gt; 324, 401, 2121, 180,…\n$ FamilyIncomeBetween20_40KVerbal                   &lt;dbl&gt; 527, 500, 495, 526, …\n$ FamilyIncomeBetween40_60KMath                     &lt;dbl&gt; 539, 517, 520, 543, …\n$ FamilyIncomeBetween40_60KTestTakers               &lt;dbl&gt; 442, 539, 2270, 245,…\n$ FamilyIncomeBetween40_60KVerbal                   &lt;dbl&gt; 551, 522, 518, 555, …\n$ FamilyIncomeBetween60_80KMath                     &lt;dbl&gt; 550, 513, 524, 553, …\n$ FamilyIncomeBetween60_80KTestTakers               &lt;dbl&gt; 473, 603, 2372, 227,…\n$ FamilyIncomeBetween60_80KVerbal                   &lt;dbl&gt; 564, 519, 523, 570, …\n$ FamilyIncomeBetween80_100KMath                    &lt;dbl&gt; 566, 528, 534, 570, …\n$ FamilyIncomeBetween80_100KTestTakers              &lt;dbl&gt; 475, 444, 1866, 147,…\n$ FamilyIncomeBetween80_100KVerbal                  &lt;dbl&gt; 577, 534, 533, 580, …\n$ FamilyIncomeLessThan20KMath                       &lt;dbl&gt; 462, 464, 485, 489, …\n$ FamilyIncomeLessThan20KTestTakers                 &lt;dbl&gt; 175, 191, 891, 107, …\n$ FamilyIncomeLessThan20KVerbal                     &lt;dbl&gt; 474, 467, 474, 486, …\n$ FamilyIncomeMoreThan100KMath                      &lt;dbl&gt; 588, 541, 554, 572, …\n$ FamilyIncomeMoreThan100KTestTakers                &lt;dbl&gt; 980, 540, 3083, 314,…\n$ FamilyIncomeMoreThan100KVerbal                    &lt;dbl&gt; 590, 544, 546, 589, …\n$ GpaAMinusMath                                     &lt;dbl&gt; 569, 544, 541, 559, …\n$ GpaAMinusTestTakers                               &lt;dbl&gt; 724, 673, 3334, 298,…\n$ GpaAMinusVerbal                                   &lt;dbl&gt; 575, 546, 535, 572, …\n$ GpaAPlusMath                                      &lt;dbl&gt; 622, 600, 605, 629, …\n$ GpaAPlusTestTakers                                &lt;dbl&gt; 563, 173, 1684, 273,…\n$ GpaAPlusVerbal                                    &lt;dbl&gt; 623, 604, 593, 639, …\n$ GpaAMath                                          &lt;dbl&gt; 600, 580, 571, 579, …\n$ GpaATestTakers                                    &lt;dbl&gt; 1032, 671, 3854, 457…\n$ GpaAVerbal                                        &lt;dbl&gt; 608, 578, 563, 583, …\n$ GpaBMath                                          &lt;dbl&gt; 514, 492, 498, 492, …\n$ GpaBTestTakers                                    &lt;dbl&gt; 1253, 1622, 7193, 43…\n$ GpaBVerbal                                        &lt;dbl&gt; 525, 499, 499, 511, …\n$ GpaCMath                                          &lt;dbl&gt; 436, 466, 458, 419, …\n$ GpaCTestTakers                                    &lt;dbl&gt; 188, 418, 1184, 57, …\n$ GpaCVerbal                                        &lt;dbl&gt; 451, 472, 464, 436, …\n$ GpaDOrLowerMath                                   &lt;dbl&gt; 0, 424, 439, 0, 419,…\n$ GpaDOrLowerTestTakers                             &lt;dbl&gt; 0, 12, 16, 0, 240, 1…\n$ GpaDOrLowerVerbal                                 &lt;dbl&gt; 0, 466, 435, 0, 408,…\n$ GpaNoResponseMath                                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ GpaNoResponseTestTakers                           &lt;dbl&gt; 225, 427, 919, 78, 1…\n$ GpaNoResponseVerbal                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ GenderFemaleMath                                  &lt;dbl&gt; 538, 505, 513, 536, …\n$ GenderFemaleTestTakers                            &lt;dbl&gt; 2072, 2161, 9806, 85…\n$ GenderFemaleVerbal                                &lt;dbl&gt; 561, 521, 522, 558, …\n$ GenderMaleMath                                    &lt;dbl&gt; 582, 535, 549, 570, …\n$ GenderMaleTestTakers                              &lt;dbl&gt; 1913, 1835, 8378, 74…\n$ GenderMaleVerbal                                  &lt;dbl&gt; 574, 526, 531, 570, …\n$ ScoreRangesBetween200To300MathFemales             &lt;dbl&gt; 22, 30, 119, 12, 297…\n$ ScoreRangesBetween200To300MathMales               &lt;dbl&gt; 10, 20, 72, 7, 1453,…\n$ ScoreRangesBetween200To300MathTotal               &lt;dbl&gt; 32, 50, 191, 19, 443…\n$ ScoreRangesBetween200To300VerbalFemales           &lt;dbl&gt; 14, 26, 115, 9, 3382…\n$ ScoreRangesBetween200To300VerbalMales             &lt;dbl&gt; 17, 26, 86, 3, 2433,…\n$ ScoreRangesBetween200To300VerbalTotal             &lt;dbl&gt; 31, 52, 201, 12, 581…\n$ ScoreRangesBetween300To400MathFemales             &lt;dbl&gt; 173, 233, 881, 68, 1…\n$ ScoreRangesBetween300To400MathMales               &lt;dbl&gt; 93, 153, 450, 31, 71…\n$ ScoreRangesBetween300To400MathTotal               &lt;dbl&gt; 266, 386, 1331, 99, …\n$ ScoreRangesBetween300To400VerbalFemales           &lt;dbl&gt; 123, 218, 739, 46, 1…\n$ ScoreRangesBetween300To400VerbalMales             &lt;dbl&gt; 84, 171, 613, 42, 10…\n$ ScoreRangesBetween300To400VerbalTotal             &lt;dbl&gt; 207, 389, 1352, 88, …\n$ ScoreRangesBetween400To500MathFemales             &lt;dbl&gt; 514, 696, 3215, 210,…\n$ ScoreRangesBetween400To500MathMales               &lt;dbl&gt; 293, 485, 1948, 137,…\n$ ScoreRangesBetween400To500MathTotal               &lt;dbl&gt; 807, 1181, 5163, 347…\n$ ScoreRangesBetween400To500VerbalFemales           &lt;dbl&gt; 430, 656, 3048, 183,…\n$ ScoreRangesBetween400To500VerbalMales             &lt;dbl&gt; 332, 552, 2398, 141,…\n$ ScoreRangesBetween400To500VerbalTotal             &lt;dbl&gt; 762, 1208, 5446, 324…\n$ ScoreRangesBetween500To600MathFemales             &lt;dbl&gt; 722, 813, 3576, 316,…\n$ ScoreRangesBetween500To600MathMales               &lt;dbl&gt; 614, 616, 3152, 244,…\n$ ScoreRangesBetween500To600MathTotal               &lt;dbl&gt; 1336, 1429, 6728, 56…\n$ ScoreRangesBetween500To600VerbalFemales           &lt;dbl&gt; 690, 729, 3661, 302,…\n$ ScoreRangesBetween500To600VerbalMales             &lt;dbl&gt; 617, 596, 3101, 236,…\n$ ScoreRangesBetween500To600VerbalTotal             &lt;dbl&gt; 1307, 1325, 6762, 53…\n$ ScoreRangesBetween600To700MathFemales             &lt;dbl&gt; 485, 342, 1688, 204,…\n$ ScoreRangesBetween600To700MathMales               &lt;dbl&gt; 611, 445, 2126, 239,…\n$ ScoreRangesBetween600To700MathTotal               &lt;dbl&gt; 1096, 787, 3814, 443…\n$ ScoreRangesBetween600To700VerbalFemales           &lt;dbl&gt; 596, 423, 1831, 242,…\n$ ScoreRangesBetween600To700VerbalMales             &lt;dbl&gt; 613, 375, 1679, 226,…\n$ ScoreRangesBetween600To700VerbalTotal             &lt;dbl&gt; 1209, 798, 3510, 468…\n$ ScoreRangesBetween700To800MathFemales             &lt;dbl&gt; 156, 47, 327, 49, 54…\n$ ScoreRangesBetween700To800MathMales               &lt;dbl&gt; 292, 116, 630, 83, 8…\n$ ScoreRangesBetween700To800MathTotal               &lt;dbl&gt; 448, 163, 957, 132, …\n$ ScoreRangesBetween700To800VerbalFemales           &lt;dbl&gt; 219, 109, 412, 77, 5…\n$ ScoreRangesBetween700To800VerbalMales             &lt;dbl&gt; 250, 115, 501, 93, 4…\n$ ScoreRangesBetween700To800VerbalTotal             &lt;dbl&gt; 469, 224, 913, 170, …\n\n\n\nskim(scores_cleaned)\n\n\nData summary\n\n\nName\nscores_cleaned\n\n\nNumber of rows\n577\n\n\nNumber of columns\n99\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n97\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nStateCode\n0\n1\n2\n2\n0\n53\n0\n\n\nStateName\n0\n1\n4\n20\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1\n2010.02\n3.17\n2005.00\n2007.00\n2010.00\n2013.00\n2015.00\n▇▅▅▆▆\n\n\nTotalMath\n0\n1\n535.68\n46.17\n383.00\n504.00\n527.00\n571.00\n619.00\n▁▁▇▆▅\n\n\nTotalTestTakers\n0\n1\n27914.24\n45602.11\n134.00\n2536.00\n6468.00\n35799.00\n241553.00\n▇▁▁▁▁\n\n\nTotalVerbal\n0\n1\n531.33\n44.32\n401.00\n496.00\n522.00\n572.00\n612.00\n▁▂▇▃▅\n\n\nAcademicSubjectsArtsMusicAverageGpa\n0\n1\n3.82\n0.09\n3.43\n3.76\n3.85\n3.90\n3.96\n▁▁▂▆▇\n\n\nAcademicSubjectsArtsMusicAverageYears\n0\n1\n2.29\n0.32\n1.20\n2.10\n2.30\n2.50\n3.10\n▁▂▇▅▂\n\n\nAcademicSubjectsEnglishAverageGpa\n0\n1\n3.50\n0.19\n3.03\n3.35\n3.51\n3.67\n3.88\n▂▆▇▇▃\n\n\nAcademicSubjectsEnglishAverageYears\n0\n1\n3.93\n0.09\n3.50\n3.90\n3.90\n4.00\n4.10\n▁▁▂▇▇\n\n\nAcademicSubjectsForeignLanguagesAverageGpa\n0\n1\n3.45\n0.19\n3.03\n3.30\n3.46\n3.63\n3.79\n▃▇▇▇▇\n\n\nAcademicSubjectsForeignLanguagesAverageYears\n0\n1\n2.85\n0.34\n1.80\n2.60\n2.80\n3.10\n3.60\n▁▅▆▇▃\n\n\nAcademicSubjectsMathematicsAverageGpa\n0\n1\n3.31\n0.22\n2.85\n3.12\n3.30\n3.51\n3.76\n▂▇▅▇▃\n\n\nAcademicSubjectsMathematicsAverageYears\n0\n1\n3.94\n0.17\n3.20\n3.80\n3.90\n4.10\n4.40\n▁▁▇▆▂\n\n\nAcademicSubjectsNaturalSciencesAverageGpa\n0\n1\n3.42\n0.20\n2.87\n3.25\n3.42\n3.60\n3.82\n▁▆▇▇▅\n\n\nAcademicSubjectsNaturalSciencesAverageYears\n0\n1\n3.63\n0.20\n2.80\n3.50\n3.60\n3.80\n4.20\n▁▁▇▇▁\n\n\nAcademicSubjectsSocialSciencesHistoryAverageGpa\n0\n1\n3.52\n0.18\n3.05\n3.38\n3.53\n3.68\n3.88\n▁▆▆▇▅\n\n\nAcademicSubjectsSocialSciencesHistoryAverageYears\n0\n1\n3.62\n0.18\n3.00\n3.50\n3.60\n3.70\n4.00\n▁▃▇▇▂\n\n\nFamilyIncomeBetween20_40KMath\n0\n1\n500.24\n48.46\n0.00\n471.00\n495.00\n533.00\n643.00\n▁▁▁▇▅\n\n\nFamilyIncomeBetween20_40KTestTakers\n0\n1\n3234.18\n5935.05\n5.00\n214.00\n580.00\n3179.00\n35446.00\n▇▁▁▁▁\n\n\nFamilyIncomeBetween20_40KVerbal\n0\n1\n501.33\n43.81\n387.00\n466.00\n496.00\n536.00\n634.00\n▁▇▇▅▁\n\n\nFamilyIncomeBetween40_60KMath\n0\n1\n522.84\n43.08\n381.00\n493.00\n519.00\n554.00\n629.00\n▁▂▇▆▂\n\n\nFamilyIncomeBetween40_60KTestTakers\n0\n1\n2847.20\n4638.14\n10.00\n236.00\n636.00\n3386.00\n28124.00\n▇▁▁▁▁\n\n\nFamilyIncomeBetween40_60KVerbal\n0\n1\n523.08\n41.95\n414.00\n489.00\n517.00\n558.00\n628.00\n▁▇▇▇▂\n\n\nFamilyIncomeBetween60_80KMath\n0\n1\n533.61\n42.50\n249.00\n506.00\n531.00\n564.00\n630.00\n▁▁▁▇▅\n\n\nFamilyIncomeBetween60_80KTestTakers\n0\n1\n2269.76\n3535.33\n8.00\n199.00\n523.00\n2791.00\n17937.00\n▇▁▁▁▁\n\n\nFamilyIncomeBetween60_80KVerbal\n0\n1\n533.85\n43.42\n232.00\n501.00\n527.00\n571.00\n645.00\n▁▁▁▇▅\n\n\nFamilyIncomeBetween80_100KMath\n0\n1\n547.06\n38.37\n398.00\n519.00\n539.00\n575.00\n646.00\n▁▁▇▅▂\n\n\nFamilyIncomeBetween80_100KTestTakers\n0\n1\n1803.63\n2855.65\n5.00\n164.00\n441.00\n2130.00\n15358.00\n▇▁▁▁▁\n\n\nFamilyIncomeBetween80_100KVerbal\n0\n1\n544.58\n38.07\n433.00\n514.00\n536.00\n580.00\n651.00\n▁▇▇▇▁\n\n\nFamilyIncomeLessThan20KMath\n0\n1\n461.73\n79.25\n0.00\n438.00\n465.00\n510.00\n589.00\n▁▁▁▇▇\n\n\nFamilyIncomeLessThan20KTestTakers\n0\n1\n2433.32\n5254.03\n1.00\n124.00\n347.00\n2129.00\n42551.00\n▇▁▁▁▁\n\n\nFamilyIncomeLessThan20KVerbal\n0\n1\n458.43\n67.02\n0.00\n429.00\n464.00\n501.00\n579.00\n▁▁▁▇▇\n\n\nFamilyIncomeMoreThan100KMath\n0\n1\n565.83\n42.14\n0.00\n548.00\n565.00\n587.00\n637.00\n▁▁▁▁▇\n\n\nFamilyIncomeMoreThan100KTestTakers\n0\n1\n4141.35\n7004.94\n2.00\n427.00\n1000.00\n4405.00\n46127.00\n▇▁▁▁▁\n\n\nFamilyIncomeMoreThan100KVerbal\n0\n1\n560.49\n42.59\n0.00\n538.00\n555.00\n590.00\n637.00\n▁▁▁▁▇\n\n\nGpaAMinusMath\n0\n1\n550.03\n39.00\n0.00\n532.00\n552.00\n569.00\n619.00\n▁▁▁▁▇\n\n\nGpaAMinusTestTakers\n0\n1\n4954.84\n8126.75\n0.00\n460.00\n1217.00\n6372.00\n45869.00\n▇▁▁▁▁\n\n\nGpaAMinusVerbal\n0\n1\n543.72\n36.54\n0.00\n526.00\n547.00\n566.00\n609.00\n▁▁▁▁▇\n\n\nGpaAPlusMath\n0\n1\n621.70\n42.97\n0.00\n607.00\n624.00\n644.00\n683.00\n▁▁▁▁▇\n\n\nGpaAPlusTestTakers\n0\n1\n1592.20\n2384.09\n0.00\n274.00\n524.00\n1792.00\n12184.00\n▇▁▁▁▁\n\n\nGpaAPlusVerbal\n0\n1\n613.05\n41.44\n0.00\n595.00\n616.00\n637.00\n672.00\n▁▁▁▁▇\n\n\nGpaAMath\n0\n1\n584.99\n41.79\n0.00\n565.00\n588.00\n605.00\n655.00\n▁▁▁▁▇\n\n\nGpaATestTakers\n0\n1\n4925.42\n7645.59\n0.00\n680.00\n1390.00\n6112.00\n42656.00\n▇▁▁▁▁\n\n\nGpaAVerbal\n0\n1\n576.85\n39.54\n0.00\n556.00\n580.00\n600.00\n637.00\n▁▁▁▁▇\n\n\nGpaBMath\n0\n1\n490.50\n37.77\n0.00\n472.00\n492.00\n511.00\n564.00\n▁▁▁▁▇\n\n\nGpaBTestTakers\n0\n1\n11728.67\n19924.44\n0.00\n676.00\n2282.00\n14745.00\n104693.00\n▇▁▁▁▁\n\n\nGpaBVerbal\n0\n1\n490.98\n36.59\n0.00\n470.00\n493.00\n517.00\n562.00\n▁▁▁▁▇\n\n\nGpaCMath\n0\n1\n426.22\n70.60\n0.00\n413.00\n436.00\n457.00\n553.00\n▁▁▁▇▆\n\n\nGpaCTestTakers\n0\n1\n2619.74\n4602.10\n0.00\n93.00\n445.00\n3060.00\n22802.00\n▇▂▁▁▁\n\n\nGpaCVerbal\n0\n1\n431.55\n71.62\n0.00\n415.00\n440.00\n464.00\n548.00\n▁▁▁▇▇\n\n\nGpaDOrLowerMath\n0\n1\n266.64\n209.25\n0.00\n0.00\n389.00\n428.00\n648.00\n▆▁▂▇▁\n\n\nGpaDOrLowerTestTakers\n0\n1\n90.76\n235.37\n0.00\n2.00\n12.00\n90.00\n2061.00\n▇▁▁▁▁\n\n\nGpaDOrLowerVerbal\n0\n1\n272.36\n211.13\n0.00\n0.00\n394.00\n432.00\n632.00\n▆▁▁▇▁\n\n\nGpaNoResponseMath\n0\n1\n304.01\n236.34\n0.00\n0.00\n446.00\n496.00\n589.00\n▇▁▁▆▇\n\n\nGpaNoResponseTestTakers\n0\n1\n1953.19\n3595.06\n0.00\n107.00\n399.00\n2206.00\n26744.00\n▇▁▁▁▁\n\n\nGpaNoResponseVerbal\n0\n1\n315.56\n245.70\n0.00\n0.00\n462.00\n510.00\n616.00\n▇▁▁▆▇\n\n\nGenderFemaleMath\n0\n1\n518.42\n44.22\n368.00\n488.00\n510.00\n551.00\n611.00\n▁▁▇▅▃\n\n\nGenderFemaleTestTakers\n0\n1\n15011.61\n24667.80\n73.00\n1357.00\n3428.00\n18698.00\n133217.00\n▇▁▁▁▁\n\n\nGenderFemaleVerbal\n0\n1\n528.35\n43.47\n399.00\n493.00\n519.00\n569.00\n611.00\n▁▂▇▃▅\n\n\nGenderMaleMath\n0\n1\n553.91\n48.39\n394.00\n521.00\n546.00\n592.00\n640.00\n▁▁▇▆▅\n\n\nGenderMaleTestTakers\n0\n1\n12911.25\n20959.55\n61.00\n1177.00\n2979.00\n16718.00\n108336.00\n▇▁▁▁▁\n\n\nGenderMaleVerbal\n0\n1\n534.88\n45.41\n403.00\n499.00\n525.00\n577.00\n635.00\n▁▃▇▆▃\n\n\nScoreRangesBetween200To300MathFemales\n0\n1\n441.30\n779.45\n0.00\n12.00\n102.00\n518.00\n4294.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300MathMales\n0\n1\n308.42\n509.50\n0.00\n8.00\n60.00\n468.00\n3034.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300MathTotal\n0\n1\n705.21\n1281.79\n0.00\n20.00\n162.00\n628.00\n6772.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300VerbalFemales\n0\n1\n387.74\n793.35\n0.00\n12.00\n46.00\n357.00\n5111.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300VerbalMales\n0\n1\n651.10\n1830.08\n0.00\n14.00\n83.00\n570.00\n20348.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300VerbalTotal\n0\n1\n752.66\n1514.44\n0.00\n26.00\n74.00\n790.00\n10603.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400MathFemales\n0\n1\n2180.50\n3961.85\n1.00\n95.00\n599.00\n1972.00\n24977.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400MathMales\n0\n1\n1278.97\n2396.51\n1.00\n57.00\n144.00\n1389.00\n13740.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400MathTotal\n0\n1\n3450.22\n6353.18\n0.00\n149.00\n937.00\n3288.00\n38161.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400VerbalFemales\n0\n1\n2017.92\n4037.56\n1.00\n52.00\n206.00\n1827.00\n22544.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400VerbalMales\n0\n1\n1956.98\n3578.36\n1.00\n74.00\n368.00\n2081.00\n26188.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400VerbalTotal\n0\n1\n3669.70\n7235.21\n2.00\n110.00\n394.00\n3530.00\n41262.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500MathFemales\n0\n1\n4597.65\n8104.31\n0.00\n333.00\n670.00\n5262.00\n43758.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500MathMales\n0\n1\n3142.77\n5642.54\n0.00\n125.00\n442.00\n3412.00\n29254.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500MathTotal\n0\n1\n7737.70\n13748.63\n0.00\n493.00\n1096.00\n8698.00\n73012.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500VerbalFemales\n0\n1\n4538.66\n8240.61\n1.00\n198.00\n595.00\n5082.00\n45918.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500VerbalMales\n0\n1\n5540.67\n13135.33\n2.00\n223.00\n892.00\n5351.00\n164622.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500VerbalTotal\n0\n1\n8190.33\n14833.61\n0.00\n354.00\n1063.00\n9280.00\n80535.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600MathFemales\n0\n1\n4332.81\n6939.66\n4.00\n369.00\n1006.00\n5593.00\n35778.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600MathMales\n0\n1\n3790.29\n6212.88\n3.00\n292.00\n798.00\n5163.00\n31702.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600MathTotal\n0\n1\n8125.04\n13139.57\n6.00\n651.00\n1830.00\n10753.00\n67480.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600VerbalFemales\n0\n1\n4350.62\n6945.72\n4.00\n356.00\n995.00\n5835.00\n37455.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600VerbalMales\n0\n1\n3760.08\n6010.78\n4.00\n318.00\n871.00\n5341.00\n31449.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600VerbalTotal\n0\n1\n8112.57\n12954.97\n1.00\n663.00\n1888.00\n11266.00\n68869.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700MathFemales\n0\n1\n2888.34\n5908.86\n10.00\n284.00\n699.00\n3242.00\n66431.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700MathMales\n0\n1\n3166.88\n5530.24\n15.00\n328.00\n732.00\n4178.00\n49941.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700MathTotal\n0\n1\n6055.63\n11358.02\n26.00\n609.00\n1462.00\n7308.00\n116372.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700VerbalFemales\n0\n1\n2914.30\n5949.39\n13.00\n319.00\n718.00\n3329.00\n71360.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700VerbalMales\n0\n1\n2677.58\n5129.55\n10.00\n302.00\n672.00\n3215.00\n56513.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700VerbalTotal\n0\n1\n5592.36\n11069.17\n23.00\n617.00\n1383.00\n6521.00\n127873.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800MathFemales\n0\n1\n792.63\n1787.20\n2.00\n83.00\n223.00\n821.00\n24126.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800MathMales\n0\n1\n1306.64\n2557.59\n1.00\n163.00\n406.00\n1475.00\n30815.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800MathTotal\n0\n1\n2099.26\n4334.33\n1.00\n251.00\n645.00\n2301.00\n54941.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800VerbalFemales\n0\n1\n849.82\n1665.76\n2.00\n123.00\n295.00\n987.00\n21826.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800VerbalMales\n0\n1\n847.27\n1625.23\n2.00\n121.00\n295.00\n970.00\n20460.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800VerbalTotal\n0\n1\n1697.12\n3289.67\n4.00\n246.00\n605.00\n1971.00\n42286.00\n▇▁▁▁▁\n\n\n\n\n\n\n\nData Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nYear\nRefers to the year the data was collected\nQualitative\n\n\nStateCode\nRepresent the standardized code for U.S. states\nQualitative\n\n\nTotalTestTakers\nThis column likely shows the total number of students who took the standardized test in each state for that year.\nQuantitative\n\n\nTotal”Subject”\nThis likely represents the average “Subject” score of students in each state, each year.\nQuantitative\n\n\nAcademicSubjects”Subject”AverageGpa\nThis likely represents the average GPA (Grade Point Average) of students in a “subject” for each state, each year.\nQuantitative\n\n\nAcademicSubjects“Subject”AverageYears\nThis column could represent the average number of years students spent studying “subject”, possibly during high school for each state, each year.\nQuantitative\n\n\nFamilyIncomeBetween“Number1_Number2”K“Subject”\nThe average “Subject” score of students whose family income falls between $“Number1” thousand and $“Number2” thousand annually each state, each year.\nQuantitative\n\n\nFamilyIncomeBetween“Number1_Number2”KTestTakers\nThe number of students from families earning between $“Number1” thousand and $“Number2” thousand who took the test.\nQuantitative\n\n\nFamilyIncome“LessThan/MoreThan”“Number”KTestTakers\nThe number of students from families earning Less than or more than $“Number” thousand anually\nQuantitative\n\n\nFamilyIncome”LessThan/MoreThan”“Number”K“Subject”\nThe average “Subject” score of students whose family income falls is less than or more than $“Number” thousand annually\nQuantitative\n\n\nGpa“Grade”“Subject”\nThe average “Subject” score of students with a GPA grade of “Grade”.\nQuantitative\n\n\nGpa“Grade”TestTakers\nThe number of students with a GPA of “Grade” who took the test.\nQuantitative\n\n\nGpaDOrLower“Subject”\nThe average “Subject” score of students with a GPA grade of D or lower.\nQuantitative\n\n\nGpaDOrLowerTestTakers\nThe number of students with a GPA of D or lower who took the test.\nQuantitative\n\n\nGpaNoResponse“Subject”\nThe average “Subject” score of students who did not report their GPA.\nQuantitative\n\n\nGpaNoResponseTestTakers\nThe number of students who did not report their GPA and took the test.\nQuantitative\n\n\nGender“Gender”“Subject”\nThe average “Subject” score of “Gender” students.\nQuantitative\n\n\nGender“Gender”TestTakers\nThe number of “Gender” students who took the test.\nQuantitative\n\n\nScoreRangesBetween“Number1”To“Number2”“Subject”“Gender”\nThe number of “Gender” students whose “Subject” scores fall between “Number1” and “Number2”.\nQuantitative\n\n\nScoreRangesBetween“Number1”To“Number2”“Subject”Total\nThe total number of students (both female and male) whose “Subject” scores fall between “Number1” and “Number2”.\nQuantitative\n\n\n\nI will admit i was over whelmed by the amount of data (in particular the amount of columns in this data set. But i think I’ve gotten a hand of it)\n\nObservations:\n\nThe variables Year, StateCode, and StateName are qualitative (categorical) data but they have a large number of unique levels, meaning they can’t be treated as factors. Maybe Year can be? It only have 10 different levels.\nRest all variables are quantitative either representing average GPA, number of students or average score.\nThere are no missing values for any variable.\n\n\n\n\nTarget and Predictor Variables:\nThe target variables could be either the Math score (TotalMath) or the Verbal score (TotalVerbal). You could treat them as separate target variables or focus on just one. Leaving the rest of the variables to be predictor variables."
  },
  {
    "objectID": "posts/cs2/index.html#defining-the-research-experiment",
    "href": "posts/cs2/index.html#defining-the-research-experiment",
    "title": "Case Study 2",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nThe primary goal of this research could be to predict test scores (in both math and verbal sections) based on various demographic, socioeconomic, and academic factors.\nDemographics: To understand regional and gender-based differences in education outcomes.\n\nStateCode, StateName (location-related factors)\nGenderFemaleTestTakers, GenderMaleTestTakers (gender of the test-takers)\n\nSocioeconomic Variables: Assess how financial background impacts test performance.\n\nFamilyIncomeLessThan20KTestTakers, FamilyIncomeBetween20_40KTestTakers, etc. (family income)\nFamilyIncomeLessThan20KMath, FamilyIncomeBetween20_40KMath, etc. (scores for different income levels)\n\nAcademic Background: To understand the relationship between day-to-day academic success and standardized testing success.\n\nGpaAMinusTestTakers, GpaBTestTakers, etc. (number of test-takers with specific GPAs)\nAcademicSubjectsArtsMusicAverageGpa (average GPA in academic subjects)\n\nWhy do it? To identify which factors have the most significant influence on standardized test scores.\nUse: By understanding the factors influencing standardized test performance could help education policymakers identify gaps in achievement and support more equitable access to resources.\nQuestions:\n\nDoes a higher family income correlate with better test scores.\nDo students with higher GPAs perform better on standardized tests.\nDo certain states consistently outperform others. If yes, does this relate to it’s socioeconomic status or educational policies.\nDoes gender plays a role in performance differences."
  },
  {
    "objectID": "posts/cs2/index.html#graph-1",
    "href": "posts/cs2/index.html#graph-1",
    "title": "Case Study 2",
    "section": "Graph 1:",
    "text": "Graph 1:\n\nType of graph: a Scatter Plot analyzing the visible relationships between different Quantitative variables using GGally.\n\nDefining the question:\n\nAre there patterns of performance consistency or variation between subjects?\nWhat is the Correlation between the average GPA of different subjects? Is being good / bad in one subject mean they are good/bad at all?\n\n\n\nAnalyse the Data:\nIdentifying the variables used: The quantitative values under all AcademicSubjects”Subject”AverageGpa’s quantitative data are plotted against each other. There is no particular data transformation required other than changing the variable names and cleaning the data-set a little more to only have these variables (although neither of these are really necessary)\n\nscores_modified &lt;- scores_cleaned %&gt;%\n  rename(\n    ArtsMusic = AcademicSubjectsArtsMusicAverageGpa,\n    English = AcademicSubjectsEnglishAverageGpa,\n    ForeignLanguages = AcademicSubjectsForeignLanguagesAverageGpa,\n    Mathematics = AcademicSubjectsMathematicsAverageGpa,\n    NaturalSciences = AcademicSubjectsNaturalSciencesAverageGpa,\n    SocialSciencesHistory = AcademicSubjectsSocialSciencesHistoryAverageGpa\n    )%&gt;%\n  select(ArtsMusic, English, ForeignLanguages, Mathematics,NaturalSciences, SocialSciencesHistory)\nscores_modified\n\n# A tibble: 577 × 6\n   ArtsMusic English ForeignLanguages Mathematics NaturalSciences\n       &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1      3.92    3.53             3.54        3.41            3.52\n 2      3.76    3.35             3.34        3.06            3.25\n 3      3.85    3.45             3.41        3.25            3.43\n 4      3.9     3.61             3.64        3.46            3.55\n 5      3.76    3.32             3.29        3.05            3.2 \n 6      3.88    3.49             3.41        3.33            3.43\n 7      3.66    3.13             3.03        3               3.07\n 8      3.71    3.21             3.18        3.07            3.19\n 9      3.54    3.03             3.04        2.91            2.99\n10      3.77    3.29             3.3         3.07            3.27\n# ℹ 567 more rows\n# ℹ 1 more variable: SocialSciencesHistory &lt;dbl&gt;\n\n\n\n\nReplicating the graph:\n\nGGally::ggpairs(\n  scores_modified %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"ArtsMusic\", \"English\", \"ForeignLanguages\", \"Mathematics\", \"NaturalSciences\", \"SocialSciencesHistory\"\n  ),\n  switch = \"both\",\n  progress = FALSE,\n  diag = list(continuous = \"densityDiag\"),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.03, se = FALSE)),\n  title = \"Academic Scores in Different Subjects\"\n)\n\n\n\n\n\n\n\n\n\n\nDoing a Correlation Test:\n\ncor_results &lt;- correlation::correlation(scores_modified)\ncor_results\n\n# Correlation Matrix (pearson-method)\n\nParameter1       |            Parameter2 |    r |       95% CI | t(575) |         p\n-----------------------------------------------------------------------------------\nArtsMusic        |               English | 0.91 | [0.90, 0.93] |  53.42 | &lt; .001***\nArtsMusic        |      ForeignLanguages | 0.90 | [0.88, 0.91] |  49.12 | &lt; .001***\nArtsMusic        |           Mathematics | 0.88 | [0.86, 0.90] |  43.96 | &lt; .001***\nArtsMusic        |       NaturalSciences | 0.93 | [0.91, 0.94] |  58.45 | &lt; .001***\nArtsMusic        | SocialSciencesHistory | 0.94 | [0.93, 0.95] |  66.89 | &lt; .001***\nEnglish          |      ForeignLanguages | 0.96 | [0.96, 0.97] |  87.33 | &lt; .001***\nEnglish          |           Mathematics | 0.96 | [0.95, 0.97] |  81.67 | &lt; .001***\nEnglish          |       NaturalSciences | 0.98 | [0.98, 0.98] | 119.60 | &lt; .001***\nEnglish          | SocialSciencesHistory | 0.98 | [0.98, 0.98] | 124.66 | &lt; .001***\nForeignLanguages |           Mathematics | 0.94 | [0.93, 0.95] |  68.27 | &lt; .001***\nForeignLanguages |       NaturalSciences | 0.97 | [0.96, 0.97] |  88.59 | &lt; .001***\nForeignLanguages | SocialSciencesHistory | 0.96 | [0.95, 0.97] |  82.20 | &lt; .001***\nMathematics      |       NaturalSciences | 0.98 | [0.97, 0.98] | 109.57 | &lt; .001***\nMathematics      | SocialSciencesHistory | 0.96 | [0.95, 0.96] |  78.08 | &lt; .001***\nNaturalSciences  | SocialSciencesHistory | 0.99 | [0.98, 0.99] | 136.99 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 577\n\n\n\nObservations:\nAll combination of subjects have a very small range of confidence intervals. Therefore, all correlation scores are valid i.e. all relationships are statistically significant, reliable, and consistent.\n\n\n\nObservations:\n\nThere is a high positive correlation between all subjects.\nThis consistent positive correlation might indicate that students who perform well in one subject tend to perform well in others. This could suggest a general academic competence or factors like study habits, discipline, or access to resources influencing performance across all subjects.\nThe correlation between ArtsMusic and Mathematics is relatively lower (0.878) compared to the other subject pairs but still strong. This could highlight different skill sets, as Arts/Music may engage creativity, whereas Mathematics is more focused on logical reasoning\nEnglish and Foreign Languages (0.964): These subjects may require similar skills in language comprehension, grammar, and communication.\nSocial Sciences/History and English (0.982) & Natural Science and Social Sciences/History(0.985) & English and Natural Science(0.980): All 3 subjects subjects rely heavily on reading, writing, and critical analysis, which might explain the very high correlation.\nWith mathematics, the subject with highest correlation is NaturalSciences with a score of 0.977. Both subjects share analytical and logical reasoning skills, which could explain the strong link."
  },
  {
    "objectID": "posts/cs2/index.html#graph-2",
    "href": "posts/cs2/index.html#graph-2",
    "title": "Case Study 2",
    "section": "Graph 2:",
    "text": "Graph 2:\n\nType of graph: A box plot.\n\nDefining the question:\nDoes a higher family income correlate with better test scores?\n\n\nAnalyse the Data:\nType of variables: Where a quantitative data (math scores) is plotted as a box plot separated by “levels” of income earned by the family. In this existing data-set, all math scores categorized by income are in different rows (FamilyIncomeBetween”Number1-Number2”KMath). We need to make a new column called IncomeCategory with the different levels of income and another that corresponds with it called Values where the math score data is stored.\n\nincome_data &lt;- scores_cleaned %&gt;%\n  select(\n    FamilyIncomeBetween20_40KMath,\n    FamilyIncomeLessThan20KMath,\n    FamilyIncomeBetween40_60KMath,\n    FamilyIncomeBetween60_80KMath,\n    FamilyIncomeBetween80_100KMath,\n    FamilyIncomeMoreThan100KMath\n  ) %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"IncomeCategory\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(IncomeCategory = case_when(\n    IncomeCategory == \"FamilyIncomeLessThan20KMath\" ~ \"LessThan20K\",\n    IncomeCategory == \"FamilyIncomeBetween20_40KMath\" ~ \"Between20_40K\",\n    IncomeCategory == \"FamilyIncomeBetween40_60KMath\" ~ \"Between40_60K\",\n    IncomeCategory == \"FamilyIncomeBetween60_80KMath\" ~ \"Between60_80K\",\n    IncomeCategory == \"FamilyIncomeBetween80_100KMath\" ~ \"Between80_100K\",\n    IncomeCategory == \"FamilyIncomeMoreThan100KMath\" ~ \"MoreThan100K\",\n    TRUE ~ NA_character_,\n  )) %&gt;%\n  select(IncomeCategory, Value)%&gt;%\n  dplyr::mutate(\n    IncomeCategory = factor(IncomeCategory,\n      levels = c(\"LessThan20K\", \"Between20_40K\", \"Between40_60K\",\"Between60_80K\", \"Between80_100K\", \"MoreThan100K\" ),\n      labels = c(\"LessThan20K\", \"Between20_40K\", \"Between40_60K\",\"Between60_80K\", \"Between80_100K\", \"MoreThan100K\"),\n      ordered = TRUE\n    )\n  )\n\nincome_data\n\n# A tibble: 3,462 × 2\n   IncomeCategory Value\n   &lt;ord&gt;          &lt;dbl&gt;\n 1 Between20_40K    513\n 2 LessThan20K      462\n 3 Between40_60K    539\n 4 Between60_80K    550\n 5 Between80_100K   566\n 6 MoreThan100K     588\n 7 Between20_40K    492\n 8 LessThan20K      464\n 9 Between40_60K    517\n10 Between60_80K    513\n# ℹ 3,452 more rows\n\n\n\nWhat is pivot_longer()?\n\nWide Format: This format typically has multiple columns representing different variables. For example, each family income category might have its own column for math scores.\nLong Format: This format consolidates these variables into two columns: one for the variable names (e.g., income categories) and one for their corresponding values. This format is often easier to work with for data analysis and visualization.\n\n\n\n\nReplicating the graph:\n\nincome_data %&gt;%\n  gf_boxplot(reorder(IncomeCategory, Value, FUN = median) ~ Value,\n    fill = ~IncomeCategory,\n    alpha = 0.2\n  ) %&gt;%\n  gf_labs(title = \"Math Scores vs Family Income\",subtitle = \"Chart 2\" ) %&gt;%\n  gf_labs(\n    x = \"Scores in Math\",\n    y = \"Income Class\"\n  )\n\n\n\n\n\n\n\n\n\nObservations:\n\nThere is a positive correlation between family income and math scores. As family income increases, median math scores also increase, showing that students from higher-income families tend to score better in math.\nThere are several outliers in the lower-income categories (especially for “LessThan20K” and “Between20_40K”) with students scoring exceptionally higher than their peers.\n“LessThan20K” seems to have a couple of outliers at 0. Did a lot of students not show up for tests? It’s interesting to see that other than them, only “Between20_40K” and “MoreThan100K” have students with a score of 0.\nThe chart illustrates that higher family income is associated with better math scores, with less variation in performance as income increases. However, even in lower-income groups, some students perform as well as those from higher-income families."
  },
  {
    "objectID": "posts/cs2/index.html#my-story",
    "href": "posts/cs2/index.html#my-story",
    "title": "Case Study 2",
    "section": "My Story:",
    "text": "My Story:"
  },
  {
    "objectID": "posts/Day-2/index.html",
    "href": "posts/Day-2/index.html",
    "title": "Day 2",
    "section": "",
    "text": "Throwing away data to grasp it. I am trying to reduce data into a summarized form."
  },
  {
    "objectID": "posts/Day-2/index.html#introduction",
    "href": "posts/Day-2/index.html#introduction",
    "title": "Day 2",
    "section": "",
    "text": "Throwing away data to grasp it. I am trying to reduce data into a summarized form."
  },
  {
    "objectID": "posts/Day-2/index.html#r-code",
    "href": "posts/Day-2/index.html#r-code",
    "title": "Day 2",
    "section": "R-code:",
    "text": "R-code:\n\nnote to self: don’t forget to put the label!!!!!\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows"
  },
  {
    "objectID": "posts/Day-2/index.html#the-mpg-data-set-car-stuff",
    "href": "posts/Day-2/index.html#the-mpg-data-set-car-stuff",
    "title": "Day 2",
    "section": "The mpg data-set (car stuff):",
    "text": "The mpg data-set (car stuff):\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\n\nmaking it look fancy. Arvind wrote this, i only added it here cause i wanted the full form of all columns cause i don’t know car stuff.\n\n\nmpg %&gt;%\n  head(10) %&gt;%\n  kbl(\n    # add Human Readable column names\n    col.names = c(\n      \"Manufacturer\", \"Model\", \"Engine\\nDisplacement\",\n      \"Model\\n Year\", \"Cylinders\", \"Transmission\",\n      \"Drivetrain\", \"City\\n Mileage\", \"Highway\\n Mileage\",\n      \"Fuel\", \"Class\\nOf\\nVehicle\"\n    ),\n    caption = \"MPG Dataset\"\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\n      \"striped\", \"hover\",\n      \"condensed\", \"responsive\"\n    ),\n    full_width = F, position = \"center\"\n  )\n\n\n\nMPG Dataset\n\n\nManufacturer\nModel\nEngine Displacement\nModel Year\nCylinders\nTransmission\nDrivetrain\nCity Mileage\nHighway Mileage\nFuel\nClass Of Vehicle\n\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n\n\n\n\n\n\n\nGlimpse:\n\nmpg %&gt;% dplyr::glimpse()\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\n\nTable of Variables\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nmanufacturer\nRepresents the name of the company that produces the vehicle.\nQualitative\n\n\nmodel\nThis variable indicates the specific model of the vehicle produced by the manufacturer.\nQualitative\n\n\ndispl\nEngine Displacement: measures the engine’s size or capacity\nQuantitative\n\n\nyear\nIndicates the year in which the vehicle model was manufactured.\nQualitative\n\n\ncyl\nSpecifies the number of cylinders in the vehicle’s engine.\nQualitative\n\n\ntrans\nIndicates the type of transmission system used in the vehicle\nQualitative\n\n\ndrv\nDescribes how power is delivered to the wheels (e.g., front-wheel drive, rear-wheel drive, all-wheel drive)\nQualitative\n\n\ncty\nMeasures the fuel efficiency of the vehicle when driving in urban conditions\nQuantitative\n\n\nhwy\nMeasures fuel efficiency on highways\nQuantitative\n\n\nfl\nIndicates the type of fuel used by the vehicle\nQualitative\n\n\nclass\nCategorizes vehicles into different classes based on their size, purpose, or design\nQualitative\n\n\n\n\nThrough this glimpse, i find that cylinder is encoded as an int, and so it is considered a quantitative variable. i need to change it.\n\n\nmpg_modified &lt;- mpg %&gt;%\n  dplyr::mutate(\n    cyl = as_factor(cyl),\n    fl = as_factor(fl),\n    drv = as_factor(drv),\n    class = as_factor(class),\n    trans = as_factor(trans)\n  )\nglimpse(mpg_modified)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;fct&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;fct&gt; auto(l5), manual(m5), manual(m6), auto(av), auto(l5), man…\n$ drv          &lt;fct&gt; f, f, f, f, f, f, f, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, r, …\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;fct&gt; p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, r, …\n$ class        &lt;fct&gt; compact, compact, compact, compact, compact, compact, com…\n\n\n\nInspecting the data-set with mosaic and skimr\nA mosaic inspect of the mgp data-set only to see the difference between this and the newly created mpg_modified data set\n\nmpg %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3        trans character     10 234       0\n4          drv character      3 234       0\n5           fl character      5 234       0\n6        class character      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n4 f (45.3%), 4 (44%), r (10.7%)                \n5 r (71.8%), p (22.2%), e (3.4%) ...           \n6 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cyl integer    4.0    4.0    6.0    8.0    8    5.888889 1.611534 234\n4   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n5   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n\n\n\nmpg_modified %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3          cyl    factor      4 234       0\n4        trans    factor     10 234       0\n5          drv    factor      3 234       0\n6           fl    factor      5 234       0\n7        class    factor      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 4 (34.6%), 6 (33.8%), 8 (29.9%) ...          \n4 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n5 f (45.3%), 4 (44%), r (10.7%)                \n6 r (71.8%), p (22.2%), e (3.4%) ...           \n7 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n4   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\n\nmpg_modified %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n234\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n5\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmanufacturer\n0\n1\n4\n10\n0\n15\n0\n\n\nmodel\n0\n1\n2\n22\n0\n38\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncyl\n0\n1\nFALSE\n4\n4: 81, 6: 79, 8: 70, 5: 4\n\n\ntrans\n0\n1\nFALSE\n10\naut: 83, man: 58, aut: 39, man: 19\n\n\ndrv\n0\n1\nFALSE\n3\nf: 106, 4: 103, r: 25\n\n\nfl\n0\n1\nFALSE\n5\nr: 168, p: 52, e: 8, d: 5\n\n\nclass\n0\n1\nFALSE\n7\nsuv: 62, com: 47, mid: 41, sub: 35\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndispl\n0\n1\n3.47\n1.29\n1.6\n2.4\n3.3\n4.6\n7\n▇▆▆▃▁\n\n\nyear\n0\n1\n2003.50\n4.51\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n▇▁▁▁▇\n\n\ncty\n0\n1\n16.86\n4.26\n9.0\n14.0\n17.0\n19.0\n35\n▆▇▃▁▁\n\n\nhwy\n0\n1\n23.44\n5.95\n12.0\n18.0\n24.0\n27.0\n44\n▅▅▇▁▁\n\n\n\n\n\n\n\n\nGroups within columns -\nWe can group a quantitative data based on different qualitative data-sets.\n\nHere i am finding the separate means of highway mileage for every separate level of cyl.\n\nmpg_modified %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(average_hwy = mean(hwy), count = n())\n\n# A tibble: 4 × 3\n  cyl   average_hwy count\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 4            28.8    81\n2 5            28.8     4\n3 6            22.8    79\n4 8            17.6    70\n\n\nThe most highway mileage is for 4 cylinders while the lowest is for 8 cylinders.\n\nYou can find mean based on 2 or more factors i.e. permutations and combinations of all factors\n\n\n\nSeparate means of highway mileage for every separate level of cyl and fuel.\n\nmpg_modified %&gt;%\n  group_by(cyl, fl) %&gt;%\n  summarize(average_hwy = mean(hwy), count = n())\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 13 × 4\n# Groups:   cyl [4]\n   cyl   fl    average_hwy count\n   &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n 1 4     p            27.8    22\n 2 4     r            28.3    55\n 3 4     d            43       3\n 4 4     c            36       1\n 5 5     r            28.8     4\n 6 6     p            25.3    17\n 7 6     r            22.2    60\n 8 6     e            17       1\n 9 6     d            22       1\n10 8     p            20.8    13\n11 8     r            17.5    49\n12 8     e            12.7     7\n13 8     d            17       1\n\n\nThe highest mean highway mileage is for a cars with 4 cylinders with diesel fuel while the lowest is for electric cars with 6 cylinders.\n\n\nSeparate means of city mileage for every separate level of cyl and fuel.\n\nmpg_modified %&gt;%\n  group_by(cyl, fl) %&gt;%\n  summarize(average_cty = mean(cty), count = n())\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 13 × 4\n# Groups:   cyl [4]\n   cyl   fl    average_cty count\n   &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n 1 4     p           19.9     22\n 2 4     r           20.8     55\n 3 4     d           32.3      3\n 4 4     c           24        1\n 5 5     r           20.5      4\n 6 6     p           16.8     17\n 7 6     r           16.1     60\n 8 6     e           11        1\n 9 6     d           17        1\n10 8     p           13.8     13\n11 8     r           12.7     49\n12 8     e            9.57     7\n13 8     d           14        1\n\n\nThe highest mean city mileage is for a cars with 4 cylinders with diesel fuel while the lowest is for cars with 8 cylinders and petrol fuel.\n\n\nSeparate means of city mileage for every separate level of cyl and fuel.\n\nmpg_modified %&gt;%\n  group_by(manufacturer) %&gt;%\n  summarize(average_cty = mean(cty))\n\n# A tibble: 15 × 2\n   manufacturer average_cty\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 audi                17.6\n 2 chevrolet           15  \n 3 dodge               13.1\n 4 ford                14  \n 5 honda               24.4\n 6 hyundai             18.6\n 7 jeep                13.5\n 8 land rover          11.5\n 9 lincoln             11.3\n10 mercury             13.2\n11 nissan              18.1\n12 pontiac             17  \n13 subaru              19.3\n14 toyota              18.5\n15 volkswagen          20.9\n\n\nThe highest mean city mileage is for a cars with 4 cylinders with diesel fuel while the lowest is for electric cars with 6 cylinders."
  },
  {
    "objectID": "posts/Day-2/index.html#math-anxiety-data-set",
    "href": "posts/Day-2/index.html#math-anxiety-data-set",
    "title": "Day 2",
    "section": "Math Anxiety Data-Set:",
    "text": "Math Anxiety Data-Set:\n\nmath_anx &lt;- read_delim(\"../../data/MathAnxiety.csv\", delim=\";\")\n\nRows: 599 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (2): Gender, Grade\ndbl (3): AMAS, RCMAS, Arith\nnum (1): Age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmath_anx\n\n# A tibble: 599 × 6\n     Age Gender Grade      AMAS RCMAS Arith\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1378 Boy    Secondary     9    20     6\n 2  1407 Boy    Secondary    18     8     6\n 3  1379 Girl   Secondary    23    26     5\n 4  1428 Girl   Secondary    19    18     7\n 5  1356 Boy    Secondary    23    20     1\n 6  1350 Girl   Secondary    27    33     1\n 7  1336 Boy    Secondary    22    23     4\n 8  1393 Boy    Secondary    17    11     7\n 9  1317 Girl   Secondary    28    32     2\n10  1348 Boy    Secondary    20    30     6\n# ℹ 589 more rows\n\n\n###Glimpse the data to understand which we have to change to factors.\n\nmath_anx %&gt;% dplyr::glimpse()\n\nRows: 599\nColumns: 6\n$ Age    &lt;dbl&gt; 1378, 1407, 1379, 1428, 1356, 1350, 1336, 1393, 1317, 1348, 141…\n$ Gender &lt;chr&gt; \"Boy\", \"Boy\", \"Girl\", \"Girl\", \"Boy\", \"Girl\", \"Boy\", \"Boy\", \"Gir…\n$ Grade  &lt;chr&gt; \"Secondary\", \"Secondary\", \"Secondary\", \"Secondary\", \"Secondary\"…\n$ AMAS   &lt;dbl&gt; 9, 18, 23, 19, 23, 27, 22, 17, 28, 20, 16, 20, 21, 36, 16, 27, …\n$ RCMAS  &lt;dbl&gt; 20, 8, 26, 18, 20, 33, 23, 11, 32, 30, 10, 4, 23, 26, 24, 21, 3…\n$ Arith  &lt;dbl&gt; 6, 6, 5, 7, 1, 1, 4, 7, 2, 6, 2, 5, 2, 6, 2, 7, 2, 4, 7, 3, 8, …\n\n\n\nTable of Variables\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nAge\nRepresents the age of the individual\nQualitative\n\n\nGender\nRepresents the gender of the individual(male/female)\nQualitative\n\n\nGrade\nRepresents the grade they study in\nQualitative\n\n\nAMAS\nAdult Manifest Anxiety Scale\nQuantitative\n\n\nRCMAS\nRevised Children’s Manifest Anxiety Scale\nQuantitative\n\n\nArith\nArithmetic Subtest of Intelligence Tests\nQuantitative\n\n\n\nTarget Variable: AMAS, RCMAS and Arith Predictor Variables: Age, Gender and Grade\n\n\nMutate to facors:\n\nAmong all variables, Gender, Grade and Arith must be turned into factors.\n\n\nmath_anx_modified &lt;- math_anx %&gt;%\n  dplyr::mutate(\n    Gender = as_factor(Gender),\n    Grade = as_factor(Grade),\n    Arith = as_factor(Arith)\n  )\nglimpse(math_anx_modified)\n\nRows: 599\nColumns: 6\n$ Age    &lt;dbl&gt; 1378, 1407, 1379, 1428, 1356, 1350, 1336, 1393, 1317, 1348, 141…\n$ Gender &lt;fct&gt; Boy, Boy, Girl, Girl, Boy, Girl, Boy, Boy, Girl, Boy, Boy, Boy,…\n$ Grade  &lt;fct&gt; Secondary, Secondary, Secondary, Secondary, Secondary, Secondar…\n$ AMAS   &lt;dbl&gt; 9, 18, 23, 19, 23, 27, 22, 17, 28, 20, 16, 20, 21, 36, 16, 27, …\n$ RCMAS  &lt;dbl&gt; 20, 8, 26, 18, 20, 33, 23, 11, 32, 30, 10, 4, 23, 26, 24, 21, 3…\n$ Arith  &lt;fct&gt; 6, 6, 5, 7, 1, 1, 4, 7, 2, 6, 2, 5, 2, 6, 2, 7, 2, 4, 7, 3, 8, …\n\n\n\n\nSummerising the data with skim:\n\nmath_anx_modified %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n599\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n2\nBoy: 323, Gir: 276\n\n\nGrade\n0\n1\nFALSE\n2\nPri: 401, Sec: 198\n\n\nArith\n0\n1\nFALSE\n9\n7: 112, 6: 109, 8: 94, 5: 93\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n0\n1\n1246.49\n223.11\n37\n1061.5\n1208\n1418.5\n1875\n▁▁▇▇▃\n\n\nAMAS\n0\n1\n21.98\n6.60\n4\n18.0\n22\n26.5\n45\n▂▆▇▃▁\n\n\nRCMAS\n0\n1\n19.24\n7.57\n1\n14.0\n19\n25.0\n41\n▂▇▇▅▁\n\n\n\n\n\n\nAnxiety based on Gender:\n\nmath_anx_modified %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(average_AMAS = mean(AMAS), average_RCMAS = mean(RCMAS), count =n())\n\n# A tibble: 2 × 4\n  Gender average_AMAS average_RCMAS count\n  &lt;fct&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Boy            21.2          18.1   323\n2 Girl           22.9          20.6   276\n\n\nHypothesis to Evaluate: Girls have a higher anxiety level.\n\n\nAnxiety based on Grade:\n\nmath_anx_modified %&gt;%\n  group_by(Grade) %&gt;%\n  summarize(average_AMAS = mean(AMAS), average_RCMAS = mean(RCMAS), count =n())\n\n# A tibble: 2 × 4\n  Grade     average_AMAS average_RCMAS count\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Secondary         22.3          18.5   198\n2 Primary           21.8          19.6   401\n\n\nHypothesis to Evaluate: Primary school students experience more anxiety than secondary school students.\n\n\nAnxiety based on Grade and Gender:\n\nmath_anx_modified %&gt;%\n  group_by(Grade, Gender) %&gt;%\n  summarize(average_AMAS = mean(AMAS), average_RCMAS = mean(RCMAS), count =n())\n\n`summarise()` has grouped output by 'Grade'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 5\n# Groups:   Grade [2]\n  Grade     Gender average_AMAS average_RCMAS count\n  &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Secondary Boy            21.5          17.4   124\n2 Secondary Girl           23.5          20.3    74\n3 Primary   Boy            20.9          18.6   199\n4 Primary   Girl           22.7          20.6   202\n\n\nHypothesis to Evaluate: Girls studying in secondary school have the highest anxiety and Secondary School boys have the least amount of anxiety.\n\n\nAnxiety based on Arithmetic Subtest of Intelligence Tests:\n\nmath_anx_modified %&gt;%\n  group_by(Arith) %&gt;%\n  summarize(average_AMAS = mean(AMAS), average_RCMAS = mean(RCMAS), count =n())\n\n# A tibble: 9 × 4\n  Arith average_AMAS average_RCMAS count\n  &lt;fct&gt;        &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 0             22.5          20.9    15\n2 1             24.5          23.9    25\n3 2             23.6          21.0    26\n4 3             24.2          18.2    56\n5 4             21.7          19.3    69\n6 5             21.2          19.7    93\n7 6             22.5          19.4   109\n8 7             21.4          18.1   112\n9 8             20.5          18.5    94\n\n\nI don’t really know how to read this last data."
  },
  {
    "objectID": "posts/day-4/index.html",
    "href": "posts/day-4/index.html",
    "title": "Day 4",
    "section": "",
    "text": "Quant and Qual Variable Graphs and their Siblings (god knows what that means).\nA histogram is a graphical representation of the distribution of continuous numerical data, where data is grouped into ranges (or bins), and the frequency of data points in each bin is displayed using bars. Quant variables will be present on the x-axis and the histogram shows us how frequently different values occur for that variable by showing counts/frequencies on the y-axis. we use “gf_histogram” for this.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\n##\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact"
  },
  {
    "objectID": "posts/day-4/index.html#introduction",
    "href": "posts/day-4/index.html#introduction",
    "title": "Day 4",
    "section": "",
    "text": "Quant and Qual Variable Graphs and their Siblings (god knows what that means).\nA histogram is a graphical representation of the distribution of continuous numerical data, where data is grouped into ranges (or bins), and the frequency of data points in each bin is displayed using bars. Quant variables will be present on the x-axis and the histogram shows us how frequently different values occur for that variable by showing counts/frequencies on the y-axis. we use “gf_histogram” for this.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\n##\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact"
  },
  {
    "objectID": "posts/day-4/index.html#diamonds-data-set",
    "href": "posts/day-4/index.html#diamonds-data-set",
    "title": "Day 4",
    "section": "Diamonds data-set:",
    "text": "Diamonds data-set:\n\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\nConsidering the fact that all qualitative data is already ordered and factored, we don’t mutate any values here.\n\n\nskim(diamonds)\n\n\nData summary\n\n\nName\ndiamonds\n\n\nNumber of rows\n53940\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncut\n0\n1\nTRUE\n5\nIde: 21551, Pre: 13791, Ver: 12082, Goo: 4906\n\n\ncolor\n0\n1\nTRUE\n7\nG: 11292, E: 9797, F: 9542, H: 8304\n\n\nclarity\n0\n1\nTRUE\n8\nSI1: 13065, VS2: 12258, SI2: 9194, VS1: 8171\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncarat\n0\n1\n0.80\n0.47\n0.2\n0.40\n0.70\n1.04\n5.01\n▇▂▁▁▁\n\n\ndepth\n0\n1\n61.75\n1.43\n43.0\n61.00\n61.80\n62.50\n79.00\n▁▁▇▁▁\n\n\ntable\n0\n1\n57.46\n2.23\n43.0\n56.00\n57.00\n59.00\n95.00\n▁▇▁▁▁\n\n\nprice\n0\n1\n3932.80\n3989.44\n326.0\n950.00\n2401.00\n5324.25\n18823.00\n▇▂▁▁▁\n\n\nx\n0\n1\n5.73\n1.12\n0.0\n4.71\n5.70\n6.54\n10.74\n▁▁▇▃▁\n\n\ny\n0\n1\n5.73\n1.14\n0.0\n4.72\n5.71\n6.54\n58.90\n▇▁▁▁▁\n\n\nz\n0\n1\n3.54\n0.71\n0.0\n2.91\n3.53\n4.04\n31.80\n▇▁▁▁▁\n\n\n\n\n\n\n\ncarat: weight of the diamond 0.2-5.01\ndepth: depth total depth percentage 43-79\ntable: width of top of diamond relative to widest point 43-95\nprice: price in US dollars $326-$18,823\nx: length in mm 0-10.74\ny: width in mm 0-58.9\nz(dbl): depth in mm 0-31.8\n\n\n\nThere are no missing values for any variable, all are complete with 54K entries.\n\n\nMy first histogram!!!!\n\nPlotting diamond prices.\n\n## if i want i can not specigy the bins too. \ngf_histogram(~price,\n  data = diamonds,\n  bins = 100\n) %&gt;%\n  gf_labs(\n    title = \"Plot 1A: Diamond Prices\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\nWe can infer that while a large number of diamonds are priced relatively low, there are also a significant number of diamonds that are priced very high.\n\n\nWhat is the distribution of the predictor variable carat?\n\ndiamonds %&gt;%\n  gf_histogram(~carat,\n    bins = 100\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2B: Carats of Diamonds\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\nWe can infer that there must be some, very few, diamonds of very high carat value while there a few carat values that appear to be more than common! People very commonly buy diamonds of 1 carat and a little less frequently 0.5, 1.5 and 2.\n\n\nDoes a price distribution vary based upon type of cut?\n\nfrom what i observe, the fill acts as a stack here, although it is prices that are represented in the historgram, we able to observe what portion of each bin is occupied by each of the cuts\n\n\ngf_histogram(~price, fill = ~cut, data = diamonds, bins=100) %&gt;%\n  gf_labs(title = \"Plot 3A: Diamond Prices\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price|cut, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Plot 3C: Prices by Filled and Facetted by Cut\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45, ## the angle at which the word should be placed.\n      hjust = 1 ## the incremanting space from the x axis\n    )\n  ))\n\n\n\n\n\n\n\n\nusing this, we can observe the price range of each individual cut as different graphs, but very low values (in comparison to high values of ideal) such as those in fair and good. We can make them have ranges of values in y axis based on their individual values by setting scales to be free y.\n\n## the nrow, defines the number of rows \ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~cut, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Prices Filled and Facetted by Cut\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nPrice ranges are the same regardless of cut and so that must not be the only parameter in dermeining the price\n\n\nDoes a price distribution vary based upon type of clarity?\n\ngf_histogram(~price, fill = ~clarity, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot 3A: Diamond Prices spereated by clarity\")\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~clarity, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~clarity) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Prices Filled and Facetted by Clarity\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\n\n## the nrow, defines the number of rows \ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~clarity, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~clarity, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Prices Filled and Facetted by Clarity\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nPrice ranges are the same regardless of clarity and so that must not be the only parameter in determining the price but SI1 appease to have the most in high prices\n\n\nDoes a price distribution vary based upon type of colour?\n\ngf_histogram(~price, fill = ~color, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot: Diamond Prices spereated by color\")\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~color, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~color) %&gt;%\n  gf_labs(\n    title = \"Plot: Prices Filled and Facetted by Color\",\n    subtitle = \"Free y-scale\",\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~color, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~color, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot: Prices Filled and Facetted by Color\",\n    subtitle = \"Free y-scale\",\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))"
  },
  {
    "objectID": "posts/day-4/index.html#the-race-data-set",
    "href": "posts/day-4/index.html#the-race-data-set",
    "title": "Day 4",
    "section": "The Race data-set",
    "text": "The Race data-set\n\nrace_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/race.csv\")\n\nRows: 1207 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): event, race, city, country, participation\ndbl  (6): race_year_id, distance, elevation_gain, elevation_loss, aid_statio...\ndate (1): date\ntime (1): start_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrace_df\n\n# A tibble: 1,207 × 13\n   race_year_id event    race  city  country date       start_time participation\n          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;     &lt;time&gt;     &lt;chr&gt;        \n 1        68140 Peak Di… Mill… Cast… United… 2021-09-03 19:00      solo         \n 2        72496 UTMB®    UTMB® Cham… France  2021-08-27 17:00      Solo         \n 3        69855 Grand R… Ultr… viel… France  2021-08-20 05:00      solo         \n 4        67856 Persenk… PERS… Asen… Bulgar… 2021-08-20 18:00      solo         \n 5        70469 Runfire… 100 … uluk… Turkey  2021-08-20 18:00      solo         \n 6        66887 Swiss A… 160KM Müns… Switze… 2021-08-15 17:00      solo         \n 7        67851 Salomon… Salo… Foll… Norway  2021-08-14 07:00      solo         \n 8        68241 Ultra T… 160KM Spa   Belgium 2021-08-14 07:00      solo         \n 9        70241 Québec … QMT-… Beau… Canada  2021-08-13 22:00      solo         \n10        69945 Bunketo… BBUT… LIND… Sweden  2021-08-07 10:00      solo         \n# ℹ 1,197 more rows\n# ℹ 5 more variables: distance &lt;dbl&gt;, elevation_gain &lt;dbl&gt;,\n#   elevation_loss &lt;dbl&gt;, aid_stations &lt;dbl&gt;, participants &lt;dbl&gt;\n\nrank_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/ultra_rankings.csv\")\n\nRows: 137803 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): runner, time, gender, nationality\ndbl (4): race_year_id, rank, age, time_in_seconds\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrank_df\n\n# A tibble: 137,803 × 8\n   race_year_id  rank runner      time    age gender nationality time_in_seconds\n          &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;\n 1        68140     1 VERHEUL Ja… 26H …    30 M      GBR                   95725\n 2        68140     2 MOULDING J… 27H …    43 M      GBR                   97229\n 3        68140     3 RICHARDSON… 28H …    38 M      GBR                  103747\n 4        68140     4 DYSON Fiona 30H …    55 W      GBR                  111217\n 5        68140     5 FRONTERAS … 32H …    48 W      GBR                  117981\n 6        68140     6 THOMAS Lei… 32H …    31 M      GBR                  118000\n 7        68140     7 SHORT Debo… 33H …    55 W      GBR                  120601\n 8        68140     8 CROSSLEY C… 33H …    40 W      GBR                  120803\n 9        68140     9 BUTCHER Ke… 34H …    47 M      GBR                  125656\n10        68140    10 Hendry Bill 34H …    29 M      GBR                  125979\n# ℹ 137,793 more rows\n\n\n\nglimpse(race_df)\n\nRows: 1,207\nColumns: 13\n$ race_year_id   &lt;dbl&gt; 68140, 72496, 69855, 67856, 70469, 66887, 67851, 68241,…\n$ event          &lt;chr&gt; \"Peak District Ultras\", \"UTMB®\", \"Grand Raid des Pyréné…\n$ race           &lt;chr&gt; \"Millstone 100\", \"UTMB®\", \"Ultra Tour 160\", \"PERSENK UL…\n$ city           &lt;chr&gt; \"Castleton\", \"Chamonix\", \"vielle-Aure\", \"Asenovgrad\", \"…\n$ country        &lt;chr&gt; \"United Kingdom\", \"France\", \"France\", \"Bulgaria\", \"Turk…\n$ date           &lt;date&gt; 2021-09-03, 2021-08-27, 2021-08-20, 2021-08-20, 2021-0…\n$ start_time     &lt;time&gt; 19:00:00, 17:00:00, 05:00:00, 18:00:00, 18:00:00, 17:0…\n$ participation  &lt;chr&gt; \"solo\", \"Solo\", \"solo\", \"solo\", \"solo\", \"solo\", \"solo\",…\n$ distance       &lt;dbl&gt; 166.9, 170.7, 167.0, 164.0, 159.9, 159.9, 163.8, 163.9,…\n$ elevation_gain &lt;dbl&gt; 4520, 9930, 9980, 7490, 100, 9850, 5460, 4630, 6410, 31…\n$ elevation_loss &lt;dbl&gt; -4520, -9930, -9980, -7500, -100, -9850, -5460, -4660, …\n$ aid_stations   &lt;dbl&gt; 10, 11, 13, 13, 12, 15, 5, 8, 13, 23, 13, 5, 12, 15, 0,…\n$ participants   &lt;dbl&gt; 150, 2300, 600, 150, 0, 300, 0, 200, 120, 100, 300, 50,…\n\n\n\nglimpse(rank_df)\n\nRows: 137,803\nColumns: 8\n$ race_year_id    &lt;dbl&gt; 68140, 68140, 68140, 68140, 68140, 68140, 68140, 68140…\n$ rank            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, NA, NA, NA,…\n$ runner          &lt;chr&gt; \"VERHEUL Jasper\", \"MOULDING JON\", \"RICHARDSON Phill\", …\n$ time            &lt;chr&gt; \"26H 35M 25S\", \"27H 0M 29S\", \"28H 49M 7S\", \"30H 53M 37…\n$ age             &lt;dbl&gt; 30, 43, 38, 55, 48, 31, 55, 40, 47, 29, 48, 47, 52, 49…\n$ gender          &lt;chr&gt; \"M\", \"M\", \"M\", \"W\", \"W\", \"M\", \"W\", \"W\", \"M\", \"M\", \"M\",…\n$ nationality     &lt;chr&gt; \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\"…\n$ time_in_seconds &lt;dbl&gt; 95725, 97229, 103747, 111217, 117981, 118000, 120601, …\n\n\n\nmosaic::favstats returns a data frame with several common summary statistics, such as:\n\nmin: Minimum value\nQ1: First quartile (25th percentile)\nmedian: Median (50th percentile)\nQ3: Third quartile (75th percentile)\nmax: Maximum value\nmean: Arithmetic mean\nsd: Standard deviation\nn: Number of non-missing observations\nmissing: Number of missing values\nIQR: Interquartile range (Q3 - Q1)\n\n\n\nrace_df %&gt;%\n  favstats(~distance, data = .)\n\n min    Q1 median     Q3   max     mean       sd    n missing\n   0 160.1  161.5 165.15 179.1 152.6187 39.87864 1207       0\n\n\n\nrace_df %&gt;%\n  favstats(~participants, data = .)\n\n min Q1 median  Q3  max     mean       sd    n missing\n   0  0     21 150 2900 120.4872 281.8337 1207       0\n\n\n\nrank_df %&gt;%\n  drop_na() %&gt;%\n  favstats(time_in_seconds ~ gender, data = .)\n\n  gender  min      Q1 median       Q3    max     mean       sd      n missing\n1      M 3600 96536.5 115845 149761.5 288000 123271.1 37615.42 101643       0\n2      W 9191 96695.0 107062 131464.0 296806 117296.5 34604.26  18341       0\n\n\n\nOn occasion we may need to see summaries of several Quant variables, over levels of Qual variables. This is where the package crosstable is so effective. Therefore, crosstable is useful when you need to generate summary statistics of quantitative (numeric) variables, broken down by levels of qualitative (categorical) variables.\n\n\ncrosstable(time_in_seconds + age ~ gender, data = rank_df) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariablegenderMWNAtime_in_secondsMin / Max3600.0 / 2.9e+059191.0 / 3.0e+058131.0 / 2.2e+05Med [IQR]1.2e+05 [9.7e+04;1.5e+05]1.1e+05 [9.7e+04;1.3e+05]1.2e+05 [9.9e+04;1.5e+05]Mean (std)1.2e+05 (3.8e+04)1.2e+05 (3.5e+04)1.2e+05 (4.4e+04)N (NA)101643 (15073)18341 (2716)28 (2)ageMin / Max0 / 133.00 / 81.029.0 / 59.0Med [IQR]47.0 [40.0;53.0]45.0 [39.0;52.0]40.5 [36.0;50.5]Mean (std)46.4 (10.2)45.3 (9.7)41.7 (9.0)N (NA)116716 (0)21057 (0)30 (0)\n\n\nMen participating are generally older than women. When it comes to time in seconds, while the overall central tendency (mean) remains consistent, the distribution of the data has some variation as shown by (std) and (median) but not the average - it’s the same.\n\nWhich countries host the maximum number of races? Which countries send the maximum number of participants??\n\nrace_df %&gt;%\n  count(country) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 61 × 2\n   country            n\n   &lt;chr&gt;          &lt;int&gt;\n 1 United States    438\n 2 United Kingdom   110\n 3 France            56\n 4 Australia         46\n 5 Sweden            46\n 6 China             45\n 7 Canada            32\n 8 Spain             27\n 9 Japan             24\n10 Poland            23\n# ℹ 51 more rows\n\n\n\nrank_df %&gt;%\n  count(nationality) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 133 × 2\n   nationality     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 USA         47259\n 2 FRA         28905\n 3 GBR         11076\n 4 JPN          6729\n 5 ESP          5478\n 6 CHN          4744\n 7 CAN          2822\n 8 ITA          2794\n 9 SWE          2293\n10 AUS          1683\n# ℹ 123 more rows\n\n\nThe United states hosts the most number of games and send the most number of players.\n\n\nWhich country wins the most?\n\nrank_df %&gt;%\n  filter(rank %in% c(1, 2, 3)) %&gt;%\n  count(nationality) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 69 × 2\n   nationality     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 USA          1240\n 2 GBR           347\n 3 FRA           210\n 4 AUS           140\n 5 CAN           132\n 6 CHN           128\n 7 SWE           124\n 8 ESP           113\n 9 JPN            94\n10 ITA            79\n# ℹ 59 more rows\n\n\nTo no surprise, United states wins the most too. Would it be the same case if compared the ratio of players to wins?\n\n\nAnalyze the nationality of the top 10 participants in the longest races\n\n\nslice() allows you to select, remove, and duplicate rows. slice_min() and slice_max() select rows with the smallest or largest values of a variable.\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions.\nleft_join() is a function from the dplyr package used to combine two data frames by joining them based on a common column (or columns).\n\n\n\nlongest_races &lt;- race_df %&gt;%\n  slice_max(n = 5, order_by = distance) # Longest distance races\nlongest_races\n\n# A tibble: 6 × 13\n  race_year_id event     race  city  country date       start_time participation\n         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;     &lt;time&gt;     &lt;chr&gt;        \n1        68776 Ultra To… Ut4M… Gren… France  2021-07-16 18:00      Solo         \n2        55551 Ultra Tr… Inth… Chom… Thaila… 2020-02-14 10:00      solo         \n3         7484 Le TREG®… LE T… Fada  Chad    2015-02-06 00:00      solo         \n4         7594 THE GREA… 100 … Pato… Austra… 2014-09-13 00:00      Solo         \n5        71066 ULTRA 01  Ultr… Oyon… France  2021-07-09 18:00      solo         \n6        23565 EstrelAç… Estr… Penh… Portug… 2017-10-06 18:00      Solo         \n# ℹ 5 more variables: distance &lt;dbl&gt;, elevation_gain &lt;dbl&gt;,\n#   elevation_loss &lt;dbl&gt;, aid_stations &lt;dbl&gt;, participants &lt;dbl&gt;\n\nlongest_races %&gt;%\n  left_join(., rank_df, by = \"race_year_id\") %&gt;% # total participants in longest 4 races\n  filter(rank %in% c(1:10)) %&gt;% # Top 10 ranks\n  count(nationality) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 9 × 2\n  nationality     n\n  &lt;chr&gt;       &lt;int&gt;\n1 FRA            26\n2 AUS             9\n3 POR             8\n4 THA             8\n5 BEL             1\n6 BRA             1\n7 ESP             1\n8 MAS             1\n9 RUS             1\n\n\n\nWe get 2 tables - one with the joined data set, the other with the nationality and count of the amount of wins (top 10 rank among the longest races).\n\nThese quantities show that even though USA has the most number of wins, for the longest races, france has the most participents among everyone the top 10 rank.\n\n\nWhat is the distribution of the finishing times?\n\nrank_df %&gt;%\n  gf_histogram(~time_in_seconds, bins = 75) %&gt;%\n  gf_labs(title = \"Histogram of Race Times\")\n\nWarning: Removed 17791 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nMost people finished the race at 1e+05. The histogram shows three bumps (is this a result of the difference in distance?)\n\n\nWhat is the distribution of race distances?\n\nrace_df %&gt;%\n  gf_histogram(~distance, bins = 50) %&gt;%\n  gf_labs(title = \"Histogram of Race Distances\")\n\n\n\n\n\n\n\n\nHow are there multiple races at 0 distance, is this a glitch in the data? There are very few races between 0-150. Most races seem to be set at a distance from 150 -180.\n\nrace_df %&gt;%\n  filter(distance == 0)\n\n# A tibble: 74 × 13\n   race_year_id event    race  city  country date       start_time participation\n          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;     &lt;time&gt;     &lt;chr&gt;        \n 1        64771 The Old… 100m… Hanm… New Ze… 2021-05-14 10:00      solo         \n 2        71220 Run Lov… 100M  &lt;NA&gt;  United… 2021-02-26 00:00      solo         \n 3        67160 IDAHO M… 100 … &lt;NA&gt;  United… 2020-09-12 00:00      solo         \n 4        67713 Pine cr… 100M… Well… PA, Un… 2020-09-12 00:00      solo         \n 5        51777 Chiemga… 100 … Berg… Germany 2020-07-31 13:00      Solo         \n 6        66413 Palisad… Moos… Irwin United… 2020-07-17 05:00      solo         \n 7        62593 Run Lov… 100M  &lt;NA&gt;  United… 2020-02-28 00:00      solo         \n 8        50097 The Gre… The … Hanm… New Ze… 2020-01-17 07:00      solo         \n 9        65861 Loup Ga… 100M  Vill… LA, Un… 2019-12-14 00:00      solo         \n10        59415 RIO DEL… 100 … &lt;NA&gt;  United… 2019-11-07 00:00      solo         \n# ℹ 64 more rows\n# ℹ 5 more variables: distance &lt;dbl&gt;, elevation_gain &lt;dbl&gt;,\n#   elevation_loss &lt;dbl&gt;, aid_stations &lt;dbl&gt;, participants &lt;dbl&gt;\n\n\nEven though there are so many races registerd as 100 mile races, non of the distances are 100 in the histogram.\n\n\nWhat is the distribution of finishing times for race distance around 150 faceted by time of day?\nA count of start times:\n\nrace_times &lt;- race_df %&gt;%\n  count(start_time) %&gt;%\n  arrange(desc(n))\nrace_times\n\n# A tibble: 39 × 2\n   start_time     n\n   &lt;time&gt;     &lt;int&gt;\n 1 00:00        513\n 2 06:00        114\n 3 08:00         63\n 4 10:00         60\n 5 07:00         58\n 6 18:00         50\n 7 05:00         48\n 8 12:00         38\n 9 04:00         30\n10 09:00         27\n# ℹ 29 more rows\n\n\nWe section a day into different groups of time: example morning, noon, evening and create a new column start_day_time with this information; since might and post midnight can be categorized as the same, we use fact_collapse to combine it to be the same. left_join() is used to merge race_start_factor with another data frame rank_df based on the race_year_id column, which is common between both. drop_na() removes any rows where time_in_seconds is NA (i.e., missing values). This ensures the plot only uses races with valid time data. hms- hour minute second.\n\nrace_start_factor &lt;- race_df %&gt;%\n  filter(distance == 0) %&gt;% # Races that actually took place\n  mutate(\n    ## start day time is a new column you are creating based on the values in\n    start_day_time =\n      case_when(\n        start_time &gt; hms(\"02:00:00\") &\n          start_time &lt;= hms(\"06:00:00\") ~ \"early_morning\",\n        start_time &gt; hms(\"06:00:01\") &\n          start_time &lt;= hms(\"10:00:00\") ~ \"late_morning\",\n        start_time &gt; hms(\"10:00:01\") &\n          start_time &lt;= hms(\"14:00:00\") ~ \"mid_day\",\n        start_time &gt; hms(\"14:00:01\") &\n          start_time &lt;= hms(\"18:00:00\") ~ \"afternoon\",\n        start_time &gt; hms(\"18:00:01\") &\n          start_time &lt;= hms(\"22:00:00\") ~ \"evening\",\n        start_time &gt; hms(\"22:00:01\") &\n          start_time &lt;= hms(\"23:59:59\") ~ \"night\",\n        start_time &gt;= hms(\"00:00:00\") &\n          start_time &lt;= hms(\"02:00:00\") ~ \"postmidnight\",\n        .default = \"other\"\n      )\n  ) %&gt;%\n  mutate(\n    start_day_time =\n      as_factor(start_day_time) %&gt;%\n        fct_collapse(\n          .f = .,\n          night = c(\"night\", \"postmidnight\")\n        )\n  )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `start_day_time = `%&gt;%`(...)`.\nCaused by warning:\n! Unknown levels in `f`: night\n\n##\n# Join with rank_df\nrace_start_factor %&gt;%\n  left_join(rank_df, by = \"race_year_id\") %&gt;%\n  drop_na(time_in_seconds) %&gt;%\n  gf_histogram(\n    ~time_in_seconds,\n    bins = 75,\n    fill = ~start_day_time,\n    color = ~start_day_time,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(start_day_time), ncol = 2, scales = \"free_y\") %&gt;%\n  gf_labs(title = \"Race Times by Start-Time\")\n\n\n\n\n\n\n\n\nWe see that finish times tend to be longer for afternoon and evening start races"
  },
  {
    "objectID": "posts/day-4/index.html#populations-data-set",
    "href": "posts/day-4/index.html#populations-data-set",
    "title": "Day 4",
    "section": "Populations data-set",
    "text": "Populations data-set\n\npop &lt;- read_delim(\"../../data/populations.csv\")\n\nRows: 16400 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country_code, country_name\ndbl (2): year, value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npop\n\n# A tibble: 16,400 × 4\n   country_code country_name  year value\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 ABW          Aruba         1960 54608\n 2 ABW          Aruba         1961 55811\n 3 ABW          Aruba         1962 56682\n 4 ABW          Aruba         1963 57475\n 5 ABW          Aruba         1964 58178\n 6 ABW          Aruba         1965 58782\n 7 ABW          Aruba         1966 59291\n 8 ABW          Aruba         1967 59522\n 9 ABW          Aruba         1968 59471\n10 ABW          Aruba         1969 59330\n# ℹ 16,390 more rows\n\ninspect(pop)\n\n\ncategorical variables:  \n          name     class levels     n missing\n1 country_code character    265 16400       0\n2 country_name character    265 16400       0\n                                   distribution\n1 ABW (0.4%), AFE (0.4%), AFG (0.4%) ...       \n2 Afghanistan (0.4%) ...                       \n\nquantitative variables:  \n   name   class  min       Q1  median       Q3        max         mean\n1  year numeric 1960   1975.0    1991     2006       2021 1.990529e+03\n2 value numeric 2646 986302.5 6731400 46024452 7888408686 2.140804e+08\n            sd     n missing\n1 1.789551e+01 16400       0\n2 7.040554e+08 16400       0\n\n\nThere are many countries with small populations and a few countries with very large populations. Such distributions are also called “long tailed” distributions.\n\ngf_histogram(~value, data = pop, title = \"Long Tailed Histogram\")\n\n\n\n\n\n\n\n##\ngf_density(~value, data = pop, title = \"Long Tailed Density\")\n\n\n\n\n\n\n\n\nTo develop better insights with this data, we should transform the variable concerned, using say a “log” transformation:\n\ngf_histogram(~ log10(value), data = pop, title = \"Histogram with Log transformed x-variable\")\n\n\n\n\n\n\n\n##\ngf_density(~ log10(value), data = pop, title = \"Density with Log transformed x-variable\")"
  },
  {
    "objectID": "posts/day-4/index.html#what-does-each-distribution-signify",
    "href": "posts/day-4/index.html#what-does-each-distribution-signify",
    "title": "Day 4",
    "section": "What does each distribution signify?",
    "text": "What does each distribution signify?\n\n\n\nBimodal: There could be two different underlying processes or populations contributing to the data.\nComb: The data might have been processed in a way that grouped values together in regular intervals.A comb distribution could appear in data where ages are rounded to the nearest five years (e.g., 20, 25, 30).\nEdge Peak: Could even be a data entry artifact!! All unknown / unrecorded observations are recorded as 999 !!🙀\nNormal: The data follows a typical pattern where most values are close to the mean, and extremes are rare. This distribution occurs frequently in nature and in many datasets due to the Central Limit Theorem.\nSkewed: Right skew suggests that most values are clustered at the lower end, but there are some extreme high values. Left skew suggests that most values are clustered at the higher end, but there are some extreme low values.\nUniform: This can suggest random or non-preferential selection of values."
  },
  {
    "objectID": "posts/Day-5(2)/index.html",
    "href": "posts/Day-5(2)/index.html",
    "title": "Day 5 - Part 2",
    "section": "",
    "text": "For count, both variable are quantitative"
  },
  {
    "objectID": "posts/Day-5(2)/index.html#hollywood-movies-data-set",
    "href": "posts/Day-5(2)/index.html#hollywood-movies-data-set",
    "title": "Day 5 - Part 2",
    "section": "Hollywood movies data-set",
    "text": "Hollywood movies data-set\n\nHollywoodMovies2011 -&gt; movies\nglimpse(movies)\n\nRows: 136\nColumns: 14\n$ Movie             &lt;fct&gt; \"Insidious\", \"Paranormal Activity 3\", \"Bad Teacher\",…\n$ LeadStudio        &lt;fct&gt; Sony, Independent, Independent, Warner Bros, Relativ…\n$ RottenTomatoes    &lt;int&gt; 67, 68, 44, 96, 90, 93, 75, 35, 63, 69, 69, 49, 26, …\n$ AudienceScore     &lt;int&gt; 65, 58, 38, 92, 77, 84, 91, 58, 74, 73, 72, 57, 68, …\n$ Story             &lt;fct&gt; Monster Force, Monster Force, Comedy, Rivalry, Rival…\n$ Genre             &lt;fct&gt; Horror, Horror, Comedy, Fantasy, Comedy, Romance, Dr…\n$ TheatersOpenWeek  &lt;int&gt; 2408, 3321, 3049, 4375, 2918, 944, 2534, 3615, NA, 2…\n$ BOAverageOpenWeek &lt;int&gt; 5511, 15829, 10365, 38672, 8995, 6177, 10278, 23775,…\n$ DomesticGross     &lt;dbl&gt; 54.01, 103.66, 100.29, 381.01, 169.11, 56.18, 169.22…\n$ ForeignGross      &lt;dbl&gt; 43.00, 98.24, 115.90, 947.10, 119.28, 83.00, 30.10, …\n$ WorldGross        &lt;dbl&gt; 97.009, 201.897, 216.196, 1328.111, 288.382, 139.177…\n$ Budget            &lt;dbl&gt; 1.5, 5.0, 20.0, 125.0, 32.5, 17.0, 25.0, 80.0, 0.2, …\n$ Profitability     &lt;dbl&gt; 64.672667, 40.379400, 10.809800, 10.624888, 8.873292…\n$ OpeningWeekend    &lt;dbl&gt; 13.27, 52.57, 31.60, 169.19, 26.25, 5.83, 26.04, 85.…\n\n\n\nMovie (Factor):\n\nThis is likely the title or identifier of the movie. Since it’s a factor, it means there are multiple unique movie titles.\n\nLeadStudio (Factor):\n\nThe production studio that produced or distributed the movie. It could include studios like Warner Bros, Disney, Universal, etc.\n\nRottenTomatoes (Numeric):\n\nThe movie’s rating or score on Rotten Tomatoes, a popular review aggregator. It’s often expressed as a percentage, representing the proportion of positive reviews by critics.\n\nAudienceScore (Numeric):\n\nA score reflecting how audiences rated the movie, also typically on a percentage scale. This is often gathered from surveys or audience feedback platforms.\n\nStory (Factor):\n\nThis could refer to the primary theme or storyline category of the movie (e.g., Hero’s Journey, Revenge, Coming of Age). It might represent narrative tropes or common plot structures.\n\nGenre (Factor):\n\nThe type of movie based on its theme or tone (e.g., Action, Comedy, Drama, Horror). These are pre-defined categories common to film classification.\n\nTheatersOpenWeek (Numeric):\n\nThe number of theaters where the movie was shown during its opening week. A larger number indicates a wide release, while a smaller number could suggest a limited release.\n\nBOAverageOpenWeek (Numeric):\n\nBox Office Average for the opening week, likely representing the average box office earnings per theater during that week.\n\nDomesticGross (Numeric):\n\nThe total amount of money the movie earned in the domestic market (e.g., within the U.S.) during its entire run, expressed in millions of USD.\n\nForeignGross (Numeric):\n\nThe total box office earnings from international markets outside the domestic territory, in millions of USD.\n\nWorldGross (Numeric):\n\nThe combined total of DomesticGross and ForeignGross, representing global earnings for the movie.\n\nBudget (Numeric):\n\nThe total cost of producing the movie, including marketing and production expenses, in millions of USD.\n\nProfitability (Numeric):\n\nrepresent how many times the movie’s global earnings (WorldGross) exceeded the production budget (Budget).\n\nOpeningWeekend (Numeric):\n\nThe amount of money the movie made during its opening weekend, in millions of USD. This figure is often used to predict the overall success of a film.\n\n\nObtaining only the numberic values:\n\nmovies_quant &lt;- movies %&gt;%\n  drop_na() %&gt;%\n  select(where(is.numeric))\nmovies_quant\n\n    RottenTomatoes AudienceScore TheatersOpenWeek BOAverageOpenWeek\n1               67            65             2408              5511\n2               68            58             3321             15829\n3               44            38             3049             10365\n4               96            92             4375             38672\n5               90            77             2918              8995\n6               93            84              944              6177\n7               75            91             2534             10278\n8               35            58             3615             23775\n9               69            73             2756              6860\n10              69            72             3040              9310\n11              49            57             3018              6512\n12              26            68             4061             34012\n13              35            67             4088             23937\n14              56            52             2994              8469\n15              71            73             3826             10252\n16              82            78             3379             10492\n17              83            87             3648             15024\n18              23            31             3328              2615\n19              23            50             3395             10489\n20              93            93             2458              3517\n21              93            79             2886              3929\n22              82            80             3925             12142\n23              55            57             3043              7183\n24              85            76             2199              4761\n25              71            68             2926              6364\n26              34            61             4155             21697\n27              61            56             3155              5715\n28              92            81             2961              5002\n29              93            86             3448              8672\n30              37            54             1719              2955\n31              30            39             2787              3390\n32              60            79             3703             10704\n33              38            55             1552              2470\n34              47            63             3167              7500\n35              47            54             3339              5524\n36              38            55             3122              3860\n37              60            72             2817              5979\n38              35            50             3417             10411\n39              77            80             3955             16618\n40              26            50             3579             10490\n41              78            81             3020              6326\n42              38            56             4115             16072\n43              22            40             3295              3534\n44              19            63             3548              8601\n45              78            75             3715             17512\n46              20            43             2985              4955\n47              71            71             3549              4383\n48              72            67             2840              7450\n49               4            29             2534              5921\n50              46            79             2214              4789\n51              58            81             3440              7942\n52              72            70             2802              4655\n53              36            59             3112             10349\n54              58            57             3305              5656\n55              32            57             3154              6167\n56              84            81             3507              5461\n57               4            46             3118              3504\n58              35            44             2950              4588\n59              10            32             2816              3769\n60              76            70             1826              5427\n61              84            63             3222              6935\n62              87            88             3641             15134\n63              83            76             3952              8623\n64              14            42             3482              5763\n65              71            67             2535              4880\n66              11            41             3030              4622\n67              95            89             2993              6516\n68              38            50             2473              3014\n69              44            47             3584              9335\n70              84            82             2707              4879\n71              88            69             3917              9722\n72              24            48             3017              2875\n73              34            46             2973              4405\n74              84            61                4             93230\n75              19            50             1952              5047\n76              68            61             3367              7135\n77              97            87             3440              8500\n78              16            25             2806              2995\n79              28            55             2614              3982\n80              24            50             3002              1806\n81              43            48             2888              4616\n82              24            53             2913              4645\n83              17            37             2864              5221\n84              53            52             2703              4226\n85              59            37             2760              3089\n86              75            68             3114              2477\n87              26            49             3276              3731\n88              91            79             2405              3267\n89              27            48             3816             13935\n90              23            48             3033              6284\n91              39            43             2296              3782\n92              44            50             3750              9715\n93               4            59             3438              7273\n94              23            31             2940              6060\n95              83            93             1869              2805\n96              83            84              247              7174\n97              41            59             3606              5889\n98               7            38             2661              3055\n99              25            48             2986              3132\n100             36            52             2996              2835\n101             45            38             2290              2265\n102             92            82             3376              3537\n103             56            65              707              4960\n104             22            34             3015              3324\n105             26            36             2769              3380\n106             50            48             2273              2259\n107             46            66              265              3856\n108             66            55              106              6111\n109             62            57               22              4890\n110             36            43             3117              2218\n111             38            62             2150              1513\n    DomesticGross ForeignGross WorldGross Budget Profitability OpeningWeekend\n1           54.01        43.00     97.009    1.5    64.6726667          13.27\n2          103.66        98.24    201.897    5.0    40.3794000          52.57\n3          100.29       115.90    216.196   20.0    10.8098000          31.60\n4          381.01       947.10   1328.111  125.0    10.6248880         169.19\n5          169.11       119.28    288.382   32.5     8.8732923          26.25\n6           56.18        83.00    139.177   17.0     8.1868824           5.83\n7          169.22        30.10    199.324   25.0     7.9729600          26.04\n8          254.46       327.00    581.464   80.0     7.2683000          85.95\n9           79.25        82.60    161.849   27.0     5.9944074          18.91\n10         117.54        92.10    209.638   35.0     5.9896571          28.30\n11          70.60        77.10    147.700   25.0     5.9080000          19.70\n12         260.80       374.00    634.800  110.0     5.7709091         138.12\n13         352.39       770.81   1123.195  195.0     5.7599744          97.85\n14          99.97        94.00    193.967   36.0     5.3879722          25.36\n15         143.62       341.02    484.634   90.0     5.3848222          39.23\n16         127.00       132.71    259.713   50.0     5.1942600          35.45\n17         176.70       304.52    481.226   93.0     5.1744731          54.81\n18          17.69         7.88     25.562    5.0     5.1124000           8.70\n19         142.61       419.54    562.158  110.0     5.1105273          35.61\n20          34.90         1.62     36.511    8.0     4.5638750           8.64\n21          34.68        32.33     67.007   15.0     4.4671333          11.34\n22         165.25       497.78    663.024  150.0     4.4201600          47.66\n23          63.69        67.10    130.786   30.0     4.3595333          21.86\n24          40.49        13.70     54.194   12.5     4.3355200          10.47\n25          55.80        93.74    149.541   35.0     4.2726000          18.62\n26         241.07       802.80   1043.871  250.0     4.1754840          90.15\n27          42.59       115.30    157.887   40.0     3.9471750          18.03\n28          54.71        68.57    123.278   32.0     3.8524375          14.81\n29         197.80       336.70    534.500  145.0     3.6862069          29.55\n30          13.84        41.40     55.241   15.0     3.6827333           5.08\n31          23.20        85.40    108.600   30.0     3.6200000           9.40\n32         179.04       261.00    440.040  125.0     3.5203200          39.63\n33           8.31       149.63    157.939   45.0     3.5097556           3.83\n34          52.70        19.72     72.416   21.0     3.4483810          23.75\n35          68.22       119.14    187.355   55.0     3.4064545          18.45\n36          36.49        90.90    127.393   40.0     3.1848250          12.05\n37          58.71        58.39    117.094   38.0     3.0814211          16.84\n38          83.55       128.27    211.818   70.0     3.0259714          35.57\n39         181.03       267.48    448.512  150.0     2.9900800          65.72\n40         108.09        75.87    183.953   63.0     2.9198889          37.54\n41          84.34        58.50    142.841   50.0     2.8568200          19.10\n42         191.45       360.40    551.850  200.0     2.7592500          66.14\n43          38.54        35.54     74.080   27.0     2.7437037          11.64\n44         103.03       111.92    214.945   80.0     2.6868125          30.51\n45         176.65       191.75    368.404  140.0     2.6314571          65.06\n46          33.00        63.00     96.000   37.0     2.5945946          14.80\n47          51.16        10.90     62.053   24.0     2.5855417          15.56\n48          62.50        65.37    127.868   50.2     2.5471713          21.16\n49          37.30         3.19     40.492   16.0     2.5307500          15.00\n50          43.85         0.41     44.267   18.0     2.4592778          10.60\n51          83.61       186.20    269.811  110.0     2.4528273          27.32\n52          37.41        60.57     97.983   40.0     2.4495750          13.04\n53          80.49       102.00    182.485   75.0     2.4331333          32.21\n54          38.18        58.96     97.137   40.0     2.4284250          18.69\n55          55.10        89.40    144.500   60.0     2.4083333          19.45\n56          71.08        16.86     87.947   37.0     2.3769459          19.15\n57          28.07        54.00     82.069   35.0     2.3448286          10.93\n58          45.06        38.10     83.160   36.0     2.3100000          13.54\n59          24.80        66.80     91.600   40.0     2.2900000          10.60\n60          31.18        14.25     45.429   20.0     2.2714500           9.91\n61          75.64        59.80    135.443   60.0     2.2573833          22.40\n62         146.41       207.22    353.623  160.0     2.2101437          55.10\n63         142.09       142.30    284.386  130.0     2.1875846          34.08\n64          80.36        89.94    170.301   80.0     2.1287625          20.07\n65          40.26        23.52     63.781   30.0     2.1260333          12.37\n66          37.66        51.50     89.162   42.0     2.1229048          14.01\n67          74.21        27.90    102.109   50.0     2.0421800          19.50\n68          23.18        16.48     39.664   20.0     1.9832000           7.45\n69          98.80       129.00    227.800  120.0     1.8983333          33.50\n70          58.01        17.00     75.009   40.0     1.8752250          13.21\n71         123.26       121.90    245.154  135.0     1.8159556          38.08\n72          20.25       111.90    132.147   75.0     1.7619600           8.67\n73          35.61        16.80     52.408   30.0     1.7469333          13.10\n74          13.30        41.00     54.303   32.0     1.6969687           0.37\n75          27.87         0.97     28.833   17.0     1.6960588           9.85\n76          74.50        47.00    121.504   75.0     1.6200533          24.03\n77          66.63         5.80     72.426   45.0     1.6094667          29.24\n78          18.88        19.83     38.702   25.0     1.5480800           8.40\n79          36.67        24.30     60.965   40.0     1.5241250          10.41\n80          14.01        16.42     30.426   20.0     1.5213000           5.42\n81          37.05         3.49     40.546   28.0     1.4480714          13.33\n82          37.08        33.75     70.833   52.0     1.3621731          13.53\n83          29.14        49.17     78.308   60.0     1.3051333          14.95\n84          29.20        22.00     51.200   40.0     1.2800000          11.40\n85          24.05         7.50     31.546   25.0     1.2618400           8.53\n86          18.30        18.80     37.102   30.0     1.2367333           7.71\n87          33.04        12.70     45.735   40.0     1.1433750          12.22\n88          26.69         6.46     33.152   30.0     1.1050667           7.86\n89         116.60       103.25    219.851  200.0     1.0992550          53.17\n90          36.39        53.40     89.792   82.0     1.0950244          19.06\n91          19.49         7.63     27.121   25.0     1.0848400           8.68\n92         100.24        74.58    174.821  163.0     1.0725215          36.43\n93          68.91        15.00     83.911   79.0     1.0621646          25.00\n94          48.50        21.20     69.700   70.0     0.9957143          17.80\n95          13.66         9.40     23.057   25.0     0.9222800           5.24\n96           5.31         2.95      8.258   10.0     0.8258000           1.75\n97          57.31        49.20    106.507  135.0     0.7889407          21.24\n98          21.30        17.20     38.502   50.0     0.7700400           8.13\n99          25.12        27.84     52.961   70.0     0.7565857           9.35\n100         16.93        10.50     27.428   38.0     0.7217895           8.49\n101         10.72        18.21     28.931   45.0     0.6429111           5.19\n102         33.70        57.50     91.203  150.0     0.6080200          12.07\n103         11.54         2.67     14.211   25.0     0.5684400           3.51\n104         21.30        27.50     48.795   90.0     0.5421667          10.02\n105         21.60         3.26     24.856   49.9     0.4981162           9.36\n106         13.07         8.48     21.552   45.0     0.4789333           5.14\n107          4.46         9.73     14.190   30.0     0.4730000           1.02\n108          4.40         0.40      4.800   15.0     0.3200000           0.64\n109          0.97         5.40      6.370   21.0     0.3033333           0.11\n110         21.39        17.60     38.992  150.0     0.2599467           6.91\n111          7.17         0.24      7.410   41.0     0.1807317           3.25\n\n\n\nCorrelation between domestic gross and world gross.\n\nmovies %&gt;%\n  gf_point(DomesticGross ~ WorldGross) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Gross Earnings: Domestics vs World\"\n  )\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_lm()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nThere appears to be a +ve correlation between Domestic gross and world gross.\n\n\nCorrelation between Profitability and Opening Week.\n\nmovies %&gt;%\n  gf_point(Profitability ~ OpeningWeekend) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\"\n  )\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_lm()`).\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nOpeningWeek and Profitability are also related in a linear way. There are just two movies which have been extremely profitable\n\n\nCorrelation between RottenTomatoes and AudienceScore.\n\nmovies %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Tomatoes vs Audience\"\n  )\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_lm()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nRotten tomatoes and Audience score have a +ve correlation.\n\nWe can split some of the scatter plots using one or other of the Qual variables.\n\n\n\nCorrelation between RottenTomatoes and AudienceScore seprated by Genre.\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore,\n    color = ~Genre\n  ) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Trends by Genre\"\n  )\n\n\n\n\n\n\n\n\n\n\nQuantifying Correlation\nGGally::ggpairs() : This function creates a matrix of plots for pairwise comparisons of the selected variables.\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"RottenTomatoes\", \"AudienceScore\", \"DomesticGross\", \"ForeignGross\"\n  ),\n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n\n  progress = FALSE,\n  # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  title = \"Movies Data Correlations Plot #1\"\n)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAudienceScore and RottenTomatoes are well correlated with a score of 8.3. Domestic Gross and Foreign Gross is also well correlated with a score of 8.73. Nothing else is as strongly related.\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"Budget\", \"Profitability\", \"DomesticGross\", \"ForeignGross\"\n  ),\n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n\n  progress = FALSE,\n  # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  title = \"Movies Data Correlations Plot #2\"\n)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAlthough not too much, you do see a correlation score of 0.65 between budget and Domestic gross and 0.67 between budget and Foreign Gross.\nBudget and Profitability seem to have a slight negative correlation.\n\n\nDoing a Correlation Test\nWe try and find out how certain we can be about the correlation score by doing this.\n\nWhat does this data tell me?\nestimate: This is the correlation coefficient (often denoted as “r”) between Profitability and Budget in your dataset.\nconfidence level low and confidence level high: These are the lower and upper bounds of uncertainity for the correlation coefficient.\np.value: The p-value shows the probability that the observed correlation happened by chance, assuming that there is no true correlation.\nMovie Profitability vs Budget\n\nmosaic::cor_test(Profitability ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Profitability vs Budget\"\n  )\n\n\nMovie Profitability vs Budget\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-0.08\n-0.96\n0.34\n132\n-0.25\n0.09\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nHere, the correlation coefficient is -0.08 - a slight negative correlation, but we observe that the uncertainty levels lie within a large range. We also see that that p-value is high. In other words, there is no clear linear relationship between Profitability and Budget in your data. We, I don’t have enough evidence to make strong conclusions about a consistent negative (or positive) relationship. Increasing or decreasing a movie’s budget does not reliably predict its profitability.\nMovie’s Domestic Gross vs Budget\n\nmosaic::cor_test(DomesticGross ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Domestic Gross vs Budget\"\n  )\n\n\nMovie Domestic Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n0.7\n11.06\n0\n131\n0.6\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nHere, the correlation coefficient is 0.7 - a positive correlation, and we observe that the uncertainty levels lie within a short range i.e. there’s low uncertainty around the estimate. We also see that that p-value is 0, concluding that this inference could not have been by chance making it statistically significant. Therefore, Budget has a strong positive effect on Profitability. Movies with higher budgets tend to have significantly higher Domestic Gross numbers, based on this data. This relationship is statistically significant, reliable, and consistent.\nMovie Foreign Gross vs Budget\n\nmosaic::cor_test(ForeignGross ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Foreign Gross vs Budget\"\n  )\n\n\nMovie Foreign Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n0.69\n10.22\n0\n118\n0.58\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nHas similar inferences as the previous test.\n\n\n\nThe ErrorBar Plot for Correlations"
  },
  {
    "objectID": "posts/My-name-is-Immortal/Index.html",
    "href": "posts/My-name-is-Immortal/Index.html",
    "title": "Day 1",
    "section": "",
    "text": "I’m a designer in training and like most people in the creatives, my interests cover a broad spectrum ranging from 3D Modelling and Lettering to Website Designs. I’m now working to add data visualisation and into my skill set, I hope I don’t totally suck at it!"
  },
  {
    "objectID": "posts/My-name-is-Immortal/Index.html#introduction",
    "href": "posts/My-name-is-Immortal/Index.html#introduction",
    "title": "Day 1",
    "section": "",
    "text": "I’m a designer in training and like most people in the creatives, my interests cover a broad spectrum ranging from 3D Modelling and Lettering to Website Designs. I’m now working to add data visualisation and into my skill set, I hope I don’t totally suck at it!"
  },
  {
    "objectID": "posts/My-name-is-Immortal/Index.html#my-first-piece-of-r-code",
    "href": "posts/My-name-is-Immortal/Index.html#my-first-piece-of-r-code",
    "title": "Day 1",
    "section": "My First Piece of R-code",
    "text": "My First Piece of R-code\nI’m doing this!!!!!\n\nlibrary (tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary (ggformula)\n\nLoading required package: scales\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nLoading required package: ggridges\n\nNew to ggformula?  Try the tutorials: \n    learnr::run_tutorial(\"introduction\", package = \"ggformula\")\n    learnr::run_tutorial(\"refining\", package = \"ggformula\")\n\nlibrary (babynames)\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\nGetting a list of all baby names in the USA from the year 1880\n\nbabynames\n\n# A tibble: 1,924,665 × 5\n    year sex   name          n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n 1  1880 F     Mary       7065 0.0724\n 2  1880 F     Anna       2604 0.0267\n 3  1880 F     Emma       2003 0.0205\n 4  1880 F     Elizabeth  1939 0.0199\n 5  1880 F     Minnie     1746 0.0179\n 6  1880 F     Margaret   1578 0.0162\n 7  1880 F     Ida        1472 0.0151\n 8  1880 F     Alice      1414 0.0145\n 9  1880 F     Bertha     1320 0.0135\n10  1880 F     Sarah      1288 0.0132\n# ℹ 1,924,655 more rows\n\n\n\n\n\nFiltering the presence of my name i.e. Sneha and creating a line graph with this data\n\nbabynames %&gt;% filter (name==\"Sneha\")\n\n# A tibble: 43 × 5\n    year sex   name      n       prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;\n 1  1975 F     Sneha     9 0.00000577\n 2  1976 F     Sneha     7 0.00000445\n 3  1977 F     Sneha     9 0.00000547\n 4  1978 F     Sneha     9 0.00000548\n 5  1979 F     Sneha     6 0.00000348\n 6  1980 F     Sneha     7 0.00000393\n 7  1981 F     Sneha     6 0.00000336\n 8  1982 F     Sneha     8 0.00000441\n 9  1983 F     Sneha     9 0.00000503\n10  1984 F     Sneha    14 0.00000777\n# ℹ 33 more rows\n\n\n\nbabynames %&gt;% filter(name==\"Sneha\") %&gt;% gf_line(n~year)\n\n\n\n\n\n\n\n\n\n\nFiltering the presence of the name “Trisha” and creating a line graph with this data\n\nbabynames %&gt;% filter (name==\"Trisha\")\n\n# A tibble: 84 × 5\n    year sex   name       n       prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n 1  1934 F     Trisha     5 0.00000462\n 2  1943 F     Trisha     5 0.00000348\n 3  1944 F     Trisha    33 0.0000242 \n 4  1945 F     Trisha    46 0.0000342 \n 5  1946 F     Trisha    56 0.0000347 \n 6  1947 F     Trisha    32 0.0000176 \n 7  1948 F     Trisha    29 0.0000166 \n 8  1949 F     Trisha    28 0.0000160 \n 9  1950 F     Trisha    36 0.0000205 \n10  1951 F     Trisha    50 0.0000271 \n# ℹ 74 more rows\n\n\n\nbabynames %&gt;% filter(name==\"Trisha\") %&gt;% gf_line(n~year)\n\n\n\n\n\n\n\n\n\n\nFiltering the presence of the name “Sarah” and creating a line graph with this data\n\nbabynames %&gt;% filter (name==\"Sarah\" | name==\"Sara\") \n\n# A tibble: 487 × 5\n    year sex   name      n      prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  1880 F     Sarah  1288 0.0132   \n 2  1880 F     Sara    165 0.00169  \n 3  1881 F     Sarah  1226 0.0124   \n 4  1881 F     Sara    147 0.00149  \n 5  1882 F     Sarah  1410 0.0122   \n 6  1882 F     Sara    180 0.00156  \n 7  1883 F     Sarah  1359 0.0113   \n 8  1883 F     Sara    183 0.00152  \n 9  1883 M     Sarah     7 0.0000622\n10  1884 F     Sarah  1518 0.0110   \n# ℹ 477 more rows\n\n\n\nbabynames %&gt;% filter(name==\"Sarah\" | name==\"Sara\") %&gt;% gf_line(n~year)\n\n\n\n\n\n\n\n\n\n\nGlimpse:\n\nbabynames %&gt;% dplyr::glimpse()\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\n\n\nbabynames_modified &lt;- babynames %&gt;%\n  dplyr::mutate(\n    sex = as_factor(sex),\n  )\nglimpse(babynames_modified)\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;fct&gt; F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F,…\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\n\n\nbabynames_modified %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n  name     class levels       n missing\n1  sex    factor      2 1924665       0\n2 name character  97310 1924665       0\n                                   distribution\n1 F (59.1%), M (40.9%)                         \n2 Francis (0%), James (0%) ...                 \n\nquantitative variables:  \n  name   class      min        Q1    median        Q3          max         mean\n1 year numeric 1.88e+03 1.951e+03 1.985e+03 2.003e+03 2.017000e+03 1.974851e+03\n2    n integer 5.00e+00 7.000e+00 1.200e+01 3.200e+01 9.968600e+04 1.808733e+02\n3 prop numeric 2.26e-06 3.870e-06 7.300e-06 2.288e-05 8.154561e-02 1.362963e-04\n            sd       n missing\n1 3.402948e+01 1924665       0\n2 1.533337e+03 1924665       0\n3 1.151693e-03 1924665       0\n\n\n\nbabynames_modified %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1924665\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n2\n15\n0\n97310\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1\nFALSE\n2\nF: 1138293, M: 786372\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1974.85\n34.03\n1880\n1951\n1985\n2003\n2017.00\n▁▂▃▅▇\n\n\nn\n0\n1\n180.87\n1533.34\n5\n7\n12\n32\n99686.00\n▇▁▁▁▁\n\n\nprop\n0\n1\n0.00\n0.00\n0\n0\n0\n0\n0.08\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/Experiment3/index.html",
    "href": "posts/Experiment3/index.html",
    "title": "Experiment 3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nlibrary(infer) \n\n\nAttaching package: 'infer'\n\nThe following objects are masked from 'package:mosaic':\n\n    prop_test, t_test\n\nlibrary(patchwork) \nlibrary(ggprism)\nlibrary(supernova)"
  },
  {
    "objectID": "posts/Experiment3/index.html#defining-the-research-experiment",
    "href": "posts/Experiment3/index.html#defining-the-research-experiment",
    "title": "Experiment 3",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nGoal of the Experiment: The purpose of this experiment is to evaluate whether there are significant differences in grades between students enrolled in different degree programs at Srishti School of Art, Design and Technology. There are 3 kinds of degrees offered: B.Voc, B.FA, and B.Des. By collecting and analyzing grade data from multiple disciplines, you aim to understand potential academic performance patterns or disparities associated with each program.\n\nMethodology\nSampling: The population consists of all students that are currently studying in Srishti. The sample population is a randomly chosen group of students ensuring a hopefully balanced set of participants that is a true representation of the population.The students were asked what the first Contextual Enquiry mark they got is, in the academic year 2024-2025 is.\nMethod Followed- To ensure a lack of bias, a coin was tossed to even determine if data would be collected from a person or not. This lead to random sampling ensure diversity in age, year of study, and socioeconomic background.\nSample Size: We had a total of 90 participants 30 B.Des and 30 B.Voc and 30 B.FA.\nWhy could knowing this be useful?\n\nIf certain degrees consistently lower grades, it may reveal the need for additional academic support or resources.\nInsights from this data can help academic advisors understand potential challenges in different programs, aiding students\nIdentifying patterns may stimulate discussions on grading consistency across departments, contributing to equitable academic practices.\n\n\n\nReading and Analyzing the data:\n\ngrades_data &lt;- read.csv(\"../../data/Grades.csv\")\ngrades_data\n\n   SN Degree Course Year Letter.Grade Score Gender\n1   1  B.Des    CAC    2            A   8.0      F\n2   2  B.Des    CAC    2            O   9.6      F\n3   3  B.Des   IADP    2           A+   9.2      F\n4   4  B.Des     CE    2            O   9.8      F\n5   5  B.Des   BSSD    2            P   3.0      M\n6   6  B.Des    CAC    2            O   9.5      F\n7   7  B.Des    PSD    2           A+   9.0      F\n8   8  B.Des    PSD    2           A+   9.0      F\n9   9  B.Des    PSD    2           A+   9.0      F\n10 10  B.Des   BSSD    3           A+   9.0      F\n11 11  B.Des   VCSB    2            A   8.0      F\n12 12  B.Des   VCSB    2            A   8.0      F\n13 13  B.Des  IAIDP    3            A   8.0      M\n14 14  B.Des  IAIDP    3           B+   7.0      M\n15 15  B.Des   VCSB    2           B+   7.0      F\n16 16  B.Des    HCD    3           A+   9.0      M\n17 17  B.Des    CAC    3           A+   9.1      F\n18 18  B.Des   VCSB    2            A   8.0      M\n19 19  B.Des     CE    2           A+   9.0      F\n20 20  B.Des    CAC    2           A+   9.0      M\n21 21  B.Des   IADP    2            A   8.0      F\n22 22  B.Des   IADP    3           B+   7.0      M\n23 23  B.Des   VCSB    3           A+   9.0      F\n24 24  B.Des     CE    2            O  10.0      F\n25 25  B.Des    CAC    3           A+   9.0      F\n26 26  B.Des    CAC    3           A+   9.0      F\n27 27  B.Des    PSD    3           B+   7.0      F\n28 28  B.Des    PSD    3            A   8.0      F\n29 29  B.Des    CAC    2           A+   9.0      F\n30 30  B.Des    HCD    2            A   8.0      F\n31 31  B.Voc   UIID    3            O   9.5      F\n32 32  B.Voc   UIID    3           A+   9.0      F\n33 33  B.Voc   UIID    3           A+   9.0      M\n34 34  B.Voc   UIID    3           B+   7.0      M\n35 35  B.Voc   UIID    3            A   8.0      F\n36 36  B.Voc   UIID    3            A   8.0      F\n37 37  B.Voc     ID    3           B+   7.0      M\n38 38  B.Voc    DMP    3            O   9.5      M\n39 39  B.Voc    DMP    3            B   6.5      M\n40 40  B.Voc    DMP    3            O   9.5      M\n41 41  B.Voc    DMP    3            O   9.5      M\n42 42  B.Voc    DMP    3           A+   9.0      M\n43 43  B.Voc    DMP    3            O   9.5      F\n44 44  B.Voc    DMP    3            O   9.5      F\n45 45  B.Voc    DMP    3            B   6.5      F\n46 46  B.Voc    DMP    3            C   5.0      M\n47 47  B.Voc   UIID    1           A+   9.0      M\n48 48  B.Voc   UIID    1            B   7.0      F\n49 49  B.Voc   UIID    1            B   7.0      F\n50 50  B.Voc   UIID    1            A   8.0      F\n51 51  B.Voc   UIID    1            A   8.0      F\n52 52  B.Voc   UIID    1           A+   9.0      F\n53 53  B.Voc   UIID    2            A   8.0      M\n54 54  B.Voc   GADP    1            A   8.0      F\n55 55  B.Voc   GADP    2            A   8.0      M\n56 56  B.Voc   Film    3            A   8.0      M\n57 57  B.Voc   GADP    3           A+   9.0      F\n58 58  B.Voc   GADP    3           A+   9.0      F\n59 59  B.Voc    DMP    2            A   8.0      M\n60 60  B.Voc    DMP    2            A   8.0      M\n61 61   B.FA    CAP    3            A   8.0      F\n62 62   B.FA    CAP    3           A+   9.0      M\n63 63   B.FA    CAP    3           A+   9.0      F\n64 64   B.FA    DMA    2           A+   8.5      M\n65 65   B.FA    DMA    2            A   8.0      M\n66 66   B.FA    DMA    2            A   8.0      F\n67 67   B.FA    CAP    2            A   8.0      F\n68 68   B.FA    CAP    2           A+   8.0      F\n69 69   B.FA   Film    3            B   6.0      M\n70 70   B.FA    DMA    2            A   8.0      F\n71 71   B.FA    DMA    2           B+   7.0      F\n72 72   B.FA    DMA    2           B+   7.0      F\n73 73   B.FA    DMA    2            A   8.0      F\n74 74   B.FA    DMA    2           B+   7.0      F\n75 75   B.FA    DMA    2            B   6.0      M\n76 76   B.FA   Film    3            A   8.0      M\n77 77   B.FA   Film    3            A   8.0      M\n78 78   B.FA    DMA    2            A   8.0      M\n79 79   B.FA    DMA    2            A   8.0      M\n80 80   B.FA    DMA    2           B+   7.0      F\n81 81   B.FA    DMA    2           B+   7.0      F\n82 82   B.FA   Film    2            A   8.0      F\n83 83   B.FA   Film    3           B+   7.0      F\n84 84   B.FA    DMA    2           A+   9.0      F\n85 85   B.FA    DMA    2            A   8.0      M\n86 86   B.FA    DMA    2           B+   7.0      F\n87 87   B.FA    DMA    2           B+   7.0      F\n88 88   B.FA    DMA    2           B+   7.0      M\n89 89   B.FA    DMA    2           B+   7.0      F\n90 90   B.FA    DMA    2           B+   7.0      F\n\n\n\nglimpse(grades_data)\n\nRows: 90\nColumns: 7\n$ SN           &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ Degree       &lt;chr&gt; \"B.Des\", \"B.Des\", \"B.Des\", \"B.Des\", \"B.Des\", \"B.Des\", \"B.…\n$ Course       &lt;chr&gt; \"CAC\", \"CAC\", \"IADP\", \"CE\", \"BSSD\", \"CAC\", \"PSD\", \"PSD\", …\n$ Year         &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, …\n$ Letter.Grade &lt;chr&gt; \"A\", \"O\", \"A+\", \"O\", \"P\", \"O\", \"A+\", \"A+\", \"A+\", \"A+\", \"A…\n$ Score        &lt;dbl&gt; 8.0, 9.6, 9.2, 9.8, 3.0, 9.5, 9.0, 9.0, 9.0, 9.0, 8.0, 8.…\n$ Gender       &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F…\n\n\n\nskim(grades_data)\n\n\nData summary\n\n\nName\ngrades_data\n\n\nNumber of rows\n90\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nDegree\n0\n1\n4\n5\n0\n3\n0\n\n\nCourse\n0\n1\n2\n5\n0\n15\n0\n\n\nLetter.Grade\n0\n1\n1\n2\n0\n7\n0\n\n\nGender\n0\n1\n1\n1\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSN\n0\n1\n45.50\n26.12\n1\n23.25\n45.5\n67.75\n90\n▇▇▇▇▇\n\n\nYear\n0\n1\n2.33\n0.62\n1\n2.00\n2.0\n3.00\n3\n▁▁▇▁▆\n\n\nScore\n0\n1\n8.06\n1.13\n3\n7.00\n8.0\n9.00\n10\n▁▁▆▇▇\n\n\n\n\n\n\n\nDefining the Data Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nDegree\nThe name of degree the student is studying for\nQualitative\n\n\nCourse\nThe specific field of study or program the student is enrolled in within their degrees\nQualitative\n\n\nLetter.Grade\nThe grade assigned to a student in letter format\nQualitative\n\n\nGender\nThe gender of the student\nQualitative\n\n\nSN\nA unique identifier for each record or student entry in the data-set\n-\n\n\nYear\nAre they a 1st year, 2nd year or 3rd year student?\nQualitative\n\n\nScore\nThe numerical grade earned by the student in their first CE on academic year 2024-2025, says the same thing as Letter grade but in a more accurate and precise manner\nQuantitative\n\n\n\nObservations:\n\nSN and letter grade are not a required column.\nDegree, year and and Gender need to be transformed into factors\nAre there too many levels on Course for it to be a factor? Maybe if do find a statistical difference in how different degrees are graded, for the one with the highest, I can find out if there is a course wise difference too?\n\n\n\nTransforming and inspecting the data:\n\ngrades_modified &lt;- grades_data %&gt;%\n  dplyr::mutate(\n    Gender = as_factor(Gender),\n    Year = factor(Year,\n      levels = c(\"1\",\"2\",\"3\"),\n      labels = c(\"1\",\"2\",\"3\"),\n      ordered = TRUE),\n    Degree = as_factor(Degree),\n    Course = as_factor(Course)\n    )%&gt;%\n  select(Degree , Course, Year, Gender, Score)\ngrades_modified\n\n   Degree Course Year Gender Score\n1   B.Des    CAC    2      F   8.0\n2   B.Des    CAC    2      F   9.6\n3   B.Des   IADP    2      F   9.2\n4   B.Des     CE    2      F   9.8\n5   B.Des   BSSD    2      M   3.0\n6   B.Des    CAC    2      F   9.5\n7   B.Des    PSD    2      F   9.0\n8   B.Des    PSD    2      F   9.0\n9   B.Des    PSD    2      F   9.0\n10  B.Des   BSSD    3      F   9.0\n11  B.Des   VCSB    2      F   8.0\n12  B.Des   VCSB    2      F   8.0\n13  B.Des  IAIDP    3      M   8.0\n14  B.Des  IAIDP    3      M   7.0\n15  B.Des   VCSB    2      F   7.0\n16  B.Des    HCD    3      M   9.0\n17  B.Des    CAC    3      F   9.1\n18  B.Des   VCSB    2      M   8.0\n19  B.Des     CE    2      F   9.0\n20  B.Des    CAC    2      M   9.0\n21  B.Des   IADP    2      F   8.0\n22  B.Des   IADP    3      M   7.0\n23  B.Des   VCSB    3      F   9.0\n24  B.Des     CE    2      F  10.0\n25  B.Des    CAC    3      F   9.0\n26  B.Des    CAC    3      F   9.0\n27  B.Des    PSD    3      F   7.0\n28  B.Des    PSD    3      F   8.0\n29  B.Des    CAC    2      F   9.0\n30  B.Des    HCD    2      F   8.0\n31  B.Voc   UIID    3      F   9.5\n32  B.Voc   UIID    3      F   9.0\n33  B.Voc   UIID    3      M   9.0\n34  B.Voc   UIID    3      M   7.0\n35  B.Voc   UIID    3      F   8.0\n36  B.Voc   UIID    3      F   8.0\n37  B.Voc     ID    3      M   7.0\n38  B.Voc    DMP    3      M   9.5\n39  B.Voc    DMP    3      M   6.5\n40  B.Voc    DMP    3      M   9.5\n41  B.Voc    DMP    3      M   9.5\n42  B.Voc    DMP    3      M   9.0\n43  B.Voc    DMP    3      F   9.5\n44  B.Voc    DMP    3      F   9.5\n45  B.Voc    DMP    3      F   6.5\n46  B.Voc    DMP    3      M   5.0\n47  B.Voc   UIID    1      M   9.0\n48  B.Voc   UIID    1      F   7.0\n49  B.Voc   UIID    1      F   7.0\n50  B.Voc   UIID    1      F   8.0\n51  B.Voc   UIID    1      F   8.0\n52  B.Voc   UIID    1      F   9.0\n53  B.Voc   UIID    2      M   8.0\n54  B.Voc   GADP    1      F   8.0\n55  B.Voc   GADP    2      M   8.0\n56  B.Voc   Film    3      M   8.0\n57  B.Voc   GADP    3      F   9.0\n58  B.Voc   GADP    3      F   9.0\n59  B.Voc    DMP    2      M   8.0\n60  B.Voc    DMP    2      M   8.0\n61   B.FA    CAP    3      F   8.0\n62   B.FA    CAP    3      M   9.0\n63   B.FA    CAP    3      F   9.0\n64   B.FA    DMA    2      M   8.5\n65   B.FA    DMA    2      M   8.0\n66   B.FA    DMA    2      F   8.0\n67   B.FA    CAP    2      F   8.0\n68   B.FA    CAP    2      F   8.0\n69   B.FA   Film    3      M   6.0\n70   B.FA    DMA    2      F   8.0\n71   B.FA    DMA    2      F   7.0\n72   B.FA    DMA    2      F   7.0\n73   B.FA    DMA    2      F   8.0\n74   B.FA    DMA    2      F   7.0\n75   B.FA    DMA    2      M   6.0\n76   B.FA   Film    3      M   8.0\n77   B.FA   Film    3      M   8.0\n78   B.FA    DMA    2      M   8.0\n79   B.FA    DMA    2      M   8.0\n80   B.FA    DMA    2      F   7.0\n81   B.FA    DMA    2      F   7.0\n82   B.FA   Film    2      F   8.0\n83   B.FA   Film    3      F   7.0\n84   B.FA    DMA    2      F   9.0\n85   B.FA    DMA    2      M   8.0\n86   B.FA    DMA    2      F   7.0\n87   B.FA    DMA    2      F   7.0\n88   B.FA    DMA    2      M   7.0\n89   B.FA    DMA    2      F   7.0\n90   B.FA    DMA    2      F   7.0"
  },
  {
    "objectID": "posts/Experiment2/index.html",
    "href": "posts/Experiment2/index.html",
    "title": "Experiment 2",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact"
  },
  {
    "objectID": "posts/Experiment2/index.html#defining-the-research-experiment",
    "href": "posts/Experiment2/index.html#defining-the-research-experiment",
    "title": "Experiment 2",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nGoal of the Experiment: To determine if there is a significant difference in the amount of money given as tip among vegetarian and non-vegetarian young adults.\nMethodology\nSampling: The population consists of students on a college campus who have ordered food through online delivery services like Swiggy or Zomato recently. The method of online platforms must have been chosen since it is easy to access and recollect the amount paid as tip this way and tipping in restaurants is not too socially expected in India unlike in the West. The sample population is a randomly chosen group of students from Manipal Academy of Higher Education, Bangalore, ensuring a hopefully balanced set of participants that is a true representation of the population.\nMethod Followed- To ensure a lack of bias, a coin was tossed to even determine if data would be collected from a person or not. This lead to random sampling ensure diversity in age, year of study, and socioeconomic background.\nSample Size: We have a total of 60 participants 30 vegetarians and 30 non-vegetarians.\nWhy could knowing this be useful?\n\nIt can provide insights for businesses like restaurants or food delivery services in understanding the financial behaviors of specific demographic segments.\nUnderstanding these patterns can help employers design compensation and customer engagement strategies that ensure fair tipping opportunities for their staff.\n\n\nReading and Analyzing the data:\n\ntips_data &lt;- read.csv(\"../../data/tip.csv\")\ntips_data\n\n        Name Gender Preferance Tip\n1      Aanya Female        Veg   0\n2       Adit   Male        Veg   0\n3      Aditi Female        Veg  20\n4      Akash   Male    Non-veg   0\n5    Akshita Female    Non-veg   0\n6   Anandita Female    Non-veg   0\n7     Ananya Female    Non-veg  20\n8      Anaya Female        Veg  35\n9     Anhuya Female        Veg  40\n10     Ankit   Male    Non-veg   0\n11     Anmol   Male        Veg   0\n12      Anna   Male        Veg   0\n13  Anoushka Female    Non-veg   0\n14     Arnav   Male    Non-veg   0\n15    Arushi Female    Non-veg   0\n16  Ashutosh   Male    Non-veg   0\n17     Asark Female    Non-veg  20\n18      Ayan   Male        Veg   0\n19      Debu   Male        Veg  15\n20   Dheeman   Male        Veg 100\n21      Diya Female        Veg  30\n22   Hardik    Male    Non-veg   0\n23  Ignatius   Male    Non-veg  20\n24    Khushi Female        Veg   0\n25   Kshraja Female        Veg   0\n26  Mahendra   Male    Non-veg  20\n27     Mahie Female        Veg   0\n28    Manish   Male        Veg   0\n29  Nanditha Female        Veg   0\n30    Nevaan   Male    Non-veg   0\n31     Nidhi Female        Veg   0\n32   Nishant   Male    Non-veg  20\n33     Nitya Female    Non-veg   0\n34      Ojas   Male        Veg  20\n35  Praneeta Female    Non-veg  50\n36 Priyanshu   Male    Non-veg  20\n37     Radha Female    Non-veg  20\n38      Reva Female    Non-veg  20\n39     Rikin   Male    Non-veg  30\n40    Sadnya Female        Veg   0\n41   Sanjana Female        Veg   0\n42     Sarah Female    Non-veg   0\n43   Shaivii Female    Non-veg   0\n44  Shashwat   Male        Veg   0\n45     Shiva   Male        Veg   0\n46    Shreya Female        Veg   0\n47    Symran Female    Non-veg  20\n48    Simran Female    Non-veg   0\n49   Sourabh   Male    Non-veg   0\n50    Srujan   Male        Veg   0\n51    Suhaas   Male        Veg  20\n52    Suhani Female    Non-veg  20\n53  Sushmita Female        Veg  20\n54     Taran   Male        Veg  20\n55     Varad   Male        Veg   0\n56    Vedant   Male    Non-veg  20\n57     Vinay   Male    Non-veg   0\n58   Vishesh   Male        Veg   0\n59    Vishnu   Male    Non-veg   0\n60    Zivnya Female        Veg  50\n\n\n\nglimpse(tips_data)\n\nRows: 60\nColumns: 4\n$ Name       &lt;chr&gt; \"Aanya\", \"Adit\", \"Aditi\", \"Akash\", \"Akshita\", \"Anandita\", \"…\n$ Gender     &lt;chr&gt; \"Female\", \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Fe…\n$ Preferance &lt;chr&gt; \"Veg\", \"Veg\", \"Veg\", \"Non-veg\", \"Non-veg\", \"Non-veg\", \"Non-…\n$ Tip        &lt;int&gt; 0, 0, 20, 0, 0, 0, 20, 35, 40, 0, 0, 0, 0, 0, 0, 0, 20, 0, …\n\n\n\n\nDefining the Data Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nName\nThe name of the participant\nQualitative\n\n\nGender\nThe Gender (Male/Female) of the participant\nQualitative\n\n\nPreferance\nThe dietary preference of the participant (Veg / Non-veg)\nQualitative\n\n\nTip\nThe amount of money paid as tip to the participant’s online delivery partner in their last order\nQuantitative\n\n\n\nObservations:\n\nVariables ‘Gender’ and ‘Preferance’ need to be transformed to be factors.\nPreference is spelled wrong!\n\n\n\nTransforming and inspecting the data:\n\ntips_modified &lt;- tips_data %&gt;%\n  dplyr::mutate(\n    Gender = as_factor(Gender),\n    Preferance = factor(Preferance,\n      levels = c(\"Veg\", \"Non-veg\"),\n      labels = c(\"Veg\", \"Non-veg\"),\n      ordered = TRUE)\n  )\ntips_modified\n\n        Name Gender Preferance Tip\n1      Aanya Female        Veg   0\n2       Adit   Male        Veg   0\n3      Aditi Female        Veg  20\n4      Akash   Male    Non-veg   0\n5    Akshita Female    Non-veg   0\n6   Anandita Female    Non-veg   0\n7     Ananya Female    Non-veg  20\n8      Anaya Female        Veg  35\n9     Anhuya Female        Veg  40\n10     Ankit   Male    Non-veg   0\n11     Anmol   Male        Veg   0\n12      Anna   Male        Veg   0\n13  Anoushka Female    Non-veg   0\n14     Arnav   Male    Non-veg   0\n15    Arushi Female    Non-veg   0\n16  Ashutosh   Male    Non-veg   0\n17     Asark Female    Non-veg  20\n18      Ayan   Male        Veg   0\n19      Debu   Male        Veg  15\n20   Dheeman   Male        Veg 100\n21      Diya Female        Veg  30\n22   Hardik    Male    Non-veg   0\n23  Ignatius   Male    Non-veg  20\n24    Khushi Female        Veg   0\n25   Kshraja Female        Veg   0\n26  Mahendra   Male    Non-veg  20\n27     Mahie Female        Veg   0\n28    Manish   Male        Veg   0\n29  Nanditha Female        Veg   0\n30    Nevaan   Male    Non-veg   0\n31     Nidhi Female        Veg   0\n32   Nishant   Male    Non-veg  20\n33     Nitya Female    Non-veg   0\n34      Ojas   Male        Veg  20\n35  Praneeta Female    Non-veg  50\n36 Priyanshu   Male    Non-veg  20\n37     Radha Female    Non-veg  20\n38      Reva Female    Non-veg  20\n39     Rikin   Male    Non-veg  30\n40    Sadnya Female        Veg   0\n41   Sanjana Female        Veg   0\n42     Sarah Female    Non-veg   0\n43   Shaivii Female    Non-veg   0\n44  Shashwat   Male        Veg   0\n45     Shiva   Male        Veg   0\n46    Shreya Female        Veg   0\n47    Symran Female    Non-veg  20\n48    Simran Female    Non-veg   0\n49   Sourabh   Male    Non-veg   0\n50    Srujan   Male        Veg   0\n51    Suhaas   Male        Veg  20\n52    Suhani Female    Non-veg  20\n53  Sushmita Female        Veg  20\n54     Taran   Male        Veg  20\n55     Varad   Male        Veg   0\n56    Vedant   Male    Non-veg  20\n57     Vinay   Male    Non-veg   0\n58   Vishesh   Male        Veg   0\n59    Vishnu   Male    Non-veg   0\n60    Zivnya Female        Veg  50"
  },
  {
    "objectID": "posts/Experiment2/index.html#desciptive-analysis",
    "href": "posts/Experiment2/index.html#desciptive-analysis",
    "title": "Experiment 2",
    "section": "Desciptive Analysis:",
    "text": "Desciptive Analysis:\n\nglimpse(tips_modified)\n\nRows: 60\nColumns: 4\n$ Name       &lt;chr&gt; \"Aanya\", \"Adit\", \"Aditi\", \"Akash\", \"Akshita\", \"Anandita\", \"…\n$ Gender     &lt;fct&gt; Female, Male, Female, Male, Female, Female, Female, Female,…\n$ Preferance &lt;ord&gt; Veg, Veg, Veg, Non-veg, Non-veg, Non-veg, Non-veg, Veg, Veg…\n$ Tip        &lt;int&gt; 0, 0, 20, 0, 0, 0, 20, 35, 40, 0, 0, 0, 0, 0, 0, 0, 20, 0, …\n\n\n\nskim(tips_modified)\n\n\nData summary\n\n\nName\ntips_modified\n\n\nNumber of rows\n60\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1\n4\n9\n0\n60\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n2\nFem: 30, Mal: 30\n\n\nPreferance\n0\n1\nTRUE\n2\nVeg: 30, Non: 30\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTip\n0\n1\n11.17\n17.84\n0\n0\n0\n20\n100\n▇▁▁▁▁\n\n\n\n\n\nObservations:\n\nThere are no missing values in the data-set.\nThere are 30 vegetarian and 30 non-vegetarian entries in the Food Preference factor. There are 30 male and 30 female entries in the Gender factor. Is it so that data was collected in a way that there are 15 male and 15 female vegetarians and 15 male and 15 female non-vegetarians? I should summerise and check!\nIt’s interesting to see that the progression of p0 to p100 is a series of 0, 0, 0, 20 and 100. The smallest tip given is 0 while the biggest is 100. This also indicates a large number of 0 -no tip in the data set compared to other values. It’s most likely a very right skewed distribution. It would be interesting to analyse who contributed to more of these 0s or if everyone is equally kanjoos regardless of food preference or gender.\n\n\nSummeries and Visualisations of the data\nDistribution of overall tips:\n\ntips_modified %&gt;%\n  gf_histogram(~Tip)\n\n\n\n\n\n\n\n\nObservations:\n\nLike i had thought inspecting the day, a majority of participants have not given any tip. It as a right skewed distribution.\n\nAmong people who do tip, 20rs is the most tipped value of money.\nIs tipping the value 100 an outlier?\n\n\ntips_modified %&gt;%\n  group_by(Preferance, Gender) %&gt;%\n  summarize(count =n()) \n\n`summarise()` has grouped output by 'Preferance'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Preferance [2]\n  Preferance Gender count\n  &lt;ord&gt;      &lt;fct&gt;  &lt;int&gt;\n1 Veg        Female    15\n2 Veg        Male      15\n3 Non-veg    Female    15\n4 Non-veg    Male      15\n\n\nObservation: My previous hypothesis that there are 15 male and 15 female vegetarians and 15 male and 15 female non-vegetarians. This must have been done to remove any gender bias from our analysis.\nIt gives us the opportunity to also inspect another aspect to tipping - if gender plays a roll or not- it works either ways- on doing it in this way, our analysis has no bias based on dietary decisions. For now, let’s continue with the Food preference tipping analysis.\nTips faceted by Food Preference:\n\ngf_histogram(~Tip, fill = ~Preferance, data = tips_modified, bins=100) %&gt;%\n  gf_labs(title = \"Tips faceted by Food Preference\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ntips_modified %&gt;%\n  gf_histogram(~Tip|Preferance, fill = ~Preferance, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Tips faceted by Food Preference\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1 \n    )\n  ))\n\n\n\n\n\n\n\n\n\ntips_modified %&gt;%\n  gf_density(\n    ~ Tip,\n    fill = ~ Preferance,\n    alpha = 0.5,\n    title = \"Money Spent Densities\",\n    subtitle = \"Vegetarians vs Non-Vegetarians\"\n  )\n\n\n\n\n\n\n\n\nObservations:\n\nBoth groups show a right-skewed distribution with a high count of low tip (0) amount. The count of participants giving no tip appears to be similar for both groups.\nThese graphs suggest that While the count of tips (below 25) appears to be higher for non-vegetarians, The count of higher tips (above 25), while low for both groups, is relatively higher for vegetarians.\nMaybe food preference might not have a strong influence on the tip amounts given by students? Or maybe Vegetarians tip more than non vegetarians in the sense that when they vegetarians do give tips, they tend to give a higher amounts than non- vegetarians do?\n\n\nCrosstables:\nTrying to find the difference in means and medians of tips given based on the difference in food preference within the sample population to get a better understanding of what the data is saying:\n\ncrosstable(Tip ~ Preferance, data = tips_modified) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariablePreferanceVegNon-vegTipMin / Max0 / 100.00 / 50.0Med [IQR]0 [0;20.0]0 [0;20.0]Mean (std)12.3 (21.9)10.0 (12.9)N (NA)30 (0)30 (0)\n\n\nObservations (for mean and standard deviation):\n\nThe mean value of the tips given by vegetarians(12.3) is higher than that given by non- vegetarians(10) which makes sense considering our earlier observation of the fact that the giving of higher tips is more common among vegetarians.\nThe higher standard deviation in the Vegetarian group-21.9 suggests that they have more variability in tipping behavior compared to Non-vegetarians-12.9.\n\nVisualizing the Median and IQR ranges:\n\ntips_modified %&gt;%\n  gf_boxplot(Preferance ~ Tip, fill = ~Preferance, alpha=0.5) %&gt;%\n  gf_labs(title = \"Box plot of Tip filled with Food Preference\")\n\n\n\n\n\n\n\n\nObservations:\n\nThe median tip for both groups is 0, indicating that at least half of the participants from both groups tipped nothing.\nThe IQR range for both groups is 0 to 20 i.e. 50% of the tips for both groups falls within this range.\nDoes this suggest that most participants, regardless of food preference, tend to give low tips, with the Vegetarian group having one visible outlier that skews its range higher?\n\n\n\n\nForming my Hypothesis:\nNull Hypothesis: There is no difference in the average tip amount between vegetarians and non-vegetarians.\nAlternative Hypothesis: Vegetarians tend give larger values as tips than non-vegetarians.\nIt’s not possible to come to any concrete conclusion with just the descriptive analysis of the data, and so, I move to conduct statistical analysis on this data."
  },
  {
    "objectID": "posts/Experiment2/index.html#statistical-analysis",
    "href": "posts/Experiment2/index.html#statistical-analysis",
    "title": "Experiment 2",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nA t-test:\nAssumptions: 1. The distribution for each group - male and female is normal.\n\nmosaic::t_test(Tip ~ Preferance, data = tips_modified)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     2.33      12.3        10     0.503   0.617      46.9    -6.99      11.7\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObservations:\n\nThe estimate suggests that vegetarians, on average, give 2.3rs more tip than non-vegetarians.\nThe high p-value of 0.6 1, greater than the typical significance level of 0.05, suggests that there is no statistically significant difference in the average tip amounts between vegetarians and non-vegetarians.The result could just be random, so I don’t have strong evidence that something is happening.\n0- which indicates that there is no difference on the average tip, falls within the confidence intervals which suggest the same.\nTherefore we cannot reject the null hypothesis.\n\nWhat if my assumption of these 2 distribution of tip faceted by Food Preference being normal is not right? We could do a shapiro test to confirm.\n\n\nA shapiro test:\nShapiro test is one used to determine if a sample comes from a normally distributed population or not.\nFirst we transform our data set to be 2 different ones- one with all the Veg values and the other with all the Non-veg values.\n\nfiltered_Veg &lt;- tips_modified %&gt;%\n  filter(Preferance == \"Veg\")\nfiltered_Non &lt;- tips_modified %&gt;%\n  filter(Preferance == \"Non-veg\")\nfiltered_Veg\n\n       Name Gender Preferance Tip\n1     Aanya Female        Veg   0\n2      Adit   Male        Veg   0\n3     Aditi Female        Veg  20\n4     Anaya Female        Veg  35\n5    Anhuya Female        Veg  40\n6     Anmol   Male        Veg   0\n7      Anna   Male        Veg   0\n8      Ayan   Male        Veg   0\n9      Debu   Male        Veg  15\n10  Dheeman   Male        Veg 100\n11     Diya Female        Veg  30\n12   Khushi Female        Veg   0\n13  Kshraja Female        Veg   0\n14    Mahie Female        Veg   0\n15   Manish   Male        Veg   0\n16 Nanditha Female        Veg   0\n17    Nidhi Female        Veg   0\n18     Ojas   Male        Veg  20\n19   Sadnya Female        Veg   0\n20  Sanjana Female        Veg   0\n21 Shashwat   Male        Veg   0\n22    Shiva   Male        Veg   0\n23   Shreya Female        Veg   0\n24   Srujan   Male        Veg   0\n25   Suhaas   Male        Veg  20\n26 Sushmita Female        Veg  20\n27    Taran   Male        Veg  20\n28    Varad   Male        Veg   0\n29  Vishesh   Male        Veg   0\n30   Zivnya Female        Veg  50\n\nfiltered_Non\n\n        Name Gender Preferance Tip\n1      Akash   Male    Non-veg   0\n2    Akshita Female    Non-veg   0\n3   Anandita Female    Non-veg   0\n4     Ananya Female    Non-veg  20\n5      Ankit   Male    Non-veg   0\n6   Anoushka Female    Non-veg   0\n7      Arnav   Male    Non-veg   0\n8     Arushi Female    Non-veg   0\n9   Ashutosh   Male    Non-veg   0\n10     Asark Female    Non-veg  20\n11   Hardik    Male    Non-veg   0\n12  Ignatius   Male    Non-veg  20\n13  Mahendra   Male    Non-veg  20\n14    Nevaan   Male    Non-veg   0\n15   Nishant   Male    Non-veg  20\n16     Nitya Female    Non-veg   0\n17  Praneeta Female    Non-veg  50\n18 Priyanshu   Male    Non-veg  20\n19     Radha Female    Non-veg  20\n20      Reva Female    Non-veg  20\n21     Rikin   Male    Non-veg  30\n22     Sarah Female    Non-veg   0\n23   Shaivii Female    Non-veg   0\n24    Symran Female    Non-veg  20\n25    Simran Female    Non-veg   0\n26   Sourabh   Male    Non-veg   0\n27    Suhani Female    Non-veg  20\n28    Vedant   Male    Non-veg  20\n29     Vinay   Male    Non-veg   0\n30    Vishnu   Male    Non-veg   0\n\n\nVisualizing the distribution we acquire vs a normal distribution for each group:\n\ntips_modified %&gt;%\n  gf_density( ~ Tip,\n              fill = ~ Preferance,\n              alpha = 0.5,\n              title = \"Tip given\") %&gt;%\n  gf_facet_grid(~ Preferance) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\nThese graphs don;t seem to suggest that these 2 distributions are normally distributed. does the shapiro test say the same?\n\nFor Vegetarians:\nNull hypothesis: The money given as tip by Vegetarians is normally distributed.\n\nshapiro.test(filtered_Veg$Tip)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic     p.value method                     \n      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n1     0.629 0.000000166 Shapiro-Wilk normality test\n\n\nA high w (statistic) value - one greater than 0.9 most likely indicates that a distribution is normal. With a w value so low : 0.62, we can reject the null hypothesis that it is normally distributed. The p-value suggests the same- it’s less than 0.05 which confirms that the w value we received is statistically significant, it did hot happen by chance.\nTherefore, the distribution of tips given by vegetarians are not normally distributed.\n\n\nFor Non-Vegetarians:\nNull hypothesis: The money given as tip by Non-Vegetarians is normally distributed.\n\nshapiro.test(filtered_Non$Tip)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic    p.value method                     \n      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                      \n1     0.717 0.00000275 Shapiro-Wilk normality test\n\n\nThe conclusions of this Shapiro test is similar to the last- a low w which indicates that the distribution is most likely not normally distributed. Though a little more than the the previous one. We confirm this by assessing the p-value of the test: a very low value confirming that this was not by chance.\nTherefore, the distribution of tips given by Non-vegetarians are not normally distributed.\nAnd so we through out any inferences we pulled from the t test- our assumption has been proven to be false- making it invalid.\n\n\n\nMann-Whitney Test\nThe Mann-Whitney Test is used to compare two independent groups when the data doesn’t follow a normal distribution.\n\nwilcox.test(Tip ~ Preferance, data = tips_modified, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact confidence intervals with ties\n\n\n# A tibble: 1 × 7\n    estimate statistic p.value     conf.low conf.high method         alternative\n       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;      \n1 -0.0000372       437   0.833 -0.000000989 0.0000335 Wilcoxon rank… two.sided  \n\n\nOkay i have a doubt- The test does not directly indicate which group (variable) has higher values. If i look at my skim or even my estimate1 in t-test, Veg is the first group, I also ordered my levels to be Veg first, so that’s how it works here too? Yep, that’s what I’m going with.\n\nThe estimate suggests that non-vegetarians, on average, give 3.7rs more tip than vegetarians. This does not align with my t-test.\nThe p-value is 0.8(greater than 0.05) suggesting that the estimate result could easily /probably happened by chance, so it’s not considered significant. We therefore, cannot reject the null hypothesis.\n0- which indicates that there is no difference on the average tip, falls within the confidence intervals which suggests the same- we cannot reject the null hypothesis.\n\nI received an error saying “Warning: cannot compute exact p-value with tiesWarning: cannot compute exact confidence intervals with ties”. This means that because i have so may repeating values (I’m guessing mainly 0)- there are many values with the same rank and polarity. This could lead to not having a precise p-values and confidence interval result. Therefore, i cannot reject or accept the null hypothesis right away, to make sure, i will go ahead and perform a permutation test to come to a solid inference, one that i cannot question.\n\n\nPermutation Test\nThe permutation test is very flexible and requires minimal assumptions. It doesn’t assume normality or equal variances, making it useful for small or skewed data.\n\nobs_diff_pref &lt;- diffmean(Tip ~ Preferance, \n                            data = tips_modified) \n\nobs_diff_pref\n\n diffmean \n-2.333333 \n\n\nNon-veg mean - Veg mean = -2.3\nIn my sample data, vegetarians, on average, give 2.3rs more tip than non-vegetarians.\nCreating the Null Distribution by Permutation\n\nnull_dist_tips &lt;- \n  do(4999) * diffmean(data = tips_modified, \n                      Tip ~ shuffle(Preferance))\n##null_dist_tips , it appears too long when i render it!\n\nAcquiring the p-value:\nThe p-value is calculated by comparing the obs_diff_pref (from the actual data) to the null distribution acquired through permutation. It represents the proportion of times a difference as extreme as the observed one occurs by random chance alone.\n\nprop1(~ diffmean &lt;= obs_diff_pref, data = null_dist_tips)\n\nprop_TRUE \n     0.34 \n\n\nVisualizing this:\n\ngf_histogram(data = null_dist_tips, ~ diffmean, \n             bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_pref, \n           colour = \"darkred\", linewidth = 1,\n           title = \"Null Distribution by Permutation\", \n           subtitle = \"Histogram\") %&gt;% \n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\nthis is what is happening here."
  },
  {
    "objectID": "posts/day-7/index.html",
    "href": "posts/day-7/index.html",
    "title": "Day 7",
    "section": "",
    "text": "We are working on Inference for Comparing Two Paired Means\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(broom) # Tidy Test data\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\n\n\nAttaching package: 'resampledata3'\n\nThe following object is masked from 'package:datasets':\n\n    Titanic\n\nlibrary(gt) # for tables\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following object is masked from 'package:gt':\n\n    sp500\n\nThe following object is masked from 'package:mosaic':\n\n    dotPlot\n\nThe following objects are masked from 'package:lattice':\n\n    ethanol, lsegments"
  },
  {
    "objectID": "posts/day-7/index.html#introduction",
    "href": "posts/day-7/index.html#introduction",
    "title": "Day 7",
    "section": "",
    "text": "We are working on Inference for Comparing Two Paired Means\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(broom) # Tidy Test data\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\n\n\nAttaching package: 'resampledata3'\n\nThe following object is masked from 'package:datasets':\n\n    Titanic\n\nlibrary(gt) # for tables\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following object is masked from 'package:gt':\n\n    sp500\n\nThe following object is masked from 'package:mosaic':\n\n    dotPlot\n\nThe following objects are masked from 'package:lattice':\n\n    ethanol, lsegments"
  },
  {
    "objectID": "posts/day-7/index.html#case-study-1-results-from-a-diving-championship",
    "href": "posts/day-7/index.html#case-study-1-results-from-a-diving-championship",
    "title": "Day 7",
    "section": "Case Study #1: Results from a Diving Championship",
    "text": "Case Study #1: Results from a Diving Championship\n\nInspecting and Charting Data\n\ndata(\"Diving2017\", package = \"resampledata3\")\nDiving2017\n\n                     Name     Country Semifinal  Final\n1        CHEONG Jun Hoong    Malaysia    325.50 397.50\n2                SI Yajie       China    382.80 396.00\n3                REN Qian       China    367.50 391.95\n4              KIM Mi Rae North Korea    346.00 385.55\n5              WU Melissa   Australia    318.70 370.20\n6           KIM Kuk Hyang North Korea    360.85 360.00\n7         ITAHASHI Minami       Japan    313.70 357.85\n8        BENFEITO Meaghan      Canada    355.15 331.40\n9          PAMG Pandelela    Malaysia    322.75 322.40\n10        CHAMANDY Olivia      Canada    320.55 307.15\n11       PARRATTO Jessica         USA    322.75 302.35\n12 MURILLO URREA Carolina    Colombia    325.75 283.35\n\nDiving2017_inspect &lt;- inspect(Diving2017)\nDiving2017_inspect$categorical\n\n# A tibble: 2 × 6\n  name    class  levels     n missing distribution                              \n  &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                                     \n1 Name    factor     12    12       0 \" SI Yajie (8.3%) ...                    …\n2 Country factor      8    12       0 \"Canada (16.7%), China (16.7%) ...       …\n\nDiving2017_inspect$quantitative\n\n# A tibble: 2 × 11\n  name      class     min    Q1 median    Q3   max  mean    sd     n missing\n* &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;\n1 Semifinal numeric  314.  322.   326.  357.  383.  338.  22.9    12       0\n2 Final     numeric  283.  319.   359.  387.  398.  350.  40.0    12       0\n\n\n\nDiving2017_long &lt;- Diving2017 %&gt;%\n  pivot_longer(\n    cols = c(Final, Semifinal),\n    names_to = \"race\",\n    values_to = \"scores\"\n  )\nDiving2017_long\n\n# A tibble: 24 × 4\n   Name               Country     race      scores\n   &lt;fct&gt;              &lt;fct&gt;       &lt;chr&gt;      &lt;dbl&gt;\n 1 \"CHEONG Jun Hoong\" Malaysia    Final       398.\n 2 \"CHEONG Jun Hoong\" Malaysia    Semifinal   326.\n 3 \" SI Yajie\"        China       Final       396 \n 4 \" SI Yajie\"        China       Semifinal   383.\n 5 \"REN Qian\"         China       Final       392.\n 6 \"REN Qian\"         China       Semifinal   368.\n 7 \"KIM Mi Rae\"       North Korea Final       386.\n 8 \"KIM Mi Rae\"       North Korea Semifinal   346 \n 9 \"WU Melissa\"       Australia   Final       370.\n10 \"WU Melissa\"       Australia   Semifinal   319.\n# ℹ 14 more rows\n\n\n\nDiving2017_long %&gt;%\n  gf_density(~scores,\n    fill = ~race,\n    alpha = 0.5,\n    title = \"Diving Scores\"\n  ) %&gt;%\n  gf_facet_grid(~race) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\nThe data are not normally distributed.\n\nDiving2017_long %&gt;%\n  gf_col(\n    fct_reorder(Name, scores) ~ scores,\n    fill = ~race,\n    alpha = 0.5,\n    position = \"dodge\",\n    xlab = \"Scores\",\n    ylab = \"Name\",\n    title = \"Diving Scores\"\n  )\n\n\n\n\n\n\n\n\n\nThere is no immediately identifiable trend in score changes from one race to the other.\n\n\nDiving2017_long %&gt;%\n  gf_boxplot(\n    scores ~ race,\n    fill = ~race,\n    alpha = 0.5,\n    xlab = \"Race\",\n    ylab = \"Scores\",\n    title = \"Diving Scores\"\n  )\n\n\n\n\n\n\n\n\n\nAlthough the two medians appear to be different, the box plots overlap considerably. So one cannot visually conclude that the two sets of race timings have different means."
  },
  {
    "objectID": "posts/day-7/index.html#frog-data",
    "href": "posts/day-7/index.html#frog-data",
    "title": "Day 7",
    "section": "Frog data:",
    "text": "Frog data:\n\nlibrary(patchwork) # Arranging Plots\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(supernova) # Beginner-Friendly ANOVA Tables\n\n\nfrogs_orig &lt;- read_csv(\"../../data/frogs.csv\")\n\nRows: 60 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Frogspawn sample id, Temperature13, Temperature18, Temperature25\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfrogs_orig\n\n# A tibble: 60 × 4\n   `Frogspawn sample id` Temperature13 Temperature18 Temperature25\n                   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1                     1            24            NA            NA\n 2                     2            NA            21            NA\n 3                     3            NA            NA            18\n 4                     4            26            NA            NA\n 5                     5            NA            22            NA\n 6                     6            NA            NA            14\n 7                     7            27            NA            NA\n 8                     8            NA            22            NA\n 9                     9            NA            NA            15\n10                    10            27            NA            NA\n# ℹ 50 more rows\n\n\nOur response variable is the hatching Time. Our explanatory variable is a factor, Temperature, with 3 levels: 13°C, 18°C and 25°C. \n\nClean the data:\n\nfrogs_orig %&gt;%\n  pivot_longer(\n    .,\n    cols = starts_with(\"Temperature\"),\n    cols_vary = \"fastest\",\n    # new in pivot_longer\n    names_to = \"Temp\",\n    values_to = \"Time\"\n  ) %&gt;%\n  drop_na() %&gt;%\n  ##\n  separate_wider_regex(\n    cols = Temp,\n    # knock off the unnecessary \"Temperature\" word\n    # Just keep the digits thereafter\n    patterns = c(\"Temperature\", TempFac = \"\\\\d+\"),\n    cols_remove = TRUE\n  ) %&gt;%\n  # Convert Temp into TempFac, a 3-level factor\n  mutate(TempFac = factor(\n    x = TempFac,\n    levels = c(13, 18, 25),\n    labels = c(\"13\", \"18\", \"25\")\n  )) %&gt;%\n  rename(\"Id\" = `Frogspawn sample id`) -&gt; frogs_long\nfrogs_long\n\n# A tibble: 60 × 3\n      Id TempFac  Time\n   &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;\n 1     1 13         24\n 2     2 18         21\n 3     3 25         18\n 4     4 13         26\n 5     5 18         22\n 6     6 25         14\n 7     7 13         27\n 8     8 18         22\n 9     9 25         15\n10    10 13         27\n# ℹ 50 more rows\n\n##\nfrogs_long %&gt;% count(TempFac)\n\n# A tibble: 3 × 2\n  TempFac     n\n  &lt;fct&gt;   &lt;int&gt;\n1 13         20\n2 18         20\n3 25         20\n\n\n\nThe dataset is reshaped from wide to long format, collecting temperature data into a single Temp column.\nMissing data is dropped.\nThe Temp column is cleaned to retain only the numeric values (13, 18, 25).\nThe temperature values are converted into a factor with three levels.\nThe sample ID column is renamed.\nFinally, a count is performed to see how many observations fall into each of the temperature categories (13, 18, or 25°C).\n\nThis process cleans and organizes the data, making it easier to analyze the effect of temperature on the frogs’ data.\n\n\nData Visualizations:\n\ngf_histogram(~Time,\n  fill = ~TempFac,\n  data = frogs_long, alpha = 0.5\n) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(\n    title = \"Histograms of Hatching Time Distributions vs Temperature\",\n    x = \"Hatching Time\", y = \"Count\"\n  ) %&gt;%\n  gf_text(7 ~ (mean(Time) + 2),\n    label = \"Overall Mean\"\n  ) %&gt;%\n  gf_refine(guides(fill = guide_legend(title = \"Temperature level (°C)\")))\n\n\n\n\n\n\n\n\nTemprature and hatching time seem to have a +ve corelation.\n\ngf_boxplot(\n  data = frogs_long,\n  Time ~ TempFac,\n  fill = ~TempFac,\n  alpha = 0.5\n) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(\n    title = \"Boxplots of Hatching Time Distributions vs Temperature\",\n    x = \"Temperature\", y = \"Hatching Time\",\n    caption = \"Using ggprism\"\n  ) %&gt;%\n  gf_refine(\n    scale_x_discrete(guide = \"prism_bracket\"),\n    guides(fill = guide_legend(title = \"Temperature level (°C)\"))\n  )\n\nWarning: The S3 guide system was deprecated in ggplot2 3.5.0.\nℹ It has been replaced by a ggproto system that can be extended.\n\n\n\n\n\n\n\n\n\nTemperature seems to have a huge effect of hatching time"
  },
  {
    "objectID": "posts/day-7/index.html#anova",
    "href": "posts/day-7/index.html#anova",
    "title": "Day 7",
    "section": "ANOVA",
    "text": "ANOVA\nA statistical test for 3 or more means:\n\nfrogs_anova &lt;- aov(Time ~ TempFac, data = frogs_long)\n\nErrorBar plot\n\nsupernova::pairwise(frogs_anova,\n  correction = \"Bonferroni\", # Try \"Tukey\"\n  alpha = 0.05, # 95% CI calculation\n  var_equal = TRUE, # We'll see\n  plot = TRUE\n)\n\n\n\n\n\n\n\n\n\n\n\n── Pairwise t-tests with Bonferroni correction ─────────────────────────────────\n\n\nModel: Time ~ TempFac\n\n\nTempFac\n\n\nLevels: 3\n\n\nFamily-wise error-rate: 0.049\n\n\n\n  group_1 group_2    diff pooled_se       t    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -5.861 -4.739 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.661 -9.539 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.361 -4.239 .0000\n\n\n\nsupernova::pairwise(frogs_anova,\n  correction = \"Tukey\",\n  alpha = 0.05, # 95% CI calculation\n  var_equal = TRUE, # We'll see\n  plot = TRUE\n)\n\n\n\n\n\n\n\n\n\n\n\n── Tukey's Honestly Significant Differences ────────────────────────────────────\n\n\nModel: Time ~ TempFac\n\n\nTempFac\n\n\nLevels: 3\n\n\nFamily-wise error-rate: 0.05\n\n\n\n  group_1 group_2    diff pooled_se       q    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -6.175 -4.425 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.975 -9.225 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.675 -3.925 .0000\n\n\nThe black points: Estimate difference in means of the hatching time i.e. differences in mean pair vise."
  },
  {
    "objectID": "posts/day-7/index.html#proportions",
    "href": "posts/day-7/index.html#proportions",
    "title": "Day 7",
    "section": "Proportions:",
    "text": "Proportions:\nCLT for proportions 1. sample proportion = population propotion 2. only go ahead with a sample if it has atleast 10 of the “minority”\n\ndata(yrbss, package = \"openintro\")\nyrbss\n\n# A tibble: 13,583 × 13\n     age gender grade hispanic race                     height weight helmet_12m\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;                     &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1    14 female 9     not      Black or African Americ…  NA      NA   never     \n 2    14 female 9     not      Black or African Americ…  NA      NA   never     \n 3    15 female 9     hispanic Native Hawaiian or Othe…   1.73   84.4 never     \n 4    15 female 9     not      Black or African Americ…   1.6    55.8 never     \n 5    15 female 9     not      Black or African Americ…   1.5    46.7 did not r…\n 6    15 female 9     not      Black or African Americ…   1.57   67.1 did not r…\n 7    15 female 9     not      Black or African Americ…   1.65  132.  did not r…\n 8    14 male   9     not      Black or African Americ…   1.88   71.2 never     \n 9    15 male   9     not      Black or African Americ…   1.75   63.5 never     \n10    15 male   10    not      Black or African Americ…   1.37   97.1 did not r…\n# ℹ 13,573 more rows\n# ℹ 5 more variables: text_while_driving_30d &lt;chr&gt;, physically_active_7d &lt;int&gt;,\n#   hours_tv_per_school_day &lt;chr&gt;, strength_training_7d &lt;int&gt;,\n#   school_night_hours_sleep &lt;chr&gt;"
  },
  {
    "objectID": "posts/Experiment1/index.html",
    "href": "posts/Experiment1/index.html",
    "title": "Experiment 1",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact"
  },
  {
    "objectID": "posts/Experiment1/index.html#defining-the-research-experiment",
    "href": "posts/Experiment1/index.html#defining-the-research-experiment",
    "title": "Experiment 1",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nGoal of the Experiment: To determine if there is a significant difference in the amount of money spent by male and female students on a college campus on a random day of the month, in this case, 23rd October- 2024. This could speak for the amount of money each group receives to spend i.e. the pocket money they receive.\nMethodology\nSampling: A representative sample of students from Manipal Academy of Higher Education, ensuring a hopefully balanced number of male and female participants to avoid bias.\nMethod Followed- To ensure a lack of bias, a coin was tossed to determine if data would be collected from a person or not. This lead to random sampling ensuring diversity in age, year of study, and socioeconomic background.\nSample Size: We had a total of 82 participants 41 males and 41 females.\nWhy could knowing this be useful?\n\nIt can reflect how parents or guardians may perceive the financial needs of each gender, potentially revealing implicit biases or expectations.\nIf data shows a discrepancy in pocket money that isn’t based on actual need, it could encourage families to reconsider their allocation practices.\nFor institutions, knowing whether one group typically receives less financial support could inform the design of need-based programs, financial aid, or scholarships."
  },
  {
    "objectID": "posts/Experiment1/index.html#reading-and-analyzing-the-data",
    "href": "posts/Experiment1/index.html#reading-and-analyzing-the-data",
    "title": "Experiment 1",
    "section": "Reading and Analyzing the data:",
    "text": "Reading and Analyzing the data:\n\npocket_money &lt;- read.csv(\"../../data/Pocket_Money.csv\")\npocket_money\n\n   Sr.no         Name Gender Money_spent\n1      1        Aagam   Male         150\n2      2       Aakash   Male         240\n3      3      Aarushi Female         382\n4      4      Abheeta Female          60\n5      5      Adithya   Male          68\n6      6       Aditya   Male         300\n7      7     Akanksha Female         270\n8      8       Amruta Female         190\n9      9       Anaaya Female         300\n10    10        Anish   Male           0\n11    11       Ankush   Male         250\n12    12      Anousha Female          85\n13    13     Anoushka Female         700\n14    14      Anushka Female         140\n15    15        Arjun   Male        1143\n16    16         Arya   Male         100\n17    17        Aryan   Male       10000\n18    18         Asra Female        1070\n19    19         Aziz   Male        1433\n20    20      Bhumika Female        1200\n21    21        Daana Female          66\n22    22          Dan   Male         600\n23    23    Deborishi   Male         205\n24    24     Devanshi Female         700\n25    25       Dhiman   Male         910\n26    26        Dhruv   Male         900\n27    27        Dhurv   Male         185\n28    28        Eisha Female         300\n29    29      Ezhilan   Male         842\n30    30         Geet   Male          50\n31    31       Harjot Female        3000\n32    32      Janhavi Female         150\n33    33         Jeff   Male         200\n34    34      Jeffrey   Male         150\n35    35      Kalyani Female           0\n36    36       Kartik   Male         145\n37    37       Kashvi Female         430\n38    38       Kavana Female         500\n39    39       Khushi Female         785\n40    40      Koustav   Male         250\n41    41        Krish   Male          70\n42    42     Kshirija Female         100\n43    43       Lekith   Male         100\n44    44       Maahin   Male         310\n45    45       Maanya Female          15\n46    46       Madhav   Male        4000\n47    47        Manav   Male          40\n48    48      Nandana Female           0\n49    49        Navya Female          55\n50    50    Nayantara Female         192\n51    51        Neeti Female         280\n52    52       Nithya Female          85\n53    53       Nivaan   Male         220\n54    54       Parisa Female          80\n55    55        Rikit   Male         259\n56    56        Risha Female           0\n57    57        Ritik   Male        1000\n58    58       Rudraj   Male         478\n59    59      Rukaiya Female          20\n60    60         Ryan   Male         330\n61    61       Shaivi Female         318\n62    62     Shashank   Male         400\n63    63     Shashwat   Male         149\n64    64        Shiva   Male         250\n65    65       Shreya Female         585\n66    66       Simran Female         340\n67    67 Simran Anand Female         660\n68    68      Snigdha Female         100\n69    69       Suhaas   Male         997\n70    70      Suprita Female         500\n71    71        Sutej   Male           0\n72    72       Tanmay Female         800\n73    73        Tanya Female           0\n74    74        Taran   Male        1399\n75    75       Tarini Female         200\n76    76     Tathastu   Male        1535\n77    77        Vanya Female         350\n78    78        Varad   Male         154\n79    79     Vasantha Female         418\n80    80   Vasundhara Female       13000\n81    81         Veer   Male         566\n82    82        Viraj   Male         315\n\n\n\nglimpse(pocket_money)\n\nRows: 82\nColumns: 4\n$ Sr.no       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Name        &lt;chr&gt; \"Aagam\", \"Aakash\", \"Aarushi\", \"Abheeta\", \"Adithya\", \"Adity…\n$ Gender      &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Male\", \"Femal…\n$ Money_spent &lt;int&gt; 150, 240, 382, 60, 68, 300, 270, 190, 300, 0, 250, 85, 700…\n\n\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nName\nThe name of the participant\nQualitative\n\n\nGender\nThe Gender (Male/Female) of the participant\nQualitative\n\n\nMoney_spent\nThe amount of money spent by the participant on 23rd October, 2024\nQuantitative\n\n\nX\nNA\n-\n\n\n\nObservations:\nThe variable gender needs to be mutated to be a factor, and there seems to be a column called ‘X’ that has no data in it.\n\nTansforming the data:\n\npockets_modified &lt;- pocket_money %&gt;%\n  dplyr::mutate(\n    Gender = as_factor(Gender)\n  )%&gt;%\n  select(Name, Gender, Money_spent)\npockets_modified\n\n           Name Gender Money_spent\n1         Aagam   Male         150\n2        Aakash   Male         240\n3       Aarushi Female         382\n4       Abheeta Female          60\n5       Adithya   Male          68\n6        Aditya   Male         300\n7      Akanksha Female         270\n8        Amruta Female         190\n9        Anaaya Female         300\n10        Anish   Male           0\n11       Ankush   Male         250\n12      Anousha Female          85\n13     Anoushka Female         700\n14      Anushka Female         140\n15        Arjun   Male        1143\n16         Arya   Male         100\n17        Aryan   Male       10000\n18         Asra Female        1070\n19         Aziz   Male        1433\n20      Bhumika Female        1200\n21        Daana Female          66\n22          Dan   Male         600\n23    Deborishi   Male         205\n24     Devanshi Female         700\n25       Dhiman   Male         910\n26        Dhruv   Male         900\n27        Dhurv   Male         185\n28        Eisha Female         300\n29      Ezhilan   Male         842\n30         Geet   Male          50\n31       Harjot Female        3000\n32      Janhavi Female         150\n33         Jeff   Male         200\n34      Jeffrey   Male         150\n35      Kalyani Female           0\n36       Kartik   Male         145\n37       Kashvi Female         430\n38       Kavana Female         500\n39       Khushi Female         785\n40      Koustav   Male         250\n41        Krish   Male          70\n42     Kshirija Female         100\n43       Lekith   Male         100\n44       Maahin   Male         310\n45       Maanya Female          15\n46       Madhav   Male        4000\n47        Manav   Male          40\n48      Nandana Female           0\n49        Navya Female          55\n50    Nayantara Female         192\n51        Neeti Female         280\n52       Nithya Female          85\n53       Nivaan   Male         220\n54       Parisa Female          80\n55        Rikit   Male         259\n56        Risha Female           0\n57        Ritik   Male        1000\n58       Rudraj   Male         478\n59      Rukaiya Female          20\n60         Ryan   Male         330\n61       Shaivi Female         318\n62     Shashank   Male         400\n63     Shashwat   Male         149\n64        Shiva   Male         250\n65       Shreya Female         585\n66       Simran Female         340\n67 Simran Anand Female         660\n68      Snigdha Female         100\n69       Suhaas   Male         997\n70      Suprita Female         500\n71        Sutej   Male           0\n72       Tanmay Female         800\n73        Tanya Female           0\n74        Taran   Male        1399\n75       Tarini Female         200\n76     Tathastu   Male        1535\n77        Vanya Female         350\n78        Varad   Male         154\n79     Vasantha Female         418\n80   Vasundhara Female       13000\n81         Veer   Male         566\n82        Viraj   Male         315"
  },
  {
    "objectID": "posts/Experiment1/index.html#staistical-analysis",
    "href": "posts/Experiment1/index.html#staistical-analysis",
    "title": "Experiment 1",
    "section": "Staistical Analysis",
    "text": "Staistical Analysis\nA t-test:\nAssumptions: 1. The distribution for each group - male and female is normal.\n\nmosaic::t_test(Money_spent ~ Gender, data = pockets_modified)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     55.3      749.      693.     0.136   0.893      76.5    -757.      868.\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObservations:\n\nThe estimate suggests that male students, on average, spent 55.29 more than female students.89\nSince the p-value value is as high as 0.8, we fail to reject the null hypothesis with just a t-test. This could suggests that there is no statistically significant difference between the spending of male and female students.The confidence interval of -757.07 and 867.66 suggest the same since it stadles 0 - our null hypothesis that gender does not play a role in the amount of money spent a day by a college student.\n\nBut is this t-test reliable?.. only if the the money spent by the sample is normally distributed. We test that using a Shapiro test.\n\nshapiro.test(pockets_modified$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.345 2.34e-17 Shapiro-Wilk normality test\n\n\nWe need to know if the individual distribution of both genders are normally distributed. And so we transform our data set to be 2- one with all the male values and one with all the female values.\nAcquiring the 2 data sets:\n\nfiltered_male &lt;- pockets_modified %&gt;%\n  filter(Gender == \"Male\")\nfiltered_female &lt;- pockets_modified %&gt;%\n  filter(Gender == \"Female\")\nfiltered_male\n\n        Name Gender Money_spent\n1      Aagam   Male         150\n2     Aakash   Male         240\n3    Adithya   Male          68\n4     Aditya   Male         300\n5      Anish   Male           0\n6     Ankush   Male         250\n7      Arjun   Male        1143\n8       Arya   Male         100\n9      Aryan   Male       10000\n10      Aziz   Male        1433\n11       Dan   Male         600\n12 Deborishi   Male         205\n13    Dhiman   Male         910\n14     Dhruv   Male         900\n15     Dhurv   Male         185\n16   Ezhilan   Male         842\n17      Geet   Male          50\n18      Jeff   Male         200\n19   Jeffrey   Male         150\n20    Kartik   Male         145\n21   Koustav   Male         250\n22     Krish   Male          70\n23    Lekith   Male         100\n24    Maahin   Male         310\n25    Madhav   Male        4000\n26     Manav   Male          40\n27    Nivaan   Male         220\n28     Rikit   Male         259\n29     Ritik   Male        1000\n30    Rudraj   Male         478\n31      Ryan   Male         330\n32  Shashank   Male         400\n33  Shashwat   Male         149\n34     Shiva   Male         250\n35    Suhaas   Male         997\n36     Sutej   Male           0\n37     Taran   Male        1399\n38  Tathastu   Male        1535\n39     Varad   Male         154\n40      Veer   Male         566\n41     Viraj   Male         315\n\nfiltered_female\n\n           Name Gender Money_spent\n1       Aarushi Female         382\n2       Abheeta Female          60\n3      Akanksha Female         270\n4        Amruta Female         190\n5        Anaaya Female         300\n6       Anousha Female          85\n7      Anoushka Female         700\n8       Anushka Female         140\n9          Asra Female        1070\n10      Bhumika Female        1200\n11        Daana Female          66\n12     Devanshi Female         700\n13        Eisha Female         300\n14       Harjot Female        3000\n15      Janhavi Female         150\n16      Kalyani Female           0\n17       Kashvi Female         430\n18       Kavana Female         500\n19       Khushi Female         785\n20     Kshirija Female         100\n21       Maanya Female          15\n22      Nandana Female           0\n23        Navya Female          55\n24    Nayantara Female         192\n25        Neeti Female         280\n26       Nithya Female          85\n27       Parisa Female          80\n28        Risha Female           0\n29      Rukaiya Female          20\n30       Shaivi Female         318\n31       Shreya Female         585\n32       Simran Female         340\n33 Simran Anand Female         660\n34      Snigdha Female         100\n35      Suprita Female         500\n36       Tanmay Female         800\n37        Tanya Female           0\n38       Tarini Female         200\n39        Vanya Female         350\n40     Vasantha Female         418\n41   Vasundhara Female       13000\n\n\nVisualizing the distribution we acquire vs a normal distribution:\n\npockets_modified %&gt;%\n  gf_density( ~ Money_spent,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"Money spent on 23rd Oct\") %&gt;%\n  gf_facet_grid(~ Gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\n\nConducting the Shapiro test for each of these distributions:\nFor males:\nNull hypothesis: The money spent by males is normally distributed.\n\nshapiro.test(filtered_male$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.407 1.14e-11 Shapiro-Wilk normality test\n\n\nThe statistic indicates how closely the sample data follows a normal distribution (the closer the value is the 1, the more normal the distribution is) and so the computed value of 0.40 says that the distribution is not normal.\nA p value so low- 1.135947e-11 indicates that\nFor females:\nNull hypothesis: The money spent by females is normally distributed.\n\nshapiro.test(filtered_female$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.296 8.96e-13 Shapiro-Wilk normality test\n\n\nSimilar to the inference for the distribution for males, even in females, we can reject the null hypothesis of the distribution being normal.\nwe go ahead and conduct a Mann-Whitney Test having proven that both distribution aren’t normal, proving the t-test to be invalid.\n\n\nMann-Whitney Test\n\nwilcox.test(Money_spent ~ Gender, data = pockets_modified, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact confidence intervals with ties\n\n\n# A tibble: 1 × 7\n  estimate statistic p.value conf.low conf.high method               alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;      \n1     55.0      936.   0.381    -70.0      180. Wilcoxon rank sum t… two.sided  \n\n\nObservations: The estimate of the difference between both groups is 55.00 a high p-value of 0.38 suggests that there is statistically no significant difference between the two groups’ median values i.e. we cannot possible reject the null hypothesis. 0 falls within the confidence interval which further suggest the same.\nI received an error saying “Warning: cannot compute exact p-value with ties”. This means that because i have so may repeating values (I’m guessing mainly 0)- there are many values with the same rank and polarity. This could lead to not having precise p-values.\nTherefore, i cannot reject or accept the null hypothesis right away, although the Mann-Whitney Test suggests that i cannot reject it. To make sure, i will go ahead and perform a permutation test to come to a solid inference, one that i cannot question.\n\n\nPermutation Test\n\nobs_diff_gender &lt;- diffmean(Money_spent ~ Gender, \n                            data = pockets_modified) \n\nobs_diff_gender\n\n diffmean \n-55.29268 \n\n\nHere we begin by creating a null distribution for the difference in means by performing 4,999 permutations- each engraving and different values among the 2 gender groups. We do this by randomly reassigning the ‘Gender’ variable across all observations\n\nnull_dist_money &lt;- \n  do(4999) * diffmean(data = pockets_modified, \n                      Money_spent ~ shuffle(Gender))\nnull_dist_money\n\n          diffmean\n1     696.36585366\n2    -598.90243902\n3    -697.58536585\n4     -62.31707317\n5     188.65853659\n6    -151.92682927\n7     249.68292683\n8     633.87804878\n9    -193.14634146\n10    -59.97560976\n11   -372.12195122\n12   -753.78048780\n13   -619.87804878\n14    -68.12195122\n15     24.60975610\n16   -555.14634146\n17     -8.46341463\n18    637.78048780\n19   -573.14634146\n20    151.00000000\n21   -692.95121951\n22   -104.90243902\n23    626.41463415\n24   -138.21951220\n25   -333.29268293\n26   -238.60975610\n27   -374.31707317\n28   -597.24390244\n29   -269.82926829\n30    -22.85365854\n31   -633.97560976\n32    423.00000000\n33    -49.73170732\n34   -363.82926829\n35     32.90243902\n36    256.51219512\n37    487.29268293\n38      4.60975610\n39     54.65853659\n40   -151.78048780\n41    647.78048780\n42    343.19512195\n43    471.34146341\n44   -654.41463415\n45   -676.56097561\n46    230.21951220\n47    195.58536585\n48   -428.02439024\n49   -314.56097561\n50     83.43902439\n51   -591.24390244\n52    641.92682927\n53     -1.78048780\n54   -515.73170732\n55    714.46341463\n56    -37.87804878\n57    -77.29268293\n58   -218.07317073\n59   -200.95121951\n60    314.07317073\n61    130.21951220\n62     67.34146341\n63    -94.60975610\n64   -247.73170732\n65   -267.19512195\n66   -719.97560976\n67      8.17073171\n68    617.24390244\n69    442.65853659\n70    547.53658537\n71   -384.21951220\n72    126.41463415\n73     79.24390244\n74   -323.68292683\n75    708.36585366\n76     71.14634146\n77    159.19512195\n78   -670.80487805\n79     54.36585366\n80    600.12195122\n81   -418.31707317\n82    130.95121951\n83   -518.65853659\n84   -669.87804878\n85   -595.92682927\n86    488.51219512\n87     82.41463415\n88   -523.92682927\n89    606.12195122\n90     -4.95121951\n91    222.70731707\n92    389.97560976\n93    285.04878049\n94   -520.80487805\n95    106.46341463\n96     42.26829268\n97     55.87804878\n98     59.48780488\n99   -266.26829268\n100    71.48780488\n101   151.34146341\n102  -172.26829268\n103   -13.04878049\n104   -19.68292683\n105  -569.29268293\n106  -484.46341463\n107  -350.80487805\n108   157.24390244\n109  -144.02439024\n110   -54.07317073\n111   483.87804878\n112   489.78048780\n113  -455.43902439\n114   -29.73170732\n115  -190.36585366\n116   -87.82926829\n117  -105.53658537\n118   318.60975610\n119   571.87804878\n120   -30.12195122\n121    92.95121951\n122    91.29268293\n123    53.63414634\n124  -144.07317073\n125  -377.82926829\n126  -631.87804878\n127   351.04878049\n128  -501.82926829\n129  -169.97560976\n130   -56.31707317\n131   403.97560976\n132  -740.02439024\n133   528.70731707\n134   468.31707317\n135   147.00000000\n136  -719.43902439\n137   279.68292683\n138   -48.65853659\n139  -206.80487805\n140   194.17073171\n141   536.31707317\n142   351.04878049\n143  -770.31707317\n144   644.17073171\n145   130.80487805\n146    49.29268293\n147    31.82926829\n148  -675.68292683\n149  -186.17073171\n150   530.85365854\n151  -370.02439024\n152  -737.34146341\n153   398.65853659\n154  -432.21951220\n155   574.85365854\n156  -337.97560976\n157    -7.58536585\n158   193.00000000\n159   212.85365854\n160   622.80487805\n161    40.02439024\n162   -94.80487805\n163  -508.60975610\n164    -8.80487805\n165   645.82926829\n166   474.85365854\n167  -608.17073171\n168   160.41463415\n169   592.60975610\n170  -463.29268293\n171    59.34146341\n172   162.80487805\n173  -702.60975610\n174    18.26829268\n175   341.34146341\n176  -146.56097561\n177   436.75609756\n178  -189.97560976\n179  -250.46341463\n180   250.21951220\n181   -83.39024390\n182  -500.46341463\n183  -223.19512195\n184    86.75609756\n185   175.48780488\n186  -382.21951220\n187  -235.24390244\n188  -301.82926829\n189   532.12195122\n190  -110.02439024\n191   -26.07317073\n192  -577.04878049\n193   144.17073171\n194   152.60975610\n195   587.53658537\n196   147.04878049\n197   530.80487805\n198   393.29268293\n199    46.12195122\n200   209.48780488\n201   479.48780488\n202  -448.36585366\n203    -3.97560976\n204  -450.90243902\n205   273.58536585\n206   -62.65853659\n207    75.58536585\n208   126.46341463\n209  -584.70731707\n210   565.82926829\n211    11.63414634\n212    -0.17073171\n213  -413.92682927\n214   692.07317073\n215   104.02439024\n216   251.24390244\n217    55.53658537\n218    53.00000000\n219   135.09756098\n220   761.04878049\n221  -555.09756098\n222   321.09756098\n223   673.87804878\n224    22.60975610\n225    49.00000000\n226    10.31707317\n227   -12.36585366\n228   124.46341463\n229    87.73170732\n230   -87.29268293\n231   599.48780488\n232  -636.51219512\n233  -721.87804878\n234  -121.73170732\n235  -395.73170732\n236  -157.73170732\n237  -646.80487805\n238   412.51219512\n239   573.78048780\n240  -114.80487805\n241  -551.53658537\n242   285.63414634\n243  -384.02439024\n244  -349.58536585\n245   -64.95121951\n246  -458.21951220\n247   -78.17073171\n248   337.14634146\n249  -604.36585366\n250   -11.09756098\n251  -255.04878049\n252  -111.78048780\n253   139.87804878\n254   346.46341463\n255   212.65853659\n256   332.51219512\n257   265.34146341\n258    47.63414634\n259   -39.34146341\n260   -26.02439024\n261  -608.65853659\n262   623.24390244\n263   -17.39024390\n264   161.34146341\n265   606.90243902\n266   104.17073171\n267   540.56097561\n268    69.87804878\n269   591.87804878\n270   538.60975610\n271  -748.07317073\n272   769.53658537\n273  -472.51219512\n274    35.04878049\n275   -85.00000000\n276   522.80487805\n277   264.56097561\n278  -127.04878049\n279  -204.90243902\n280  -315.34146341\n281   483.34146341\n282   342.65853659\n283  -379.92682927\n284  -442.90243902\n285   572.07317073\n286   477.19512195\n287   590.31707317\n288  -282.46341463\n289   397.04878049\n290  -765.73170732\n291  -517.92682927\n292  -165.39024390\n293  -362.70731707\n294   388.12195122\n295   -69.63414634\n296   483.92682927\n297   104.80487805\n298   118.46341463\n299   624.85365854\n300    50.12195122\n301   186.95121951\n302   -65.34146341\n303   125.58536585\n304   181.78048780\n305  -212.17073171\n306    42.56097561\n307   499.53658537\n308   597.24390244\n309  -650.95121951\n310  -167.87804878\n311   546.07317073\n312  -164.46341463\n313  -664.17073171\n314   487.09756098\n315  -149.92682927\n316  -127.68292683\n317   -49.29268293\n318   395.53658537\n319   695.97560976\n320   195.09756098\n321  -154.41463415\n322   239.24390244\n323  -798.80487805\n324  -713.73170732\n325   296.07317073\n326  -755.68292683\n327   192.17073171\n328  -356.26829268\n329   355.24390244\n330  -528.46341463\n331   667.14634146\n332  -305.78048780\n333  -811.04878049\n334  -404.31707317\n335   430.65853659\n336  -527.19512195\n337   409.68292683\n338   449.48780488\n339   431.53658537\n340  -360.12195122\n341   681.39024390\n342   479.73170732\n343    -5.63414634\n344   434.31707317\n345   699.68292683\n346  -717.73170732\n347   591.43902439\n348  -520.36585366\n349   521.58536585\n350  -197.34146341\n351   786.21951220\n352   -79.63414634\n353  -206.36585366\n354   112.41463415\n355   605.00000000\n356  -429.82926829\n357   192.90243902\n358   489.87804878\n359  -339.63414634\n360  -530.60975610\n361  -278.75609756\n362   150.36585366\n363  -487.00000000\n364  -755.48780488\n365    68.41463415\n366   375.39024390\n367  -179.24390244\n368   173.14634146\n369   534.90243902\n370    72.60975610\n371    85.78048780\n372   602.46341463\n373   453.82926829\n374   385.68292683\n375  -593.97560976\n376  -609.00000000\n377    -7.00000000\n378  -256.46341463\n379   333.04878049\n380  -394.36585366\n381    -3.82926829\n382  -282.85365854\n383  -208.51219512\n384   464.51219512\n385    41.73170732\n386   195.14634146\n387   112.07317073\n388  -712.56097561\n389  -196.26829268\n390   -90.12195122\n391  -617.14634146\n392   762.12195122\n393  -654.70731707\n394   404.56097561\n395   -85.92682927\n396  -165.04878049\n397   507.78048780\n398   -24.65853659\n399   -53.97560976\n400    -7.00000000\n401   263.78048780\n402  -100.36585366\n403   -57.58536585\n404  -518.46341463\n405  -797.92682927\n406  -589.29268293\n407    77.53658537\n408  -386.56097561\n409  -495.97560976\n410  -289.34146341\n411  -280.17073171\n412   656.26829268\n413  -643.97560976\n414   115.73170732\n415    50.60975610\n416  -543.78048780\n417   332.95121951\n418   398.80487805\n419  -409.97560976\n420  -634.65853659\n421   128.90243902\n422   503.87804878\n423    84.41463415\n424    12.36585366\n425   211.92682927\n426  -279.78048780\n427   389.19512195\n428   -78.46341463\n429   -83.39024390\n430   382.90243902\n431   -55.24390244\n432  -301.73170732\n433    72.02439024\n434  -539.73170732\n435  -193.14634146\n436    54.12195122\n437  -651.53658537\n438   -93.48780488\n439   -94.21951220\n440    56.75609756\n441   405.73170732\n442  -741.43902439\n443  -613.78048780\n444   399.19512195\n445   -30.46341463\n446   -52.46341463\n447   729.78048780\n448   327.87804878\n449  -545.43902439\n450   641.92682927\n451    83.34146341\n452   160.51219512\n453   -52.56097561\n454   -48.75609756\n455  -716.56097561\n456   217.48780488\n457   243.58536585\n458   576.75609756\n459   287.48780488\n460  -510.65853659\n461   -78.75609756\n462   204.90243902\n463   651.87804878\n464   724.80487805\n465   -55.39024390\n466   660.31707317\n467   651.78048780\n468   173.34146341\n469  -548.75609756\n470   -46.60975610\n471   376.51219512\n472  -483.43902439\n473   150.80487805\n474   540.85365854\n475   438.60975610\n476   647.43902439\n477   109.00000000\n478  -569.29268293\n479    70.80487805\n480  -187.24390244\n481  -104.51219512\n482   589.87804878\n483   548.36585366\n484    77.19512195\n485   221.73170732\n486    75.78048780\n487    49.19512195\n488    -5.78048780\n489  -306.02439024\n490    53.04878049\n491  -337.39024390\n492   397.82926829\n493  -602.51219512\n494  -503.34146341\n495  -439.39024390\n496    31.24390244\n497  -507.24390244\n498   602.21951220\n499    87.73170732\n500  -479.68292683\n501   340.21951220\n502  -568.02439024\n503  -588.26829268\n504   605.09756098\n505  -871.43902439\n506  -654.26829268\n507   243.34146341\n508  -827.78048780\n509  -117.04878049\n510    55.87804878\n511   -60.56097561\n512   626.85365854\n513   515.24390244\n514   616.41463415\n515   516.02439024\n516   -24.41463415\n517    62.85365854\n518    24.80487805\n519   108.85365854\n520    57.82926829\n521  -551.97560976\n522  -614.85365854\n523   158.02439024\n524   490.02439024\n525   606.36585366\n526  -749.00000000\n527   -90.31707317\n528  -512.90243902\n529   178.60975610\n530    90.21951220\n531   -31.48780488\n532   158.65853659\n533  -342.21951220\n534   430.80487805\n535   288.21951220\n536   685.24390244\n537  -642.07317073\n538  -256.70731707\n539   166.80487805\n540   -95.04878049\n541   -92.07317073\n542   598.31707317\n543   666.31707317\n544   370.70731707\n545   625.09756098\n546   350.07317073\n547   -66.56097561\n548  -781.82926829\n549    29.19512195\n550   708.12195122\n551   -49.34146341\n552  -120.02439024\n553   449.09756098\n554  -571.00000000\n555  -549.43902439\n556  -597.34146341\n557   482.36585366\n558  -523.04878049\n559  -605.29268293\n560   177.09756098\n561    38.90243902\n562  -585.34146341\n563   574.21951220\n564  -644.85365854\n565   406.26829268\n566   -75.09756098\n567  -615.87804878\n568   247.29268293\n569  -279.24390244\n570   416.75609756\n571  -648.07317073\n572    58.41463415\n573   608.41463415\n574   279.29268293\n575   536.95121951\n576     6.56097561\n577   321.63414634\n578   281.63414634\n579   365.63414634\n580    71.53658537\n581  -569.73170732\n582   404.70731707\n583    43.24390244\n584    66.12195122\n585  -770.80487805\n586   -47.87804878\n587   660.17073171\n588   618.07317073\n589   260.07317073\n590   -39.82926829\n591  -253.34146341\n592   411.00000000\n593  -554.65853659\n594  -542.36585366\n595   303.92682927\n596   560.12195122\n597   304.51219512\n598   -76.51219512\n599  -474.75609756\n600   651.68292683\n601   451.39024390\n602   718.75609756\n603    23.92682927\n604   137.24390244\n605  -167.73170732\n606  -772.65853659\n607   -32.26829268\n608   107.00000000\n609   340.17073171\n610  -798.70731707\n611   136.31707317\n612  -183.43902439\n613  -249.43902439\n614   170.26829268\n615  -143.78048780\n616   361.97560976\n617   -35.82926829\n618   -80.36585366\n619   249.14634146\n620  -329.34146341\n621   166.26829268\n622   290.21951220\n623  -148.21951220\n624  -339.58536585\n625   551.68292683\n626   -99.09756098\n627  -158.85365854\n628  -549.68292683\n629  -232.07317073\n630  -353.73170732\n631   465.24390244\n632   132.02439024\n633  -753.00000000\n634  -561.68292683\n635   372.26829268\n636   480.41463415\n637  -476.51219512\n638   265.73170732\n639  -140.70731707\n640   204.36585366\n641   118.85365854\n642   -39.48780488\n643   667.87804878\n644  -351.04878049\n645   679.24390244\n646    -7.00000000\n647   560.21951220\n648  -497.53658537\n649   495.68292683\n650   -98.95121951\n651  -661.34146341\n652   111.29268293\n653   118.12195122\n654   447.78048780\n655   695.34146341\n656  -173.87804878\n657   647.34146341\n658    62.31707317\n659  -111.19512195\n660  -472.51219512\n661  -688.17073171\n662  -640.65853659\n663   369.82926829\n664  -711.68292683\n665   112.17073171\n666   544.85365854\n667  -238.21951220\n668    -5.82926829\n669   115.04878049\n670   149.24390244\n671   473.78048780\n672   483.39024390\n673  -816.07317073\n674    52.12195122\n675   -92.60975610\n676  -522.60975610\n677  -598.51219512\n678  -548.70731707\n679   119.24390244\n680   534.12195122\n681     0.26829268\n682  -138.02439024\n683   590.75609756\n684   149.97560976\n685   -41.78048780\n686   -89.43902439\n687  -650.02439024\n688   768.60975610\n689   692.60975610\n690    14.80487805\n691   634.31707317\n692   516.41463415\n693   137.14634146\n694  -604.46341463\n695   176.60975610\n696   378.31707317\n697   291.39024390\n698   -85.24390244\n699    29.48780488\n700   -21.63414634\n701   190.51219512\n702  -491.68292683\n703   -53.68292683\n704  -193.29268293\n705  -530.70731707\n706   576.21951220\n707   534.07317073\n708  -667.63414634\n709  -291.63414634\n710   136.95121951\n711   464.17073171\n712    67.53658537\n713  -708.65853659\n714   -13.48780488\n715   514.26829268\n716  -526.41463415\n717   -31.00000000\n718    55.63414634\n719   311.00000000\n720  -527.00000000\n721  -553.63414634\n722  -382.65853659\n723  -346.75609756\n724   -17.48780488\n725   293.48780488\n726   207.24390244\n727   573.87804878\n728    19.19512195\n729    52.65853659\n730   662.90243902\n731   572.90243902\n732   -76.36585366\n733   285.09756098\n734  -187.48780488\n735    87.19512195\n736   482.46341463\n737    20.41463415\n738  -603.09756098\n739  -642.80487805\n740  -494.90243902\n741  -348.75609756\n742   -48.70731707\n743   651.39024390\n744   142.85365854\n745  -581.29268293\n746    66.41463415\n747  -139.63414634\n748    88.21951220\n749  -707.14634146\n750   -15.68292683\n751   568.60975610\n752   461.14634146\n753   515.14634146\n754  -604.75609756\n755  -697.78048780\n756  -407.63414634\n757   312.70731707\n758   -67.58536585\n759   -10.85365854\n760    53.09756098\n761  -422.70731707\n762   708.51219512\n763  -593.92682927\n764  -615.78048780\n765   -49.09756098\n766   516.02439024\n767   185.78048780\n768    53.92682927\n769  -559.58536585\n770  -201.92682927\n771  -240.60975610\n772  -504.65853659\n773   290.26829268\n774  -498.51219512\n775  -445.24390244\n776   -94.12195122\n777  -198.41463415\n778   345.19512195\n779  -799.39024390\n780  -260.60975610\n781  -121.92682927\n782   -76.41463415\n783  -754.51219512\n784   114.70731707\n785   -76.02439024\n786   -25.04878049\n787   390.36585366\n788   -56.70731707\n789  -128.70731707\n790    20.21951220\n791   795.78048780\n792  -313.43902439\n793  -160.46341463\n794    72.46341463\n795   145.39024390\n796  -409.00000000\n797   -82.70731707\n798   185.92682927\n799   571.58536585\n800  -484.51219512\n801  -183.39024390\n802  -415.09756098\n803    81.48780488\n804  -251.29268293\n805   -81.58536585\n806   160.26829268\n807  -442.65853659\n808  -352.85365854\n809  -511.48780488\n810   501.19512195\n811  -829.68292683\n812   377.82926829\n813  -565.00000000\n814   402.85365854\n815   580.31707317\n816    -1.78048780\n817   422.56097561\n818   705.14634146\n819   -41.97560976\n820   359.68292683\n821  -154.65853659\n822   117.92682927\n823    26.07317073\n824    16.31707317\n825   439.34146341\n826   215.43902439\n827  -115.53658537\n828   258.31707317\n829  -638.31707317\n830  -269.82926829\n831  -678.80487805\n832   -61.00000000\n833   -48.07317073\n834  -315.34146341\n835   409.92682927\n836   396.51219512\n837   185.34146341\n838   -86.41463415\n839  -743.92682927\n840   -72.46341463\n841   343.24390244\n842   -77.09756098\n843  -542.75609756\n844   538.02439024\n845  -204.07317073\n846    34.12195122\n847   -41.19512195\n848  -297.92682927\n849   441.82926829\n850   526.26829268\n851    62.90243902\n852   181.82926829\n853  -307.58536585\n854  -364.80487805\n855  -565.87804878\n856  -668.36585366\n857  -699.53658537\n858   148.26829268\n859   101.09756098\n860   530.51219512\n861    60.21951220\n862   595.92682927\n863  -439.29268293\n864   163.04878049\n865    86.70731707\n866   894.75609756\n867  -106.12195122\n868   335.73170732\n869  -160.12195122\n870  -318.60975610\n871   527.24390244\n872   192.95121951\n873    73.00000000\n874   256.70731707\n875    -3.29268293\n876   692.31707317\n877  -219.63414634\n878   -54.41463415\n879  -556.41463415\n880   310.90243902\n881  -534.65853659\n882  -790.41463415\n883   151.48780488\n884   241.82926829\n885   214.51219512\n886  -441.63414634\n887  -361.09756098\n888  -726.12195122\n889   617.04878049\n890   -28.60975610\n891  -264.65853659\n892    -5.14634146\n893  -542.85365854\n894  -238.95121951\n895  -573.04878049\n896  -671.19512195\n897  -136.85365854\n898   522.07317073\n899  -297.29268293\n900   -36.51219512\n901   -89.68292683\n902   577.14634146\n903   196.56097561\n904   355.78048780\n905  -314.26829268\n906   218.60975610\n907  -669.04878049\n908   564.60975610\n909  -379.68292683\n910   200.21951220\n911  -134.56097561\n912   120.75609756\n913   519.00000000\n914   169.53658537\n915   -20.51219512\n916  -439.68292683\n917   231.53658537\n918  -221.87804878\n919  -118.07317073\n920  -222.02439024\n921   -69.82926829\n922   690.17073171\n923  -236.26829268\n924  -171.09756098\n925   512.80487805\n926  -189.34146341\n927  -120.36585366\n928   330.51219512\n929  -189.09756098\n930   -80.51219512\n931   534.02439024\n932  -643.29268293\n933    13.82926829\n934   202.60975610\n935    75.63414634\n936  -419.39024390\n937   -23.24390244\n938   -21.24390244\n939  -417.24390244\n940    81.73170732\n941   -79.63414634\n942  -379.43902439\n943  -517.00000000\n944   372.51219512\n945   530.02439024\n946    87.29268293\n947  -549.87804878\n948   155.97560976\n949  -194.56097561\n950   532.17073171\n951   551.73170732\n952    43.58536585\n953   101.24390244\n954  -476.56097561\n955  -651.24390244\n956   601.63414634\n957  -181.00000000\n958  -738.07317073\n959   222.17073171\n960  -639.19512195\n961    56.70731707\n962  -680.95121951\n963   214.07317073\n964   538.46341463\n965   199.19512195\n966   486.07317073\n967  -213.48780488\n968    44.26829268\n969   345.63414634\n970   -43.29268293\n971  -105.73170732\n972   196.31707317\n973  -456.36585366\n974  -613.39024390\n975   376.95121951\n976  -190.65853659\n977   469.00000000\n978   133.09756098\n979    61.34146341\n980   328.46341463\n981   443.24390244\n982  -267.19512195\n983  -103.34146341\n984  -612.80487805\n985    38.36585366\n986  -494.36585366\n987   596.95121951\n988  -616.80487805\n989   302.17073171\n990   439.53658537\n991   -71.53658537\n992   162.70731707\n993   246.65853659\n994  -179.73170732\n995   744.02439024\n996  -560.36585366\n997   118.90243902\n998   -32.26829268\n999    73.24390244\n1000   -3.58536585\n1001 -106.41463415\n1002  200.12195122\n1003  215.58536585\n1004  -94.90243902\n1005  137.48780488\n1006  758.51219512\n1007 -354.70731707\n1008  260.02439024\n1009  169.04878049\n1010    7.97560976\n1011  602.90243902\n1012  -42.17073171\n1013  661.92682927\n1014   -0.85365854\n1015 -163.82926829\n1016  817.04878049\n1017  -24.95121951\n1018  235.39024390\n1019   61.04878049\n1020  -24.07317073\n1021 -104.41463415\n1022 -159.29268293\n1023  157.24390244\n1024  193.00000000\n1025  511.68292683\n1026 -455.78048780\n1027  340.21951220\n1028  657.48780488\n1029  684.26829268\n1030 -178.46341463\n1031  103.04878049\n1032  -98.02439024\n1033  585.29268293\n1034  343.53658537\n1035  317.68292683\n1036   76.75609756\n1037  159.24390244\n1038 -340.95121951\n1039  410.21951220\n1040  327.48780488\n1041 -112.70731707\n1042 -486.26829268\n1043  -26.95121951\n1044 -124.75609756\n1045 -446.60975610\n1046  -55.48780488\n1047 -474.90243902\n1048 -557.09756098\n1049   19.09756098\n1050 -636.46341463\n1051 -588.95121951\n1052  -84.51219512\n1053  -16.41463415\n1054   26.65853659\n1055 -437.14634146\n1056  -32.21951220\n1057 -425.78048780\n1058   30.56097561\n1059 -133.43902439\n1060 -176.31707317\n1061 -119.53658537\n1062 -583.63414634\n1063 -297.43902439\n1064 -439.43902439\n1065  -85.19512195\n1066 -152.46341463\n1067  -10.80487805\n1068  887.04878049\n1069 -383.29268293\n1070   49.87804878\n1071 -648.85365854\n1072 -607.53658537\n1073 -275.68292683\n1074  611.53658537\n1075 -122.80487805\n1076   45.19512195\n1077  110.36585366\n1078 -402.12195122\n1079  145.04878049\n1080  -15.58536585\n1081  374.90243902\n1082  611.14634146\n1083  563.24390244\n1084  614.60975610\n1085  151.97560976\n1086  438.02439024\n1087  560.51219512\n1088   71.53658537\n1089 -132.65853659\n1090 -573.87804878\n1091   58.51219512\n1092 -702.95121951\n1093 -688.07317073\n1094  653.87804878\n1095  733.78048780\n1096  426.95121951\n1097 -118.26829268\n1098  -53.39024390\n1099  721.19512195\n1100 -423.04878049\n1101 -394.51219512\n1102   50.65853659\n1103 -229.68292683\n1104 -173.34146341\n1105  704.02439024\n1106  -90.21951220\n1107 -120.36585366\n1108  -17.09756098\n1109    9.09756098\n1110   67.63414634\n1111 -287.82926829\n1112 -254.51219512\n1113 -495.97560976\n1114  652.41463415\n1115  274.75609756\n1116 -477.97560976\n1117  166.41463415\n1118  716.26829268\n1119 -551.92682927\n1120  606.12195122\n1121  311.97560976\n1122 -478.51219512\n1123  -26.56097561\n1124  -83.14634146\n1125 -190.46341463\n1126  248.07317073\n1127 -361.87804878\n1128 -254.70731707\n1129  187.53658537\n1130  158.65853659\n1131    3.73170732\n1132 -168.75609756\n1133 -441.24390244\n1134  102.17073171\n1135  537.63414634\n1136  -79.73170732\n1137   56.21951220\n1138  518.46341463\n1139  122.36585366\n1140 -210.56097561\n1141   66.95121951\n1142  111.29268293\n1143 -187.48780488\n1144  -49.14634146\n1145  386.56097561\n1146  307.53658537\n1147  151.19512195\n1148 -111.78048780\n1149 -207.04878049\n1150 -591.39024390\n1151 -544.95121951\n1152  121.39024390\n1153 -672.17073171\n1154  523.19512195\n1155  -28.80487805\n1156  367.43902439\n1157  578.36585366\n1158  310.90243902\n1159   26.65853659\n1160 -521.78048780\n1161  537.82926829\n1162   83.58536585\n1163 -541.78048780\n1164  662.21951220\n1165  626.90243902\n1166  124.46341463\n1167 -585.43902439\n1168  337.92682927\n1169 -112.02439024\n1170 -103.29268293\n1171  171.00000000\n1172  653.09756098\n1173 -178.75609756\n1174  138.60975610\n1175 -150.56097561\n1176 -581.97560976\n1177   65.53658537\n1178  -81.14634146\n1179  666.21951220\n1180  277.19512195\n1181 -630.95121951\n1182  837.48780488\n1183 -434.07317073\n1184 -699.04878049\n1185 -117.14634146\n1186 -487.82926829\n1187  395.34146341\n1188  -43.19512195\n1189 -237.24390244\n1190 -656.51219512\n1191  196.80487805\n1192   29.68292683\n1193   99.73170732\n1194    4.90243902\n1195 -154.07317073\n1196 -217.97560976\n1197  457.24390244\n1198  -27.43902439\n1199  -22.90243902\n1200  -77.92682927\n1201  609.09756098\n1202 -224.90243902\n1203  447.87804878\n1204 -675.73170732\n1205  -61.09756098\n1206  456.85365854\n1207  273.73170732\n1208  -22.60975610\n1209  102.26829268\n1210  535.82926829\n1211  614.75609756\n1212 -112.46341463\n1213  -86.90243902\n1214  410.17073171\n1215 -437.58536585\n1216 -369.00000000\n1217   48.85365854\n1218 -634.17073171\n1219 -601.48780488\n1220  322.75609756\n1221  -35.19512195\n1222  256.70731707\n1223 -184.85365854\n1224  651.09756098\n1225  573.29268293\n1226  526.31707317\n1227  450.36585366\n1228  128.65853659\n1229  144.12195122\n1230 -574.60975610\n1231  675.73170732\n1232  517.39024390\n1233  551.63414634\n1234 -440.95121951\n1235  359.19512195\n1236 -837.73170732\n1237  618.07317073\n1238 -504.12195122\n1239  377.39024390\n1240  169.78048780\n1241 -543.68292683\n1242  -83.53658537\n1243 -120.41463415\n1244  151.48780488\n1245  -83.53658537\n1246   67.58536585\n1247 -379.87804878\n1248  497.09756098\n1249 -212.31707317\n1250  131.00000000\n1251   53.87804878\n1252   86.56097561\n1253 -643.04878049\n1254 -385.29268293\n1255  -60.70731707\n1256 -478.80487805\n1257  172.70731707\n1258   98.02439024\n1259 -287.00000000\n1260 -262.95121951\n1261 -585.14634146\n1262  399.34146341\n1263 -474.51219512\n1264 -192.75609756\n1265  672.75609756\n1266 -514.26829268\n1267 -466.21951220\n1268  -34.36585366\n1269  180.70731707\n1270 -449.43902439\n1271 -144.46341463\n1272 -300.70731707\n1273  465.48780488\n1274 -539.43902439\n1275  204.02439024\n1276  203.82926829\n1277  539.97560976\n1278  -49.97560976\n1279  502.36585366\n1280  -34.90243902\n1281 -601.24390244\n1282 -440.60975610\n1283 -203.63414634\n1284 -698.95121951\n1285 -691.39024390\n1286 -571.04878049\n1287  648.21951220\n1288 -712.70731707\n1289  211.92682927\n1290 -198.02439024\n1291 -225.82926829\n1292 -359.00000000\n1293  500.75609756\n1294 -629.63414634\n1295  730.31707317\n1296 -384.60975610\n1297  161.29268293\n1298   80.51219512\n1299 -537.09756098\n1300 -250.70731707\n1301   53.97560976\n1302 -406.02439024\n1303 -610.07317073\n1304 -571.87804878\n1305 -107.87804878\n1306  439.39024390\n1307  -53.34146341\n1308 -496.60975610\n1309   81.97560976\n1310  339.97560976\n1311  450.51219512\n1312  167.09756098\n1313   28.60975610\n1314 -347.92682927\n1315  686.90243902\n1316  118.12195122\n1317 -496.21951220\n1318    1.82926829\n1319 -518.12195122\n1320 -192.21951220\n1321  534.95121951\n1322 -154.56097561\n1323  161.14634146\n1324 -697.87804878\n1325  419.24390244\n1326  452.75609756\n1327 -625.04878049\n1328  545.53658537\n1329 -235.29268293\n1330   -1.92682927\n1331  200.60975610\n1332  587.68292683\n1333 -115.53658537\n1334  -83.58536585\n1335 -119.63414634\n1336  113.29268293\n1337 -169.04878049\n1338 -571.39024390\n1339 -482.80487805\n1340 -680.36585366\n1341 -355.58536585\n1342 -707.78048780\n1343  811.68292683\n1344  -90.60975610\n1345  149.68292683\n1346  704.36585366\n1347  304.80487805\n1348   26.80487805\n1349  325.68292683\n1350  178.65853659\n1351 -170.95121951\n1352  698.02439024\n1353    9.43902439\n1354  623.04878049\n1355  601.00000000\n1356  605.92682927\n1357  291.09756098\n1358   -0.02439024\n1359  -38.80487805\n1360   18.80487805\n1361 -582.60975610\n1362   44.85365854\n1363  585.09756098\n1364  111.63414634\n1365  555.53658537\n1366   17.78048780\n1367  -57.09756098\n1368  -94.07317073\n1369 -143.63414634\n1370  540.46341463\n1371 -646.65853659\n1372 -288.70731707\n1373 -291.58536585\n1374  594.21951220\n1375  563.34146341\n1376  374.21951220\n1377  -88.36585366\n1378 -473.68292683\n1379 -640.17073171\n1380 -847.68292683\n1381  465.58536585\n1382   44.12195122\n1383 -152.65853659\n1384 -191.39024390\n1385   -1.43902439\n1386  -78.46341463\n1387  -90.95121951\n1388 -560.46341463\n1389  366.46341463\n1390 -157.63414634\n1391 -568.41463415\n1392   12.90243902\n1393  175.48780488\n1394 -183.73170732\n1395  409.00000000\n1396  404.65853659\n1397   34.41463415\n1398   51.68292683\n1399  627.24390244\n1400 -187.34146341\n1401  156.21951220\n1402 -326.51219512\n1403 -558.95121951\n1404   28.80487805\n1405 -288.36585366\n1406 -180.31707317\n1407  132.75609756\n1408 -511.53658537\n1409    8.56097561\n1410  420.85365854\n1411  291.00000000\n1412  167.58536585\n1413  526.95121951\n1414  648.46341463\n1415 -585.48780488\n1416 -201.39024390\n1417  371.73170732\n1418   48.17073171\n1419  189.04878049\n1420 -454.12195122\n1421   53.39024390\n1422 -238.41463415\n1423 -466.21951220\n1424   99.82926829\n1425   23.48780488\n1426  396.56097561\n1427  182.41463415\n1428  600.85365854\n1429  132.85365854\n1430 -652.21951220\n1431 -541.68292683\n1432  -21.34146341\n1433  121.39024390\n1434 -601.87804878\n1435  872.70731707\n1436  153.87804878\n1437 -141.04878049\n1438 -641.82926829\n1439  708.26829268\n1440  245.58536585\n1441 -169.78048780\n1442 -163.34146341\n1443  639.63414634\n1444  376.12195122\n1445  137.43902439\n1446   90.07317073\n1447  541.00000000\n1448  194.31707317\n1449  184.07317073\n1450  590.07317073\n1451 -148.36585366\n1452  360.90243902\n1453    8.41463415\n1454  630.07317073\n1455 -114.56097561\n1456 -108.65853659\n1457 -140.95121951\n1458    1.14634146\n1459  192.60975610\n1460 -529.58536585\n1461 -528.65853659\n1462  -80.90243902\n1463  171.78048780\n1464 -637.34146341\n1465  188.41463415\n1466  694.21951220\n1467  -51.00000000\n1468  718.31707317\n1469  108.31707317\n1470  -15.29268293\n1471 -162.65853659\n1472  522.41463415\n1473 -462.21951220\n1474  193.39024390\n1475  172.17073171\n1476  363.19512195\n1477 -689.19512195\n1478 -462.65853659\n1479  -10.02439024\n1480  619.04878049\n1481 -229.39024390\n1482  -31.09756098\n1483  195.82926829\n1484  255.34146341\n1485  228.75609756\n1486  628.02439024\n1487  166.90243902\n1488 -317.63414634\n1489  498.21951220\n1490 -708.31707317\n1491 -562.02439024\n1492 -138.65853659\n1493   86.80487805\n1494   85.04878049\n1495  117.43902439\n1496 -430.95121951\n1497  -82.95121951\n1498 -251.48780488\n1499  429.14634146\n1500  330.31707317\n1501  571.39024390\n1502 -313.19512195\n1503  680.65853659\n1504    0.60975610\n1505  289.29268293\n1506 -174.95121951\n1507  530.80487805\n1508  604.02439024\n1509  -63.78048780\n1510   -0.80487805\n1511 -733.48780488\n1512  189.48780488\n1513 -639.82926829\n1514 -428.56097561\n1515 -739.53658537\n1516  628.80487805\n1517 -600.17073171\n1518 -421.53658537\n1519 -306.12195122\n1520  504.21951220\n1521  -12.56097561\n1522 -321.34146341\n1523 -366.51219512\n1524  585.34146341\n1525  533.58536585\n1526 -167.87804878\n1527 -154.51219512\n1528  534.56097561\n1529  150.60975610\n1530  -79.24390244\n1531  -97.53658537\n1532  530.56097561\n1533   85.34146341\n1534 -581.43902439\n1535  332.56097561\n1536  -17.92682927\n1537  326.80487805\n1538  354.60975610\n1539 -605.48780488\n1540 -574.12195122\n1541 -587.39024390\n1542 -339.53658537\n1543  756.41463415\n1544   62.56097561\n1545 -466.70731707\n1546 -536.70731707\n1547  146.07317073\n1548  -27.63414634\n1549 -696.02439024\n1550  107.04878049\n1551 -420.31707317\n1552   70.17073171\n1553 -379.24390244\n1554 -574.46341463\n1555 -484.70731707\n1556   35.29268293\n1557  260.85365854\n1558   31.29268293\n1559   42.31707317\n1560  620.70731707\n1561  206.65853659\n1562   70.41463415\n1563  638.85365854\n1564  694.26829268\n1565 -145.58536585\n1566 -306.02439024\n1567  -24.12195122\n1568 -389.78048780\n1569   37.68292683\n1570   17.87804878\n1571 -588.12195122\n1572  265.39024390\n1573  196.17073171\n1574  191.63414634\n1575 -161.00000000\n1576 -643.53658537\n1577 -109.68292683\n1578 -540.95121951\n1579   54.51219512\n1580   -8.31707317\n1581   37.97560976\n1582 -234.90243902\n1583 -118.12195122\n1584 -153.92682927\n1585 -191.58536585\n1586 -596.65853659\n1587 -362.75609756\n1588  -27.78048780\n1589  148.07317073\n1590  137.00000000\n1591 -576.70731707\n1592  649.00000000\n1593 -285.73170732\n1594  -78.36585366\n1595    0.95121951\n1596 -813.68292683\n1597   84.02439024\n1598  -60.90243902\n1599  752.80487805\n1600  -62.31707317\n1601  214.70731707\n1602 -706.07317073\n1603   40.41463415\n1604  327.87804878\n1605   52.90243902\n1606  -36.80487805\n1607 -409.34146341\n1608  218.65853659\n1609  108.31707317\n1610  482.46341463\n1611 -311.48780488\n1612   35.78048780\n1613 -416.65853659\n1614 -563.87804878\n1615  -95.43902439\n1616  540.65853659\n1617   76.41463415\n1618  197.43902439\n1619 -200.17073171\n1620 -412.90243902\n1621 -612.02439024\n1622   -4.21951220\n1623  225.34146341\n1624  -72.41463415\n1625 -743.04878049\n1626   32.07317073\n1627  563.43902439\n1628  521.04878049\n1629  440.95121951\n1630  753.58536585\n1631 -555.92682927\n1632  273.34146341\n1633   -4.46341463\n1634 -483.29268293\n1635  734.41463415\n1636  459.82926829\n1637 -624.80487805\n1638  -96.80487805\n1639  586.56097561\n1640  118.36585366\n1641  405.53658537\n1642  -34.26829268\n1643   59.43902439\n1644 -609.39024390\n1645 -379.43902439\n1646  420.17073171\n1647  465.09756098\n1648  -39.39024390\n1649  101.39024390\n1650  402.51219512\n1651  452.17073171\n1652  743.39024390\n1653 -232.75609756\n1654 -149.29268293\n1655  141.29268293\n1656 -114.60975610\n1657   53.04878049\n1658  -10.51219512\n1659 -483.00000000\n1660  -91.04878049\n1661  763.14634146\n1662 -262.80487805\n1663  464.31707317\n1664 -715.04878049\n1665 -638.65853659\n1666  -84.56097561\n1667  436.70731707\n1668  512.60975610\n1669  -63.82926829\n1670  -80.60975610\n1671  361.39024390\n1672 -535.00000000\n1673 -428.41463415\n1674  572.85365854\n1675   79.14634146\n1676  249.29268293\n1677  412.46341463\n1678  773.34146341\n1679 -107.43902439\n1680 -232.65853659\n1681 -232.65853659\n1682  234.17073171\n1683  718.31707317\n1684 -681.73170732\n1685 -720.17073171\n1686  -72.07317073\n1687 -285.63414634\n1688 -808.17073171\n1689 -867.24390244\n1690  436.51219512\n1691 -630.31707317\n1692   21.43902439\n1693  733.73170732\n1694 -482.31707317\n1695 -378.95121951\n1696   34.60975610\n1697 -236.21951220\n1698  759.58536585\n1699 -246.51219512\n1700  -58.41463415\n1701   44.12195122\n1702  328.02439024\n1703 -264.46341463\n1704 -504.21951220\n1705 -748.90243902\n1706  184.65853659\n1707 -273.48780488\n1708  -86.21951220\n1709  101.97560976\n1710  432.85365854\n1711 -294.41463415\n1712 -698.02439024\n1713   35.09756098\n1714 -112.46341463\n1715  -45.68292683\n1716  679.63414634\n1717   73.04878049\n1718  136.31707317\n1719  239.14634146\n1720  418.26829268\n1721  -92.31707317\n1722  749.34146341\n1723 -642.56097561\n1724 -162.12195122\n1725 -121.14634146\n1726  259.97560976\n1727  146.12195122\n1728 -101.82926829\n1729   91.68292683\n1730 -544.26829268\n1731    9.97560976\n1732  587.00000000\n1733    1.14634146\n1734 -596.41463415\n1735 -456.51219512\n1736 -200.21951220\n1737    9.63414634\n1738  381.14634146\n1739 -496.31707317\n1740  514.95121951\n1741 -668.07317073\n1742 -328.41463415\n1743  503.63414634\n1744 -438.41463415\n1745  529.53658537\n1746 -784.17073171\n1747 -541.68292683\n1748 -183.29268293\n1749  351.97560976\n1750  309.34146341\n1751   79.97560976\n1752  660.46341463\n1753 -227.43902439\n1754  561.09756098\n1755  656.17073171\n1756 -384.41463415\n1757 -691.00000000\n1758  590.60975610\n1759  -78.70731707\n1760   76.07317073\n1761   61.34146341\n1762  -72.51219512\n1763 -464.21951220\n1764 -619.00000000\n1765  427.92682927\n1766  104.56097561\n1767  578.95121951\n1768  729.29268293\n1769  -64.17073171\n1770 -304.70731707\n1771  817.24390244\n1772  -60.75609756\n1773 -246.26829268\n1774  678.36585366\n1775 -173.53658537\n1776 -149.58536585\n1777  -53.04878049\n1778 -543.09756098\n1779  447.73170732\n1780   55.14634146\n1781  434.95121951\n1782 -731.00000000\n1783  478.46341463\n1784 -589.34146341\n1785  178.95121951\n1786 -135.82926829\n1787 -132.75609756\n1788   83.43902439\n1789 -169.92682927\n1790 -134.51219512\n1791 -116.51219512\n1792 -121.73170732\n1793 -346.75609756\n1794  -23.48780488\n1795 -179.04878049\n1796   90.31707317\n1797   90.80487805\n1798 -288.26829268\n1799 -271.09756098\n1800    2.46341463\n1801 -163.53658537\n1802  -24.95121951\n1803 -480.60975610\n1804 -764.12195122\n1805 -567.78048780\n1806  714.80487805\n1807  -75.04878049\n1808 -606.70731707\n1809   60.17073171\n1810  633.97560976\n1811  -35.39024390\n1812  -84.12195122\n1813 -605.73170732\n1814  220.21951220\n1815  439.87804878\n1816  166.17073171\n1817  -13.58536585\n1818  110.12195122\n1819  122.17073171\n1820  -42.17073171\n1821  197.82926829\n1822  101.04878049\n1823 -402.75609756\n1824 -412.26829268\n1825  405.53658537\n1826  623.68292683\n1827 -214.56097561\n1828 -612.36585366\n1829 -116.21951220\n1830  125.73170732\n1831  358.46341463\n1832 -518.17073171\n1833  134.36585366\n1834  224.85365854\n1835  589.92682927\n1836  -12.65853659\n1837 -551.34146341\n1838 -193.53658537\n1839  639.34146341\n1840 -400.17073171\n1841 -615.48780488\n1842 -113.78048780\n1843   17.43902439\n1844  168.56097561\n1845 -607.43902439\n1846  514.17073171\n1847 -524.12195122\n1848  -56.56097561\n1849  185.39024390\n1850 -415.19512195\n1851 -497.43902439\n1852 -153.19512195\n1853  201.87804878\n1854  409.53658537\n1855  712.95121951\n1856 -138.31707317\n1857  456.31707317\n1858  578.70731707\n1859 -638.31707317\n1860 -480.65853659\n1861  107.39024390\n1862 -570.02439024\n1863   90.46341463\n1864  142.26829268\n1865  115.73170732\n1866   71.58536585\n1867 -301.48780488\n1868  -77.82926829\n1869   37.34146341\n1870  487.24390244\n1871  215.53658537\n1872  410.07317073\n1873 -450.65853659\n1874 -148.85365854\n1875  482.17073171\n1876  307.09756098\n1877  625.09756098\n1878 -541.97560976\n1879   27.43902439\n1880 -589.63414634\n1881  314.65853659\n1882  571.82926829\n1883    4.60975610\n1884 -427.24390244\n1885 -159.39024390\n1886  -72.02439024\n1887  276.51219512\n1888 -667.82926829\n1889 -497.39024390\n1890  655.39024390\n1891  771.24390244\n1892   28.75609756\n1893  673.43902439\n1894   -6.17073171\n1895  -31.87804878\n1896  118.70731707\n1897 -569.29268293\n1898 -373.48780488\n1899 -622.70731707\n1900   70.36585366\n1901  111.29268293\n1902 -319.68292683\n1903 -724.60975610\n1904  117.73170732\n1905 -410.65853659\n1906   46.65853659\n1907   39.78048780\n1908   18.51219512\n1909  561.04878049\n1910 -645.29268293\n1911  547.73170732\n1912 -710.21951220\n1913 -533.34146341\n1914 -674.56097561\n1915 -354.56097561\n1916 -568.90243902\n1917 -599.82926829\n1918  143.00000000\n1919  -17.14634146\n1920  328.07317073\n1921 -107.87804878\n1922 -190.51219512\n1923 -144.46341463\n1924   28.70731707\n1925  438.70731707\n1926 -780.90243902\n1927   28.26829268\n1928  327.43902439\n1929 -674.56097561\n1930 -272.75609756\n1931 -480.60975610\n1932  124.65853659\n1933 -710.12195122\n1934  671.39024390\n1935  216.12195122\n1936 -103.14634146\n1937   89.04878049\n1938  -30.65853659\n1939 -961.87804878\n1940   34.46341463\n1941   22.36585366\n1942 -546.60975610\n1943  515.29268293\n1944  535.87804878\n1945 -542.70731707\n1946  -13.39024390\n1947  -86.21951220\n1948  573.82926829\n1949  201.29268293\n1950 -263.82926829\n1951  457.09756098\n1952  -22.41463415\n1953  -38.12195122\n1954  627.39024390\n1955  120.51219512\n1956  683.53658537\n1957  333.48780488\n1958 -582.75609756\n1959 -691.97560976\n1960  543.39024390\n1961 -180.17073171\n1962 -592.85365854\n1963   52.90243902\n1964 -119.14634146\n1965    1.00000000\n1966 -137.87804878\n1967  588.80487805\n1968 -696.51219512\n1969  546.80487805\n1970  456.26829268\n1971 -618.41463415\n1972  198.65853659\n1973   15.78048780\n1974  136.41463415\n1975 -588.41463415\n1976 -680.70731707\n1977  225.00000000\n1978  565.19512195\n1979 -197.00000000\n1980 -636.95121951\n1981 -138.46341463\n1982 -490.41463415\n1983 -223.39024390\n1984 -148.85365854\n1985  -16.90243902\n1986  566.17073171\n1987   32.21951220\n1988  625.78048780\n1989 -106.12195122\n1990 -758.51219512\n1991  426.02439024\n1992 -642.51219512\n1993  506.60975610\n1994  451.34146341\n1995 -289.48780488\n1996 -374.90243902\n1997  563.04878049\n1998   82.85365854\n1999   27.82926829\n2000  -39.78048780\n2001  303.82926829\n2002  642.85365854\n2003  506.36585366\n2004  720.12195122\n2005   -4.41463415\n2006  462.12195122\n2007  618.46341463\n2008 -582.56097561\n2009 -408.70731707\n2010 -492.60975610\n2011  145.09756098\n2012  264.51219512\n2013  416.60975610\n2014  415.63414634\n2015 -350.90243902\n2016  156.65853659\n2017 -660.60975610\n2018 -370.26829268\n2019 -167.09756098\n2020 -165.78048780\n2021 -507.00000000\n2022   45.53658537\n2023  -55.24390244\n2024 -603.09756098\n2025 -716.95121951\n2026  521.58536585\n2027 -156.80487805\n2028  142.65853659\n2029 -161.39024390\n2030  489.48780488\n2031   31.19512195\n2032 -464.31707317\n2033  105.68292683\n2034 -117.73170732\n2035  607.14634146\n2036 -615.97560976\n2037 -350.41463415\n2038 -124.21951220\n2039  -95.14634146\n2040 -631.97560976\n2041  677.82926829\n2042 -464.60975610\n2043  -55.43902439\n2044 -549.58536585\n2045 -265.92682927\n2046 -747.97560976\n2047  632.95121951\n2048   17.63414634\n2049 -652.26829268\n2050 -446.02439024\n2051  402.02439024\n2052  144.95121951\n2053 -149.73170732\n2054   -0.70731707\n2055  -47.82926829\n2056 -170.36585366\n2057  145.53658537\n2058  -37.29268293\n2059 -471.19512195\n2060 -125.39024390\n2061  144.07317073\n2062  302.31707317\n2063  509.00000000\n2064 -771.04878049\n2065 -517.14634146\n2066  524.41463415\n2067  -93.53658537\n2068 -743.82926829\n2069 -297.24390244\n2070 -393.58536585\n2071 -191.34146341\n2072  -60.36585366\n2073  638.70731707\n2074  106.65853659\n2075  390.65853659\n2076 -578.56097561\n2077  523.19512195\n2078 -175.24390244\n2079 -130.46341463\n2080  527.24390244\n2081   34.85365854\n2082   10.60975610\n2083  -32.17073171\n2084   61.63414634\n2085 -111.82926829\n2086  -11.09756098\n2087  606.02439024\n2088 -355.04878049\n2089  159.24390244\n2090  174.75609756\n2091 -692.46341463\n2092   47.34146341\n2093  150.51219512\n2094  -14.07317073\n2095 -368.12195122\n2096 -137.19512195\n2097 -654.07317073\n2098  267.34146341\n2099  164.02439024\n2100 -460.51219512\n2101 -181.43902439\n2102 -207.04878049\n2103  427.34146341\n2104  515.97560976\n2105   21.48780488\n2106  520.21951220\n2107 -640.21951220\n2108   -7.24390244\n2109  564.46341463\n2110  -90.80487805\n2111  574.17073171\n2112  -86.07317073\n2113 -721.34146341\n2114  358.07317073\n2115 -236.46341463\n2116  182.75609756\n2117 -400.26829268\n2118  608.21951220\n2119   16.26829268\n2120   48.51219512\n2121  272.21951220\n2122  554.02439024\n2123  426.75609756\n2124 -436.02439024\n2125  210.02439024\n2126  576.51219512\n2127  180.75609756\n2128  -88.41463415\n2129 -175.19512195\n2130 -454.17073171\n2131  406.80487805\n2132  397.04878049\n2133  170.65853659\n2134  109.43902439\n2135   53.68292683\n2136    0.85365854\n2137  494.65853659\n2138  595.04878049\n2139  550.56097561\n2140  361.97560976\n2141  -30.46341463\n2142  390.60975610\n2143  -65.92682927\n2144 -473.97560976\n2145 -581.43902439\n2146  -23.58536585\n2147  714.75609756\n2148  685.58536585\n2149   55.58536585\n2150  125.82926829\n2151 -534.75609756\n2152  306.95121951\n2153  -21.97560976\n2154  591.58536585\n2155  -78.02439024\n2156 -283.82926829\n2157    7.00000000\n2158 -100.31707317\n2159 -456.36585366\n2160 -140.02439024\n2161  595.00000000\n2162 -116.26829268\n2163  208.75609756\n2164  458.46341463\n2165   40.12195122\n2166 -182.95121951\n2167  150.85365854\n2168 -509.58536585\n2169  595.34146341\n2170  694.75609756\n2171   -2.90243902\n2172  626.41463415\n2173   74.90243902\n2174  -25.63414634\n2175  412.21951220\n2176   62.85365854\n2177  612.95121951\n2178  105.14634146\n2179  137.04878049\n2180  -32.12195122\n2181  461.48780488\n2182  123.87804878\n2183   42.80487805\n2184  634.12195122\n2185  799.82926829\n2186  366.95121951\n2187  791.97560976\n2188 -133.73170732\n2189   99.73170732\n2190  217.29268293\n2191 -256.41463415\n2192 -114.46341463\n2193  157.14634146\n2194  471.58536585\n2195 -198.70731707\n2196  252.85365854\n2197 -107.29268293\n2198  630.46341463\n2199  517.14634146\n2200  464.75609756\n2201 -107.24390244\n2202  207.43902439\n2203 -194.65853659\n2204   34.46341463\n2205  -36.75609756\n2206 -188.65853659\n2207  -45.53658537\n2208  392.56097561\n2209 -134.95121951\n2210  253.34146341\n2211  724.46341463\n2212 -208.02439024\n2213  284.12195122\n2214 -144.56097561\n2215  263.82926829\n2216  325.73170732\n2217 -578.31707317\n2218 -167.00000000\n2219  555.00000000\n2220 -352.31707317\n2221  106.51219512\n2222  -73.34146341\n2223  363.04878049\n2224 -233.19512195\n2225  260.56097561\n2226  -92.41463415\n2227   81.73170732\n2228  -88.07317073\n2229 -116.60975610\n2230 -169.39024390\n2231 -467.24390244\n2232  -87.82926829\n2233  665.63414634\n2234 -347.97560976\n2235  -10.31707317\n2236  -97.58536585\n2237 -273.29268293\n2238  625.53658537\n2239  550.65853659\n2240 -122.02439024\n2241   75.68292683\n2242  -53.19512195\n2243 -483.73170732\n2244  443.34146341\n2245 -553.97560976\n2246  -16.07317073\n2247  167.73170732\n2248 -743.14634146\n2249  573.29268293\n2250 -513.29268293\n2251  -71.19512195\n2252  449.82926829\n2253   67.92682927\n2254  729.82926829\n2255 -689.53658537\n2256   13.53658537\n2257  273.09756098\n2258 -720.65853659\n2259 -615.39024390\n2260  -34.46341463\n2261  228.31707317\n2262 -643.19512195\n2263  218.41463415\n2264  -97.68292683\n2265  362.36585366\n2266  281.87804878\n2267  550.90243902\n2268  208.31707317\n2269   18.26829268\n2270 -221.34146341\n2271 -316.80487805\n2272  -75.78048780\n2273  -56.60975610\n2274 -614.17073171\n2275 -436.95121951\n2276  -64.75609756\n2277 -530.95121951\n2278 -447.92682927\n2279  -97.92682927\n2280  -86.65853659\n2281 -160.75609756\n2282 -197.24390244\n2283 -403.19512195\n2284  -93.09756098\n2285  279.19512195\n2286 -672.90243902\n2287 -271.63414634\n2288 -533.19512195\n2289   17.73170732\n2290  -38.80487805\n2291 -593.19512195\n2292   83.19512195\n2293  -95.53658537\n2294  115.97560976\n2295  -78.36585366\n2296   72.41463415\n2297 -805.73170732\n2298 -410.17073171\n2299  -36.90243902\n2300  204.65853659\n2301 -324.70731707\n2302 -547.24390244\n2303 -230.31707317\n2304 -490.70731707\n2305  825.24390244\n2306 -645.14634146\n2307   72.75609756\n2308  585.29268293\n2309   30.02439024\n2310  -16.85365854\n2311  122.51219512\n2312 -137.19512195\n2313  351.53658537\n2314   96.26829268\n2315  231.78048780\n2316  691.34146341\n2317 -559.19512195\n2318  532.65853659\n2319 -106.12195122\n2320    2.80487805\n2321  159.43902439\n2322 -415.53658537\n2323 -104.51219512\n2324  513.68292683\n2325 -571.29268293\n2326  641.09756098\n2327  265.29268293\n2328  -10.90243902\n2329  575.29268293\n2330  181.43902439\n2331   18.41463415\n2332  -10.70731707\n2333 -556.02439024\n2334  -21.00000000\n2335 -562.85365854\n2336 -296.07317073\n2337  466.12195122\n2338  343.58536585\n2339 -211.09756098\n2340 -200.36585366\n2341 -139.04878049\n2342  614.51219512\n2343  734.07317073\n2344 -709.00000000\n2345 -703.97560976\n2346  -80.90243902\n2347  692.95121951\n2348  562.41463415\n2349  388.36585366\n2350  383.73170732\n2351  519.68292683\n2352 -156.65853659\n2353  145.19512195\n2354  289.48780488\n2355 -580.85365854\n2356  -81.92682927\n2357  -39.24390244\n2358  625.24390244\n2359 -788.26829268\n2360   11.78048780\n2361 -889.09756098\n2362   24.07317073\n2363 -559.09756098\n2364 -291.14634146\n2365  -71.29268293\n2366 -588.95121951\n2367  331.04878049\n2368  708.80487805\n2369  585.00000000\n2370 -644.21951220\n2371   14.85365854\n2372 -432.56097561\n2373    1.09756098\n2374  504.26829268\n2375 -399.87804878\n2376  136.46341463\n2377   -0.85365854\n2378  134.07317073\n2379   11.63414634\n2380  -53.39024390\n2381  157.97560976\n2382  267.53658537\n2383  635.24390244\n2384 -779.63414634\n2385 -106.51219512\n2386  420.17073171\n2387  125.39024390\n2388 -267.53658537\n2389 -526.46341463\n2390 -199.24390244\n2391 -346.02439024\n2392   63.04878049\n2393  586.36585366\n2394  160.85365854\n2395  461.53658537\n2396 -263.39024390\n2397 -477.04878049\n2398  -22.51219512\n2399  -51.63414634\n2400  198.26829268\n2401   30.56097561\n2402 -113.14634146\n2403 -592.75609756\n2404 -149.78048780\n2405  180.26829268\n2406  776.90243902\n2407  -88.60975610\n2408 -667.63414634\n2409 -559.09756098\n2410  -94.12195122\n2411 -626.60975610\n2412  303.04878049\n2413  778.36585366\n2414  245.19512195\n2415  263.58536585\n2416 -121.43902439\n2417  397.00000000\n2418   72.80487805\n2419 -393.48780488\n2420  519.43902439\n2421 -260.56097561\n2422  214.60975610\n2423  -92.41463415\n2424 -255.87804878\n2425 -562.65853659\n2426  439.68292683\n2427  482.75609756\n2428  389.00000000\n2429  423.58536585\n2430 -100.07317073\n2431  117.24390244\n2432  -66.46341463\n2433   51.00000000\n2434 -473.14634146\n2435  149.92682927\n2436 -398.07317073\n2437   11.68292683\n2438  251.43902439\n2439 -123.43902439\n2440 -605.09756098\n2441 -228.65853659\n2442 -166.51219512\n2443   25.00000000\n2444  736.70731707\n2445 -141.92682927\n2446   41.58536585\n2447 -663.92682927\n2448  606.85365854\n2449   56.07317073\n2450  139.43902439\n2451 -470.36585366\n2452 -195.53658537\n2453 -384.02439024\n2454 -151.78048780\n2455   -9.39024390\n2456  -97.09756098\n2457 -452.51219512\n2458  -24.07317073\n2459  -32.07317073\n2460 -434.36585366\n2461 -141.78048780\n2462  560.17073171\n2463   21.82926829\n2464  569.48780488\n2465  411.58536585\n2466  395.53658537\n2467 -546.31707317\n2468 -320.46341463\n2469 -792.07317073\n2470 -754.65853659\n2471 -201.68292683\n2472  467.43902439\n2473  272.51219512\n2474 -112.36585366\n2475   55.19512195\n2476  425.87804878\n2477   66.56097561\n2478 -340.60975610\n2479 -179.97560976\n2480  206.21951220\n2481 -732.07317073\n2482    9.00000000\n2483   58.46341463\n2484  516.51219512\n2485  -27.97560976\n2486 -373.43902439\n2487  -59.09756098\n2488 -730.21951220\n2489  108.51219512\n2490 -520.36585366\n2491   33.29268293\n2492 -607.73170732\n2493  111.63414634\n2494   46.70731707\n2495   80.36585366\n2496   30.17073171\n2497   28.21951220\n2498  108.31707317\n2499  528.95121951\n2500  599.14634146\n2501   23.14634146\n2502  300.60975610\n2503  504.85365854\n2504 -207.97560976\n2505  316.12195122\n2506  305.53658537\n2507 -203.87804878\n2508 -755.09756098\n2509  221.53658537\n2510 -292.17073171\n2511 -288.41463415\n2512 -701.24390244\n2513 -465.58536585\n2514  159.19512195\n2515  -92.80487805\n2516  341.78048780\n2517  485.97560976\n2518  417.29268293\n2519 -457.68292683\n2520  451.48780488\n2521  585.09756098\n2522  738.70731707\n2523 -460.60975610\n2524 -682.02439024\n2525  -72.90243902\n2526  556.65853659\n2527 -228.12195122\n2528 -654.60975610\n2529   63.43902439\n2530 -294.07317073\n2531  422.02439024\n2532  622.02439024\n2533  619.53658537\n2534  203.68292683\n2535  -75.97560976\n2536  -78.65853659\n2537  -97.48780488\n2538 -182.85365854\n2539  316.56097561\n2540   29.58536585\n2541  170.85365854\n2542 -295.68292683\n2543  668.75609756\n2544  559.43902439\n2545  -95.68292683\n2546  632.36585366\n2547  831.34146341\n2548  -83.43902439\n2549    6.85365854\n2550  185.82926829\n2551  159.43902439\n2552  617.58536585\n2553  471.78048780\n2554 -427.53658537\n2555  533.29268293\n2556 -132.07317073\n2557   92.51219512\n2558   66.51219512\n2559 -597.39024390\n2560   73.48780488\n2561  163.34146341\n2562 -352.12195122\n2563 1013.24390244\n2564  691.19512195\n2565 -705.48780488\n2566  499.68292683\n2567  378.90243902\n2568 -229.43902439\n2569 -321.04878049\n2570  187.78048780\n2571 -304.02439024\n2572  -11.29268293\n2573 -354.85365854\n2574  -89.09756098\n2575  -75.14634146\n2576 -205.14634146\n2577  -47.63414634\n2578  339.04878049\n2579 -511.39024390\n2580 -101.53658537\n2581 -205.09756098\n2582  126.36585366\n2583  596.46341463\n2584  241.82926829\n2585 -116.36585366\n2586  487.14634146\n2587  241.14634146\n2588  678.26829268\n2589  433.34146341\n2590 -693.43902439\n2591   19.68292683\n2592 -142.41463415\n2593  209.24390244\n2594 -209.53658537\n2595 -503.09756098\n2596  187.73170732\n2597 -210.51219512\n2598  192.26829268\n2599  298.60975610\n2600 -102.65853659\n2601  518.07317073\n2602 -438.26829268\n2603 -166.17073171\n2604  463.29268293\n2605  542.02439024\n2606 -556.02439024\n2607 -719.58536585\n2608  -65.87804878\n2609  188.90243902\n2610 -567.73170732\n2611 -647.14634146\n2612  -10.80487805\n2613  -43.34146341\n2614 -128.65853659\n2615 -447.53658537\n2616 -775.39024390\n2617  442.65853659\n2618  545.48780488\n2619  324.65853659\n2620 -473.78048780\n2621  -39.09756098\n2622   63.39024390\n2623 -170.26829268\n2624  578.70731707\n2625  106.65853659\n2626 -292.75609756\n2627  497.43902439\n2628 -400.21951220\n2629  -68.70731707\n2630 -136.95121951\n2631  -52.80487805\n2632  -80.75609756\n2633  603.00000000\n2634  -95.43902439\n2635   64.31707317\n2636  522.85365854\n2637 -645.58536585\n2638 -688.12195122\n2639  -13.48780488\n2640  490.80487805\n2641 -261.43902439\n2642  272.02439024\n2643    9.14634146\n2644  506.02439024\n2645 -693.82926829\n2646 -395.73170732\n2647   25.24390244\n2648  210.46341463\n2649 -560.51219512\n2650  425.04878049\n2651  402.26829268\n2652 -129.43902439\n2653 -706.41463415\n2654  120.85365854\n2655  -62.90243902\n2656 -514.41463415\n2657 -119.78048780\n2658  145.04878049\n2659  181.63414634\n2660  556.65853659\n2661  -44.31707317\n2662 -404.70731707\n2663 -161.04878049\n2664 -357.78048780\n2665 -620.41463415\n2666  296.80487805\n2667  504.46341463\n2668  320.85365854\n2669 -541.68292683\n2670 -661.92682927\n2671  255.34146341\n2672 -614.26829268\n2673  507.87804878\n2674  561.82926829\n2675  -85.68292683\n2676  -41.97560976\n2677  352.12195122\n2678 -489.63414634\n2679 -140.12195122\n2680  571.00000000\n2681  325.19512195\n2682  354.21951220\n2683  -66.60975610\n2684  582.26829268\n2685  286.36585366\n2686  -36.51219512\n2687 -720.65853659\n2688 -566.31707317\n2689 -128.21951220\n2690   64.85365854\n2691   12.60975610\n2692   41.29268293\n2693  559.87804878\n2694  264.21951220\n2695 -522.12195122\n2696  539.24390244\n2697 -121.68292683\n2698  -11.43902439\n2699   37.14634146\n2700  611.39024390\n2701   57.09756098\n2702 -734.17073171\n2703   77.14634146\n2704 -604.02439024\n2705 -246.36585366\n2706  231.29268293\n2707  301.48780488\n2708 -123.82926829\n2709 -132.75609756\n2710  130.17073171\n2711  -91.82926829\n2712  196.31707317\n2713 -211.87804878\n2714 -375.78048780\n2715 -728.17073171\n2716   25.14634146\n2717  461.68292683\n2718  508.65853659\n2719  -34.46341463\n2720  477.53658537\n2721  184.56097561\n2722    5.34146341\n2723  191.87804878\n2724  193.87804878\n2725  541.19512195\n2726  172.26829268\n2727 -555.87804878\n2728 -210.80487805\n2729  619.00000000\n2730  118.65853659\n2731 -161.29268293\n2732 -550.65853659\n2733 -532.07317073\n2734 -117.68292683\n2735 -552.95121951\n2736 -127.14634146\n2737  481.58536585\n2738 -600.90243902\n2739  -66.60975610\n2740  581.78048780\n2741  517.09756098\n2742 -362.31707317\n2743    8.36585366\n2744 -594.51219512\n2745 -442.60975610\n2746   35.14634146\n2747 -104.41463415\n2748    3.24390244\n2749  279.14634146\n2750  -82.65853659\n2751  666.75609756\n2752 -743.97560976\n2753 -518.56097561\n2754  195.58536585\n2755  -51.29268293\n2756  214.90243902\n2757 -527.92682927\n2758  436.21951220\n2759 -133.29268293\n2760 -228.31707317\n2761 -275.00000000\n2762   78.46341463\n2763  767.92682927\n2764  112.02439024\n2765   22.95121951\n2766 -571.97560976\n2767  -92.12195122\n2768  223.63414634\n2769 -281.53658537\n2770 -285.97560976\n2771  251.78048780\n2772  705.73170732\n2773  135.92682927\n2774 -575.73170732\n2775 -132.17073171\n2776 -447.09756098\n2777  587.04878049\n2778 -544.36585366\n2779  479.14634146\n2780 -549.92682927\n2781   -2.85365854\n2782  -34.90243902\n2783  286.85365854\n2784 -626.46341463\n2785  113.34146341\n2786  498.60975610\n2787  -18.80487805\n2788 -212.36585366\n2789 -438.46341463\n2790 -678.90243902\n2791  -91.78048780\n2792  382.70731707\n2793 -617.09756098\n2794 -151.24390244\n2795  147.92682927\n2796   96.31707317\n2797   53.14634146\n2798 -702.02439024\n2799  672.17073171\n2800 -669.68292683\n2801 -785.09756098\n2802  500.07317073\n2803  277.82926829\n2804  251.14634146\n2805  484.07317073\n2806 -589.82926829\n2807  -79.68292683\n2808  -90.95121951\n2809 -637.68292683\n2810 -173.00000000\n2811   79.00000000\n2812 -577.19512195\n2813  617.97560976\n2814  319.04878049\n2815  184.80487805\n2816 -142.80487805\n2817  -53.82926829\n2818  467.39024390\n2819   51.04878049\n2820  -12.70731707\n2821 -186.51219512\n2822 -112.31707317\n2823 -746.75609756\n2824  319.09756098\n2825 -580.21951220\n2826  399.00000000\n2827 -392.26829268\n2828  176.36585366\n2829 -144.31707317\n2830  321.73170732\n2831  368.56097561\n2832 -162.31707317\n2833  632.56097561\n2834  515.73170732\n2835  -54.26829268\n2836 -297.14634146\n2837  156.75609756\n2838 -635.43902439\n2839  -83.87804878\n2840  505.19512195\n2841  567.48780488\n2842  -23.73170732\n2843    4.80487805\n2844 -656.70731707\n2845 -498.02439024\n2846   55.63414634\n2847  338.02439024\n2848 -595.87804878\n2849 -506.41463415\n2850  150.75609756\n2851  183.19512195\n2852  -55.73170732\n2853  551.48780488\n2854  665.09756098\n2855  705.97560976\n2856  201.92682927\n2857  292.26829268\n2858  528.51219512\n2859 -291.92682927\n2860   81.68292683\n2861  -94.46341463\n2862  242.51219512\n2863  525.87804878\n2864 -143.09756098\n2865  418.56097561\n2866  141.63414634\n2867 -337.97560976\n2868  -20.21951220\n2869 -537.63414634\n2870  512.02439024\n2871  435.53658537\n2872  506.75609756\n2873  -90.46341463\n2874 -503.48780488\n2875  -22.90243902\n2876 -539.73170732\n2877 -472.46341463\n2878 -321.97560976\n2879 -199.87804878\n2880 -544.17073171\n2881  -27.68292683\n2882  -68.70731707\n2883  410.56097561\n2884 -460.90243902\n2885   82.90243902\n2886  -51.97560976\n2887  736.56097561\n2888  240.41463415\n2889   45.39024390\n2890  437.29268293\n2891  603.87804878\n2892 -326.31707317\n2893  164.46341463\n2894  615.43902439\n2895 -495.24390244\n2896    4.70731707\n2897   84.65853659\n2898 -242.41463415\n2899  -18.26829268\n2900  333.09756098\n2901  171.68292683\n2902 -713.00000000\n2903  138.31707317\n2904  472.12195122\n2905  543.58536585\n2906 -332.36585366\n2907  108.07317073\n2908  357.82926829\n2909  495.53658537\n2910  447.92682927\n2911  743.92682927\n2912  459.58536585\n2913 -444.41463415\n2914 -455.34146341\n2915  -49.04878049\n2916  375.78048780\n2917   20.56097561\n2918 -392.95121951\n2919  -54.26829268\n2920 -465.39024390\n2921 -604.26829268\n2922  656.85365854\n2923 -121.24390244\n2924 -598.75609756\n2925  547.82926829\n2926  642.90243902\n2927   54.36585366\n2928 -257.09756098\n2929 -131.68292683\n2930 -449.39024390\n2931  506.21951220\n2932  139.19512195\n2933  600.95121951\n2934 -132.51219512\n2935 -474.90243902\n2936  583.78048780\n2937  657.82926829\n2938 -220.65853659\n2939   -5.24390244\n2940 -261.87804878\n2941 -267.87804878\n2942  -25.43902439\n2943 -134.31707317\n2944  620.31707317\n2945 -314.41463415\n2946  436.07317073\n2947   57.97560976\n2948 -536.65853659\n2949  -24.21951220\n2950 -558.85365854\n2951  191.04878049\n2952  -78.56097561\n2953  222.60975610\n2954  261.09756098\n2955  -57.24390244\n2956  112.31707317\n2957 -255.39024390\n2958  584.41463415\n2959 -607.68292683\n2960  -81.63414634\n2961  163.04878049\n2962 -367.48780488\n2963 -194.41463415\n2964  -14.85365854\n2965 -476.07317073\n2966  -67.24390244\n2967 -568.31707317\n2968   96.26829268\n2969 -437.63414634\n2970 -384.36585366\n2971  -12.75609756\n2972 -582.90243902\n2973  351.73170732\n2974  566.85365854\n2975 -299.97560976\n2976 -122.12195122\n2977   76.17073171\n2978 -320.70731707\n2979 -165.63414634\n2980 -702.31707317\n2981 -623.82926829\n2982  661.43902439\n2983  382.70731707\n2984  861.19512195\n2985   55.68292683\n2986  481.53658537\n2987 -163.00000000\n2988   -6.36585366\n2989 -222.65853659\n2990 -165.53658537\n2991 -585.34146341\n2992 -348.26829268\n2993  512.26829268\n2994   93.39024390\n2995 -579.87804878\n2996  -67.00000000\n2997  586.51219512\n2998 -602.95121951\n2999 -517.34146341\n3000 -146.80487805\n3001  615.73170732\n3002 -137.43902439\n3003  167.34146341\n3004  591.00000000\n3005 -113.29268293\n3006 -615.19512195\n3007 -135.09756098\n3008  115.48780488\n3009   66.95121951\n3010 -666.12195122\n3011  -10.51219512\n3012   31.34146341\n3013  166.65853659\n3014 -501.53658537\n3015  638.56097561\n3016  -88.21951220\n3017  498.12195122\n3018   70.07317073\n3019  209.19512195\n3020 -514.80487805\n3021 -431.14634146\n3022 -158.36585366\n3023  184.26829268\n3024  701.97560976\n3025  528.85365854\n3026 -250.80487805\n3027  210.12195122\n3028  452.41463415\n3029 -790.90243902\n3030   59.09756098\n3031  -60.56097561\n3032  566.51219512\n3033 -577.78048780\n3034 -213.68292683\n3035 -119.14634146\n3036 -158.07317073\n3037 -211.53658537\n3038 -501.92682927\n3039  -72.02439024\n3040 -557.87804878\n3041 -195.39024390\n3042   40.56097561\n3043  604.17073171\n3044   63.04878049\n3045 -197.87804878\n3046 -613.04878049\n3047  -64.75609756\n3048   -2.46341463\n3049 -142.21951220\n3050 -171.68292683\n3051   22.46341463\n3052  234.21951220\n3053  560.02439024\n3054  532.95121951\n3055 -268.21951220\n3056  -63.97560976\n3057  734.07317073\n3058  -11.43902439\n3059  168.41463415\n3060  395.68292683\n3061 -358.60975610\n3062 -609.63414634\n3063  -15.58536585\n3064  463.24390244\n3065 -703.68292683\n3066  302.51219512\n3067  853.92682927\n3068 -132.02439024\n3069 -187.58536585\n3070  100.80487805\n3071 -494.02439024\n3072  582.60975610\n3073  344.12195122\n3074  360.85365854\n3075  425.48780488\n3076 -520.07317073\n3077 -642.46341463\n3078  -18.65853659\n3079 -336.26829268\n3080 -156.75609756\n3081 -249.14634146\n3082  319.00000000\n3083  196.12195122\n3084  505.14634146\n3085  244.02439024\n3086 -439.58536585\n3087   22.12195122\n3088 -542.17073171\n3089  609.97560976\n3090 -380.95121951\n3091  -83.00000000\n3092 -727.68292683\n3093  -11.78048780\n3094  414.26829268\n3095  211.82926829\n3096   39.73170732\n3097  595.39024390\n3098  -95.78048780\n3099 -531.14634146\n3100 -594.56097561\n3101  193.97560976\n3102  -34.65853659\n3103   32.60975610\n3104 -150.07317073\n3105  652.65853659\n3106  175.19512195\n3107  120.36585366\n3108   77.43902439\n3109   10.75609756\n3110  128.31707317\n3111  642.36585366\n3112   25.63414634\n3113  101.63414634\n3114 -591.24390244\n3115  565.24390244\n3116  332.17073171\n3117  278.31707317\n3118   40.12195122\n3119   59.92682927\n3120 -405.39024390\n3121  159.04878049\n3122  -76.41463415\n3123  128.46341463\n3124  212.17073171\n3125   30.65853659\n3126   95.53658537\n3127  625.14634146\n3128  119.68292683\n3129 -542.70731707\n3130 -381.82926829\n3131   78.41463415\n3132  477.48780488\n3133 -561.43902439\n3134  251.58536585\n3135  542.85365854\n3136 -469.48780488\n3137 -420.17073171\n3138 -239.43902439\n3139  592.02439024\n3140 -381.00000000\n3141 -114.70731707\n3142  -51.09756098\n3143  477.43902439\n3144   17.34146341\n3145 -585.97560976\n3146   54.02439024\n3147   53.78048780\n3148  365.43902439\n3149  516.17073171\n3150  516.60975610\n3151 -501.58536585\n3152 -382.85365854\n3153  202.75609756\n3154  113.48780488\n3155  -75.39024390\n3156  269.58536585\n3157  488.80487805\n3158   73.78048780\n3159  435.58536585\n3160   16.21951220\n3161  485.43902439\n3162 -145.53658537\n3163  -23.14634146\n3164  548.95121951\n3165 -204.85365854\n3166  -36.51219512\n3167 -375.34146341\n3168 -644.26829268\n3169 -514.41463415\n3170 -585.39024390\n3171  -59.04878049\n3172  -63.53658537\n3173 -129.04878049\n3174  -52.02439024\n3175  238.56097561\n3176 -753.04878049\n3177 -765.48780488\n3178 -206.56097561\n3179   55.92682927\n3180  -37.92682927\n3181   25.00000000\n3182   30.41463415\n3183  259.04878049\n3184  354.17073171\n3185 -423.04878049\n3186 -404.70731707\n3187 -204.70731707\n3188  -31.63414634\n3189  -43.58536585\n3190  589.97560976\n3191  -67.87804878\n3192 -672.75609756\n3193  -74.36585366\n3194  -98.31707317\n3195  518.95121951\n3196  370.60975610\n3197  -51.97560976\n3198 -442.90243902\n3199  -13.82926829\n3200  178.56097561\n3201 -821.92682927\n3202  -87.29268293\n3203   -6.41463415\n3204 -149.29268293\n3205 -228.90243902\n3206 -590.31707317\n3207    5.63414634\n3208 -568.26829268\n3209   33.00000000\n3210 -442.46341463\n3211 -125.97560976\n3212 -100.90243902\n3213  424.46341463\n3214 -543.09756098\n3215 -567.48780488\n3216  320.70731707\n3217   65.53658537\n3218  723.24390244\n3219 -396.70731707\n3220 -523.82926829\n3221  122.07317073\n3222  -14.95121951\n3223 -502.41463415\n3224 -603.24390244\n3225 -553.43902439\n3226  235.97560976\n3227 -634.36585366\n3228  420.17073171\n3229  250.65853659\n3230  420.46341463\n3231  717.14634146\n3232 -260.95121951\n3233 -267.97560976\n3234   72.12195122\n3235  -10.02439024\n3236 -551.68292683\n3237  460.60975610\n3238    7.73170732\n3239  473.09756098\n3240 -181.04878049\n3241  -75.09756098\n3242  210.46341463\n3243 -175.97560976\n3244  342.26829268\n3245  462.12195122\n3246 -425.48780488\n3247  786.65853659\n3248  365.73170732\n3249 -421.43902439\n3250 -370.41463415\n3251 -520.60975610\n3252 -564.12195122\n3253  353.53658537\n3254  177.48780488\n3255  449.63414634\n3256  -65.82926829\n3257 -245.34146341\n3258 -423.78048780\n3259  653.87804878\n3260  -48.80487805\n3261   -1.14634146\n3262 -371.00000000\n3263 -279.73170732\n3264 -379.19512195\n3265  555.24390244\n3266 -815.68292683\n3267   17.82926829\n3268 -650.60975610\n3269 -370.51219512\n3270 -366.17073171\n3271 -121.34146341\n3272   50.80487805\n3273   70.75609756\n3274  -27.48780488\n3275   24.85365854\n3276  -98.02439024\n3277 -663.04878049\n3278 -548.12195122\n3279 -131.63414634\n3280  -55.87804878\n3281 -304.80487805\n3282  533.92682927\n3283  223.34146341\n3284 -217.48780488\n3285  120.02439024\n3286 -421.48780488\n3287  120.31707317\n3288 -687.82926829\n3289 -741.92682927\n3290  640.70731707\n3291  103.63414634\n3292  111.14634146\n3293 -291.73170732\n3294  -64.07317073\n3295  475.43902439\n3296  284.85365854\n3297  739.43902439\n3298 -623.87804878\n3299 -532.75609756\n3300 -618.07317073\n3301  714.36585366\n3302  -72.85365854\n3303  411.34146341\n3304 -217.04878049\n3305  525.82926829\n3306   21.34146341\n3307   -6.60975610\n3308  263.73170732\n3309   54.31707317\n3310   -7.63414634\n3311  313.68292683\n3312  403.82926829\n3313  -85.97560976\n3314  -81.53658537\n3315 -740.21951220\n3316  619.29268293\n3317   98.17073171\n3318 -592.17073171\n3319   29.04878049\n3320  521.82926829\n3321 -135.73170732\n3322   32.95121951\n3323  -96.17073171\n3324  706.07317073\n3325  182.21951220\n3326  271.14634146\n3327  588.46341463\n3328  635.97560976\n3329  560.85365854\n3330 -864.60975610\n3331 -264.26829268\n3332 -197.19512195\n3333 -368.85365854\n3334    9.68292683\n3335  137.82926829\n3336  339.53658537\n3337  426.17073171\n3338  571.97560976\n3339 -719.58536585\n3340  496.75609756\n3341 -381.53658537\n3342  440.56097561\n3343  -89.00000000\n3344   53.24390244\n3345 -462.17073171\n3346   87.82926829\n3347 -493.63414634\n3348 -410.85365854\n3349  242.70731707\n3350  511.39024390\n3351  546.31707317\n3352   39.63414634\n3353  415.14634146\n3354   31.04878049\n3355 -427.09756098\n3356  -10.46341463\n3357  172.85365854\n3358  605.78048780\n3359    4.07317073\n3360  546.65853659\n3361 -427.82926829\n3362 -527.04878049\n3363 -549.48780488\n3364  755.04878049\n3365  665.87804878\n3366  -88.46341463\n3367 -733.78048780\n3368  661.19512195\n3369  514.65853659\n3370 -117.34146341\n3371 -844.36585366\n3372  191.78048780\n3373  -14.80487805\n3374  125.29268293\n3375   36.85365854\n3376  548.26829268\n3377  -18.41463415\n3378  948.51219512\n3379 -283.87804878\n3380   66.12195122\n3381  711.82926829\n3382 -854.41463415\n3383 -170.80487805\n3384  227.09756098\n3385  562.12195122\n3386  -46.56097561\n3387 -252.90243902\n3388 -424.95121951\n3389  326.70731707\n3390  699.29268293\n3391  195.82926829\n3392  568.21951220\n3393  374.60975610\n3394  761.00000000\n3395  171.00000000\n3396 -187.29268293\n3397  -99.39024390\n3398   43.04878049\n3399 -542.41463415\n3400   17.68292683\n3401 -451.09756098\n3402 -401.24390244\n3403  173.04878049\n3404  333.58536585\n3405  381.53658537\n3406 -357.87804878\n3407 -134.90243902\n3408 -304.21951220\n3409  368.60975610\n3410 -636.80487805\n3411  124.85365854\n3412  641.19512195\n3413 -574.95121951\n3414  366.36585366\n3415 -612.41463415\n3416  470.17073171\n3417 -387.39024390\n3418 -319.53658537\n3419 -252.70731707\n3420  303.00000000\n3421 -470.12195122\n3422   15.78048780\n3423  -25.19512195\n3424 -267.39024390\n3425 -643.97560976\n3426 -224.17073171\n3427  500.02439024\n3428  -98.36585366\n3429 -187.09756098\n3430 -627.29268293\n3431  581.92682927\n3432  173.14634146\n3433  689.53658537\n3434   53.14634146\n3435 -759.34146341\n3436  242.12195122\n3437  194.21951220\n3438  649.78048780\n3439  170.75609756\n3440 -493.68292683\n3441 -750.46341463\n3442 -735.68292683\n3443   95.58536585\n3444  196.51219512\n3445 -172.26829268\n3446 -501.92682927\n3447    9.09756098\n3448 -440.07317073\n3449  136.07317073\n3450 -213.53658537\n3451  229.92682927\n3452 -645.92682927\n3453 -629.48780488\n3454  -58.17073171\n3455 -562.80487805\n3456  444.95121951\n3457  168.31707317\n3458  541.58536585\n3459  781.39024390\n3460   54.70731707\n3461  161.43902439\n3462  700.07317073\n3463  106.21951220\n3464 -191.87804878\n3465  535.39024390\n3466 -234.21951220\n3467 -207.43902439\n3468 -185.14634146\n3469  540.21951220\n3470  225.24390244\n3471  528.56097561\n3472  392.17073171\n3473    6.90243902\n3474  407.97560976\n3475  -11.97560976\n3476  471.82926829\n3477  188.21951220\n3478  228.12195122\n3479  228.51219512\n3480  625.92682927\n3481  118.31707317\n3482  -46.80487805\n3483 -149.09756098\n3484 -560.12195122\n3485 -453.68292683\n3486  620.56097561\n3487 -197.48780488\n3488  573.87804878\n3489   -1.24390244\n3490    7.68292683\n3491   -4.07317073\n3492  -89.34146341\n3493  -94.56097561\n3494 -655.53658537\n3495  -57.48780488\n3496   36.51219512\n3497  -65.68292683\n3498  -96.90243902\n3499  586.70731707\n3500  244.41463415\n3501 -449.04878049\n3502  138.95121951\n3503 -117.87804878\n3504  133.58536585\n3505 -599.92682927\n3506  258.80487805\n3507  -74.56097561\n3508 -289.53658537\n3509  645.34146341\n3510   78.46341463\n3511  164.26829268\n3512  747.04878049\n3513 -523.92682927\n3514   -8.26829268\n3515  344.17073171\n3516 -102.21951220\n3517  166.07317073\n3518 -490.07317073\n3519  -79.19512195\n3520   41.53658537\n3521  643.14634146\n3522  774.95121951\n3523  401.34146341\n3524   82.02439024\n3525   -0.75609756\n3526  113.29268293\n3527 -365.00000000\n3528 -380.95121951\n3529 -571.82926829\n3530  517.43902439\n3531 -246.95121951\n3532  -23.39024390\n3533  -98.36585366\n3534 -387.78048780\n3535 -422.60975610\n3536 -113.97560976\n3537  228.56097561\n3538  675.19512195\n3539 -508.21951220\n3540  780.21951220\n3541 -333.04878049\n3542  -78.17073171\n3543  256.02439024\n3544 -105.19512195\n3545  399.29268293\n3546  -64.80487805\n3547 -435.97560976\n3548  118.75609756\n3549  446.17073171\n3550  -55.97560976\n3551  282.17073171\n3552 -186.80487805\n3553  152.65853659\n3554 -469.82926829\n3555  657.29268293\n3556  -31.97560976\n3557  -45.53658537\n3558  161.82926829\n3559 -149.39024390\n3560 -169.24390244\n3561  369.48780488\n3562 -653.00000000\n3563  720.21951220\n3564 -571.43902439\n3565 -508.85365854\n3566  101.73170732\n3567  609.82926829\n3568  -51.63414634\n3569 -415.58536585\n3570  478.36585366\n3571 -498.51219512\n3572  -79.73170732\n3573 -352.95121951\n3574 -703.87804878\n3575    4.85365854\n3576   32.70731707\n3577 -434.75609756\n3578  556.46341463\n3579  512.26829268\n3580  515.29268293\n3581  733.14634146\n3582   55.48780488\n3583  542.36585366\n3584  -94.51219512\n3585 -704.75609756\n3586   38.46341463\n3587 -165.63414634\n3588  525.14634146\n3589 -121.78048780\n3590 -579.97560976\n3591 -625.00000000\n3592 -187.09756098\n3593  541.19512195\n3594  -89.63414634\n3595 -199.43902439\n3596 -422.90243902\n3597  -92.12195122\n3598 -555.82926829\n3599  -66.41463415\n3600  172.36585366\n3601  497.53658537\n3602  138.12195122\n3603 -636.31707317\n3604 -341.24390244\n3605 -291.53658537\n3606 -708.75609756\n3607  205.63414634\n3608  319.53658537\n3609 -601.53658537\n3610  186.80487805\n3611 -204.07317073\n3612 -609.92682927\n3613  -19.43902439\n3614  315.09756098\n3615  189.09756098\n3616  432.65853659\n3617   25.73170732\n3618 -454.75609756\n3619   66.31707317\n3620  528.65853659\n3621  735.34146341\n3622  -12.65853659\n3623  -38.07317073\n3624  -42.70731707\n3625  338.26829268\n3626  -52.07317073\n3627   76.26829268\n3628  360.51219512\n3629 -103.29268293\n3630 -465.04878049\n3631   48.60975610\n3632 -296.21951220\n3633  409.00000000\n3634 -430.80487805\n3635 -245.34146341\n3636  490.80487805\n3637 -175.39024390\n3638   37.97560976\n3639 -722.75609756\n3640  556.12195122\n3641 -683.39024390\n3642 -550.75609756\n3643    6.56097561\n3644 -527.39024390\n3645  -78.02439024\n3646  -80.80487805\n3647 -486.60975610\n3648  360.21951220\n3649   -1.82926829\n3650  651.73170732\n3651 -412.07317073\n3652  111.73170732\n3653  537.97560976\n3654  417.04878049\n3655 -578.85365854\n3656  400.60975610\n3657  413.39024390\n3658  535.00000000\n3659  246.12195122\n3660 -654.36585366\n3661 -646.80487805\n3662 -587.92682927\n3663 -784.46341463\n3664  748.31707317\n3665  590.26829268\n3666  497.73170732\n3667  351.09756098\n3668 -533.82926829\n3669  133.04878049\n3670  432.80487805\n3671 -517.39024390\n3672   28.95121951\n3673  -67.34146341\n3674  304.60975610\n3675  129.00000000\n3676 -204.65853659\n3677 -401.82926829\n3678  705.58536585\n3679  180.02439024\n3680    0.85365854\n3681 -637.29268293\n3682  361.68292683\n3683  328.36585366\n3684  867.43902439\n3685  447.48780488\n3686   66.85365854\n3687  475.97560976\n3688  540.80487805\n3689 -638.21951220\n3690 -272.26829268\n3691  651.48780488\n3692 -631.04878049\n3693  251.82926829\n3694 -536.56097561\n3695  640.65853659\n3696  132.26829268\n3697  470.65853659\n3698 -273.19512195\n3699 -188.65853659\n3700  589.39024390\n3701 -209.39024390\n3702 -611.68292683\n3703 -596.36585366\n3704   65.53658537\n3705  -98.51219512\n3706 -221.58536585\n3707  -99.29268293\n3708  132.46341463\n3709  144.02439024\n3710  613.58536585\n3711  393.78048780\n3712  -84.46341463\n3713 -401.34146341\n3714  206.12195122\n3715  155.34146341\n3716  440.70731707\n3717 -378.70731707\n3718 -372.95121951\n3719  179.92682927\n3720  103.04878049\n3721 -596.60975610\n3722  512.85365854\n3723  470.21951220\n3724   61.04878049\n3725  -13.00000000\n3726  768.36585366\n3727  388.12195122\n3728  -77.82926829\n3729  114.90243902\n3730  183.19512195\n3731 -701.09756098\n3732    7.63414634\n3733 -734.21951220\n3734  452.56097561\n3735  122.21951220\n3736   98.90243902\n3737 -376.56097561\n3738  273.78048780\n3739  125.63414634\n3740  481.29268293\n3741  155.34146341\n3742 -415.48780488\n3743  -95.04878049\n3744  216.60975610\n3745 -147.14634146\n3746 -180.85365854\n3747 -198.02439024\n3748   48.56097561\n3749   51.29268293\n3750  238.75609756\n3751   63.48780488\n3752 -486.21951220\n3753  601.58536585\n3754  493.48780488\n3755 -430.95121951\n3756 -154.26829268\n3757 -510.21951220\n3758  647.24390244\n3759 -234.36585366\n3760 -134.31707317\n3761 -193.97560976\n3762 -670.36585366\n3763  201.24390244\n3764   68.85365854\n3765   26.85365854\n3766 -457.63414634\n3767  297.04878049\n3768 -589.73170732\n3769 -201.87804878\n3770 -571.97560976\n3771  -37.04878049\n3772 -606.17073171\n3773  626.07317073\n3774  604.65853659\n3775   23.58536585\n3776  472.07317073\n3777 -402.70731707\n3778  -27.63414634\n3779  417.92682927\n3780  667.48780488\n3781  105.97560976\n3782   86.60975610\n3783  820.95121951\n3784  131.73170732\n3785  372.65853659\n3786 -232.80487805\n3787    7.43902439\n3788 -202.46341463\n3789 -382.56097561\n3790 -737.63414634\n3791 -362.31707317\n3792 -532.75609756\n3793 -231.29268293\n3794 -140.02439024\n3795  718.95121951\n3796  299.14634146\n3797 -253.63414634\n3798  -33.73170732\n3799 -580.90243902\n3800  235.04878049\n3801  269.09756098\n3802   64.75609756\n3803  301.09756098\n3804  476.31707317\n3805 -611.63414634\n3806 -147.78048780\n3807 -103.58536585\n3808 -163.24390244\n3809 -507.78048780\n3810 -707.78048780\n3811   78.41463415\n3812    4.46341463\n3813  123.73170732\n3814  521.53658537\n3815  586.41463415\n3816   -8.26829268\n3817 -554.85365854\n3818 -242.56097561\n3819    8.41463415\n3820  197.04878049\n3821 -428.65853659\n3822  465.82926829\n3823  152.36585366\n3824  636.65853659\n3825  -39.29268293\n3826 -431.04878049\n3827  157.53658537\n3828  -76.65853659\n3829  751.29268293\n3830  319.14634146\n3831 -497.24390244\n3832  151.43902439\n3833 -672.70731707\n3834  -90.65853659\n3835  -42.65853659\n3836   87.29268293\n3837  591.48780488\n3838  662.56097561\n3839  465.24390244\n3840  503.14634146\n3841  452.75609756\n3842  -37.04878049\n3843  616.41463415\n3844  132.31707317\n3845   15.04878049\n3846  427.82926829\n3847 -190.07317073\n3848 -317.73170732\n3849  605.63414634\n3850  -17.82926829\n3851 -687.00000000\n3852 -152.26829268\n3853 -235.24390244\n3854  389.29268293\n3855 -522.46341463\n3856   26.75609756\n3857  -89.48780488\n3858  759.00000000\n3859   31.63414634\n3860 -502.85365854\n3861  662.46341463\n3862   55.87804878\n3863  662.56097561\n3864   66.51219512\n3865  483.09756098\n3866  468.21951220\n3867  315.63414634\n3868 -257.00000000\n3869  221.78048780\n3870 -224.51219512\n3871  -49.78048780\n3872   29.09756098\n3873   38.95121951\n3874  -68.90243902\n3875   45.68292683\n3876   89.68292683\n3877   -1.48780488\n3878  -77.24390244\n3879 -345.39024390\n3880 -650.95121951\n3881  439.24390244\n3882  214.02439024\n3883 -594.90243902\n3884 -256.41463415\n3885 -426.70731707\n3886  -97.48780488\n3887 -486.51219512\n3888 -675.14634146\n3889  751.53658537\n3890  207.34146341\n3891 -398.12195122\n3892  384.70731707\n3893 -634.56097561\n3894 -635.43902439\n3895 -594.60975610\n3896 -639.68292683\n3897  -24.31707317\n3898  503.00000000\n3899 -229.53658537\n3900  470.12195122\n3901 -596.12195122\n3902  555.43902439\n3903  370.21951220\n3904  -31.97560976\n3905 -488.80487805\n3906  445.78048780\n3907 -738.31707317\n3908  257.43902439\n3909  141.63414634\n3910 -578.36585366\n3911  477.24390244\n3912 -192.12195122\n3913 -484.80487805\n3914 -319.09756098\n3915 -498.17073171\n3916 -697.48780488\n3917   53.58536585\n3918  352.65853659\n3919  428.07317073\n3920  -94.17073171\n3921 -198.51219512\n3922 -140.70731707\n3923 -800.90243902\n3924  188.12195122\n3925  527.39024390\n3926  518.36585366\n3927   53.04878049\n3928 -777.63414634\n3929 -730.12195122\n3930  547.43902439\n3931  251.53658537\n3932   -3.00000000\n3933 -628.26829268\n3934   -1.19512195\n3935 -111.39024390\n3936 -381.29268293\n3937 -431.00000000\n3938  284.21951220\n3939  107.78048780\n3940 -621.19512195\n3941   10.17073171\n3942  585.82926829\n3943 -119.63414634\n3944  574.56097561\n3945   88.51219512\n3946  215.87804878\n3947   36.95121951\n3948 -137.14634146\n3949 -544.65853659\n3950 -319.39024390\n3951 -170.51219512\n3952 -225.97560976\n3953  570.17073171\n3954   57.04878049\n3955 -109.14634146\n3956 -164.17073171\n3957  633.58536585\n3958   16.02439024\n3959 -436.12195122\n3960 -519.29268293\n3961 -574.41463415\n3962 -478.31707317\n3963  611.29268293\n3964   18.46341463\n3965  393.78048780\n3966 -491.19512195\n3967  103.04878049\n3968 -172.80487805\n3969   64.12195122\n3970  -27.29268293\n3971 -633.29268293\n3972  157.82926829\n3973  547.78048780\n3974 -187.87804878\n3975 -702.85365854\n3976  214.75609756\n3977   42.21951220\n3978  137.34146341\n3979  696.80487805\n3980  -68.90243902\n3981  -58.65853659\n3982 -681.24390244\n3983 -635.58536585\n3984 -442.60975610\n3985 -695.63414634\n3986  701.82926829\n3987 -149.92682927\n3988    0.90243902\n3989  -45.43902439\n3990 -460.36585366\n3991  557.82926829\n3992  563.68292683\n3993 -202.02439024\n3994  507.68292683\n3995  793.97560976\n3996  389.34146341\n3997  249.34146341\n3998  653.48780488\n3999 -235.14634146\n4000  462.46341463\n4001  526.56097561\n4002 -598.65853659\n4003 -677.73170732\n4004  444.07317073\n4005  437.39024390\n4006 -813.09756098\n4007 -406.85365854\n4008 -682.36585366\n4009  192.65853659\n4010  562.46341463\n4011    1.19512195\n4012  469.58536585\n4013   64.02439024\n4014   11.68292683\n4015 -566.51219512\n4016 -337.09756098\n4017  546.26829268\n4018  316.12195122\n4019   38.41463415\n4020 -174.21951220\n4021  116.90243902\n4022 -157.53658537\n4023  628.26829268\n4024 -478.70731707\n4025  175.09756098\n4026  524.41463415\n4027  212.12195122\n4028 -514.85365854\n4029  181.97560976\n4030 -615.29268293\n4031 -114.07317073\n4032 -473.73170732\n4033 -726.41463415\n4034  514.02439024\n4035  -69.73170732\n4036  414.51219512\n4037 -600.60975610\n4038 -632.26829268\n4039 -525.24390244\n4040  477.09756098\n4041 -725.39024390\n4042 -163.58536585\n4043   58.12195122\n4044  151.19512195\n4045 -267.00000000\n4046  -33.53658537\n4047 -765.73170732\n4048  592.02439024\n4049 -422.56097561\n4050   12.65853659\n4051  117.58536585\n4052  421.68292683\n4053   17.29268293\n4054  703.39024390\n4055  196.31707317\n4056 -216.65853659\n4057  485.48780488\n4058 -552.90243902\n4059 -146.95121951\n4060 -782.70731707\n4061 -112.65853659\n4062  -78.65853659\n4063 -308.60975610\n4064 -817.04878049\n4065  183.04878049\n4066 -105.39024390\n4067 -404.65853659\n4068  127.92682927\n4069  -73.87804878\n4070  289.92682927\n4071   -4.70731707\n4072 -493.29268293\n4073  428.75609756\n4074 -224.17073171\n4075 -482.90243902\n4076 -530.80487805\n4077 -343.00000000\n4078 -721.87804878\n4079 -527.97560976\n4080 -478.75609756\n4081    3.73170732\n4082  -21.24390244\n4083 -622.65853659\n4084 -656.70731707\n4085 -583.92682927\n4086   12.36585366\n4087 -528.95121951\n4088 -536.80487805\n4089  893.09756098\n4090   86.21951220\n4091 -240.80487805\n4092   54.90243902\n4093   32.85365854\n4094  766.65853659\n4095  371.19512195\n4096 -455.24390244\n4097  498.02439024\n4098 -154.17073171\n4099 -464.70731707\n4100  -80.95121951\n4101  563.53658537\n4102 -599.24390244\n4103  -92.31707317\n4104 -733.58536585\n4105   -7.97560976\n4106  428.56097561\n4107  746.36585366\n4108   91.39024390\n4109  517.73170732\n4110   -1.97560976\n4111 -549.58536585\n4112 -402.70731707\n4113  185.34146341\n4114 -131.24390244\n4115  551.82926829\n4116  193.58536585\n4117 -608.02439024\n4118  547.14634146\n4119 -321.48780488\n4120  515.14634146\n4121 -107.73170732\n4122 -484.75609756\n4123 -722.75609756\n4124 -568.90243902\n4125  120.51219512\n4126  595.39024390\n4127 -310.31707317\n4128 -354.85365854\n4129  287.58536585\n4130 -240.07317073\n4131  597.34146341\n4132  -41.82926829\n4133 -455.53658537\n4134  309.19512195\n4135   71.24390244\n4136   53.87804878\n4137  -43.92682927\n4138  487.34146341\n4139 -466.41463415\n4140  327.97560976\n4141  176.46341463\n4142  738.41463415\n4143   70.51219512\n4144  642.26829268\n4145 -806.46341463\n4146   21.92682927\n4147  -37.39024390\n4148  775.53658537\n4149 -302.12195122\n4150   26.95121951\n4151  199.24390244\n4152 -450.95121951\n4153  676.80487805\n4154  198.51219512\n4155 -463.39024390\n4156  -81.78048780\n4157  451.29268293\n4158  732.02439024\n4159  -94.12195122\n4160  547.04878049\n4161 -662.60975610\n4162 -672.17073171\n4163  558.80487805\n4164 -361.00000000\n4165  149.87804878\n4166   75.14634146\n4167  -48.21951220\n4168  130.36585366\n4169 -130.85365854\n4170 -186.51219512\n4171 -524.70731707\n4172    3.53658537\n4173 -583.68292683\n4174 -515.04878049\n4175  610.17073171\n4176 -557.68292683\n4177  477.00000000\n4178  201.68292683\n4179   91.87804878\n4180  -85.63414634\n4181  -71.34146341\n4182  -47.53658537\n4183   92.46341463\n4184  231.63414634\n4185  582.60975610\n4186  -99.68292683\n4187  506.90243902\n4188  533.73170732\n4189 -233.09756098\n4190 -577.43902439\n4191   56.12195122\n4192  628.95121951\n4193  407.82926829\n4194  -54.70731707\n4195  288.65853659\n4196 -144.46341463\n4197  122.95121951\n4198   98.85365854\n4199  145.04878049\n4200  263.24390244\n4201  783.43902439\n4202 -481.87804878\n4203 -408.36585366\n4204  419.04878049\n4205 -579.43902439\n4206   58.02439024\n4207  -21.68292683\n4208 -520.17073171\n4209 -114.90243902\n4210   33.14634146\n4211  -50.95121951\n4212  276.60975610\n4213 -457.63414634\n4214 -693.87804878\n4215 -643.04878049\n4216  616.21951220\n4217  133.48780488\n4218 -179.48780488\n4219  358.07317073\n4220   81.68292683\n4221 -190.80487805\n4222 -704.36585366\n4223 -508.51219512\n4224  434.56097561\n4225 -726.51219512\n4226  246.36585366\n4227 -276.17073171\n4228   77.63414634\n4229  255.82926829\n4230  156.51219512\n4231  174.90243902\n4232  435.00000000\n4233 -601.48780488\n4234 -168.21951220\n4235  531.78048780\n4236 -522.41463415\n4237 -139.63414634\n4238 -532.65853659\n4239  480.51219512\n4240 -434.36585366\n4241   48.65853659\n4242 -115.24390244\n4243 -261.43902439\n4244 -351.29268293\n4245 -103.39024390\n4246  -74.02439024\n4247  618.85365854\n4248 -129.43902439\n4249  -45.53658537\n4250 -315.29268293\n4251 -412.65853659\n4252  200.31707317\n4253 -285.48780488\n4254   -5.39024390\n4255  633.34146341\n4256 -509.97560976\n4257  -37.00000000\n4258  178.41463415\n4259  109.43902439\n4260 -197.97560976\n4261  -97.97560976\n4262 -115.04878049\n4263 -230.80487805\n4264  682.12195122\n4265  633.68292683\n4266  625.78048780\n4267 -416.31707317\n4268  437.68292683\n4269   94.21951220\n4270  -71.39024390\n4271 -234.75609756\n4272 -482.31707317\n4273 -182.07317073\n4274 -159.73170732\n4275  357.63414634\n4276  102.51219512\n4277 -607.24390244\n4278  560.60975610\n4279 -539.53658537\n4280 -410.46341463\n4281  330.07317073\n4282 -407.43902439\n4283  225.82926829\n4284  -68.36585366\n4285  205.58536585\n4286  -15.04878049\n4287   67.92682927\n4288   50.60975610\n4289   92.75609756\n4290  -23.68292683\n4291  587.97560976\n4292  700.75609756\n4293  544.31707317\n4294  579.39024390\n4295   49.68292683\n4296    3.92682927\n4297  656.12195122\n4298  574.41463415\n4299 -392.17073171\n4300  211.48780488\n4301 -618.65853659\n4302  466.60975610\n4303  159.19512195\n4304 -133.24390244\n4305  576.51219512\n4306 -365.48780488\n4307 -667.82926829\n4308 -216.21951220\n4309 -647.00000000\n4310 -443.48780488\n4311 -463.19512195\n4312   40.17073171\n4313 -394.85365854\n4314   11.39024390\n4315  560.12195122\n4316  485.39024390\n4317  131.39024390\n4318    0.70731707\n4319  629.78048780\n4320  -20.95121951\n4321  181.53658537\n4322 -654.70731707\n4323 -641.63414634\n4324 -356.51219512\n4325  -58.80487805\n4326  122.17073171\n4327  295.48780488\n4328  -45.53658537\n4329   32.60975610\n4330   21.53658537\n4331   58.12195122\n4332  542.07317073\n4333 -602.90243902\n4334   37.53658537\n4335  816.31707317\n4336  300.46341463\n4337  -34.17073171\n4338 -207.39024390\n4339 -829.00000000\n4340 -299.58536585\n4341  621.63414634\n4342 -381.97560976\n4343 -668.26829268\n4344 -366.60975610\n4345  571.39024390\n4346   -4.70731707\n4347   85.19512195\n4348 -143.29268293\n4349 -669.87804878\n4350 -588.07317073\n4351  706.60975610\n4352  546.12195122\n4353  -64.95121951\n4354 -534.60975610\n4355 -794.70731707\n4356 -513.04878049\n4357  256.07317073\n4358  267.48780488\n4359  523.14634146\n4360  123.92682927\n4361  606.95121951\n4362 -131.58536585\n4363 -335.00000000\n4364 -171.63414634\n4365 -132.36585366\n4366 -464.41463415\n4367  -13.63414634\n4368 -153.82926829\n4369   44.36585366\n4370 -135.43902439\n4371 -568.41463415\n4372 -196.51219512\n4373  573.58536585\n4374 -693.58536585\n4375  208.65853659\n4376 -691.58536585\n4377 -423.97560976\n4378 -252.21951220\n4379   31.04878049\n4380 -391.00000000\n4381  432.02439024\n4382   23.48780488\n4383  -96.02439024\n4384  402.90243902\n4385 -453.68292683\n4386 -570.12195122\n4387  287.78048780\n4388 -630.85365854\n4389  -63.87804878\n4390 -564.75609756\n4391 -587.19512195\n4392  623.39024390\n4393  190.75609756\n4394 -167.87804878\n4395 -443.78048780\n4396 -115.68292683\n4397 -217.39024390\n4398  -96.12195122\n4399 -363.68292683\n4400  338.26829268\n4401 -527.09756098\n4402   73.09756098\n4403  -76.56097561\n4404  653.53658537\n4405  515.14634146\n4406  609.09756098\n4407 -631.63414634\n4408 -167.78048780\n4409  407.73170732\n4410  492.36585366\n4411   30.26829268\n4412  686.70731707\n4413  307.82926829\n4414  651.92682927\n4415 -199.14634146\n4416   78.70731707\n4417 -704.21951220\n4418 -476.21951220\n4419  140.21951220\n4420 -379.39024390\n4421 -864.21951220\n4422  830.60975610\n4423  533.73170732\n4424 -610.70731707\n4425  400.46341463\n4426 -724.46341463\n4427 -542.12195122\n4428  -67.24390244\n4429  531.09756098\n4430  506.60975610\n4431  -35.87804878\n4432 -138.75609756\n4433  287.58536585\n4434   71.48780488\n4435   16.51219512\n4436  -44.51219512\n4437  205.87804878\n4438  554.75609756\n4439 -630.51219512\n4440 -256.80487805\n4441 -388.70731707\n4442   76.41463415\n4443  597.43902439\n4444  773.92682927\n4445  746.65853659\n4446  539.82926829\n4447 -438.46341463\n4448  127.97560976\n4449  205.53658537\n4450  171.43902439\n4451  -47.87804878\n4452 -326.90243902\n4453 -602.41463415\n4454 -519.29268293\n4455  502.56097561\n4456  -19.39024390\n4457  473.87804878\n4458  -46.41463415\n4459  155.97560976\n4460   30.75609756\n4461 -631.14634146\n4462  153.09756098\n4463 -128.02439024\n4464  370.65853659\n4465 -574.31707317\n4466  539.29268293\n4467  759.14634146\n4468  563.19512195\n4469 -700.26829268\n4470  536.95121951\n4471 -116.17073171\n4472  -36.90243902\n4473   83.78048780\n4474  430.31707317\n4475  488.41463415\n4476 -202.12195122\n4477   -6.17073171\n4478 -153.09756098\n4479 -155.24390244\n4480 -705.73170732\n4481  662.31707317\n4482 -217.63414634\n4483 -497.29268293\n4484 -251.82926829\n4485 -115.29268293\n4486  606.41463415\n4487  604.56097561\n4488  103.48780488\n4489  748.46341463\n4490  638.17073171\n4491 -102.41463415\n4492   40.36585366\n4493 -206.90243902\n4494 -586.21951220\n4495   -6.56097561\n4496 -614.70731707\n4497  566.80487805\n4498 -477.39024390\n4499  -21.87804878\n4500  119.73170732\n4501  611.09756098\n4502 -203.29268293\n4503  232.46341463\n4504 -642.36585366\n4505  -81.24390244\n4506 -443.63414634\n4507 -311.82926829\n4508  -43.68292683\n4509  553.00000000\n4510 -775.92682927\n4511 -577.53658537\n4512   86.26829268\n4513 -214.31707317\n4514  -30.51219512\n4515 -561.34146341\n4516  306.90243902\n4517 -119.14634146\n4518  411.48780488\n4519  470.21951220\n4520 -174.95121951\n4521 -186.21951220\n4522 -446.60975610\n4523 -201.39024390\n4524  700.51219512\n4525 -633.97560976\n4526  592.17073171\n4527  380.21951220\n4528  538.17073171\n4529 -381.53658537\n4530  375.92682927\n4531  724.02439024\n4532  -89.14634146\n4533 -632.65853659\n4534 -471.09756098\n4535 -177.58536585\n4536 -400.56097561\n4537  590.65853659\n4538 -322.21951220\n4539 -201.82926829\n4540   59.29268293\n4541   34.36585366\n4542  -13.73170732\n4543 -152.95121951\n4544 -764.36585366\n4545 -381.58536585\n4546  121.39024390\n4547 -211.97560976\n4548  204.02439024\n4549  541.58536585\n4550  759.04878049\n4551  132.75609756\n4552 -352.36585366\n4553 -717.73170732\n4554 -133.09756098\n4555  -79.63414634\n4556  218.51219512\n4557 -488.90243902\n4558  111.68292683\n4559   -2.31707317\n4560  111.43902439\n4561 -107.78048780\n4562  573.34146341\n4563 -337.04878049\n4564   52.36585366\n4565 -343.14634146\n4566  478.80487805\n4567 -208.65853659\n4568  -23.29268293\n4569 -575.53658537\n4570  487.68292683\n4571 -136.90243902\n4572  503.34146341\n4573 -329.14634146\n4574   62.80487805\n4575  430.65853659\n4576  127.19512195\n4577 -457.53658537\n4578   89.82926829\n4579  558.60975610\n4580 -201.24390244\n4581   33.92682927\n4582  510.80487805\n4583  157.73170732\n4584  431.43902439\n4585 -778.70731707\n4586 -729.29268293\n4587  401.43902439\n4588 -170.41463415\n4589   19.00000000\n4590 -520.02439024\n4591  188.60975610\n4592 -171.53658537\n4593  359.63414634\n4594  534.56097561\n4595  248.75609756\n4596  -30.90243902\n4597  518.75609756\n4598  477.63414634\n4599 -418.36585366\n4600 -536.21951220\n4601  479.78048780\n4602   68.60975610\n4603  208.12195122\n4604  441.00000000\n4605 -118.85365854\n4606 -523.09756098\n4607  818.12195122\n4608  -46.02439024\n4609 -193.19512195\n4610  360.85365854\n4611   88.85365854\n4612   27.09756098\n4613   89.39024390\n4614 -335.24390244\n4615  -17.82926829\n4616  -35.73170732\n4617  406.70731707\n4618  -38.95121951\n4619  257.24390244\n4620  337.97560976\n4621 -214.65853659\n4622  654.21951220\n4623 -524.02439024\n4624 -160.90243902\n4625 -176.02439024\n4626   74.21951220\n4627  460.46341463\n4628 -648.70731707\n4629 -526.46341463\n4630 -775.14634146\n4631  305.78048780\n4632  560.95121951\n4633 -594.95121951\n4634 -542.31707317\n4635 -695.92682927\n4636  -89.39024390\n4637  572.80487805\n4638  536.95121951\n4639 -362.51219512\n4640  -13.24390244\n4641 -125.48780488\n4642 -433.29268293\n4643  114.51219512\n4644 -712.41463415\n4645  297.04878049\n4646 -424.95121951\n4647 -189.87804878\n4648  -71.19512195\n4649 -427.97560976\n4650  426.02439024\n4651   89.04878049\n4652 -564.41463415\n4653 -137.68292683\n4654  674.07317073\n4655  212.17073171\n4656 -150.31707317\n4657 -129.04878049\n4658  426.90243902\n4659 -152.36585366\n4660  637.53658537\n4661 -601.04878049\n4662  638.70731707\n4663  527.29268293\n4664  785.43902439\n4665  472.41463415\n4666 -561.04878049\n4667 -361.39024390\n4668  120.02439024\n4669  -38.36585366\n4670 -133.68292683\n4671  150.90243902\n4672 -233.48780488\n4673  437.04878049\n4674  112.31707317\n4675  443.78048780\n4676 -703.73170732\n4677   45.09756098\n4678   62.12195122\n4679    9.82926829\n4680  429.24390244\n4681  369.53658537\n4682 -718.31707317\n4683 -536.80487805\n4684  378.95121951\n4685 -613.87804878\n4686  -63.48780488\n4687 -450.95121951\n4688 -270.80487805\n4689 -130.31707317\n4690 -312.02439024\n4691  -30.56097561\n4692  631.34146341\n4693 -428.02439024\n4694 -189.24390244\n4695  365.73170732\n4696   33.43902439\n4697  548.41463415\n4698 -482.26829268\n4699 -176.56097561\n4700  141.82926829\n4701 -499.34146341\n4702  473.97560976\n4703  332.02439024\n4704  590.75609756\n4705 -112.02439024\n4706    8.75609756\n4707 -612.31707317\n4708   88.80487805\n4709  116.02439024\n4710  465.68292683\n4711  533.24390244\n4712  327.09756098\n4713  164.36585366\n4714 -674.80487805\n4715  127.24390244\n4716 -776.80487805\n4717  -46.51219512\n4718 -331.73170732\n4719  208.36585366\n4720  196.12195122\n4721  -47.97560976\n4722   83.00000000\n4723 -586.60975610\n4724 -488.60975610\n4725  154.26829268\n4726 -737.53658537\n4727   54.26829268\n4728  474.70731707\n4729 -570.56097561\n4730  -49.43902439\n4731  509.19512195\n4732  512.02439024\n4733 -664.75609756\n4734  118.65853659\n4735  170.56097561\n4736 -118.21951220\n4737 -121.09756098\n4738 -457.24390244\n4739  189.24390244\n4740  -17.00000000\n4741  137.19512195\n4742  482.85365854\n4743  153.78048780\n4744  434.07317073\n4745 -842.02439024\n4746 -486.07317073\n4747    0.60975610\n4748  -61.00000000\n4749 -294.51219512\n4750 -360.21951220\n4751  119.58536585\n4752 -400.21951220\n4753 -108.95121951\n4754  635.53658537\n4755   -6.21951220\n4756 -150.51219512\n4757  640.75609756\n4758 -486.46341463\n4759  385.24390244\n4760 -238.07317073\n4761 -157.63414634\n4762 -312.51219512\n4763  315.63414634\n4764 -635.58536585\n4765  766.70731707\n4766  193.34146341\n4767  469.48780488\n4768  534.46341463\n4769  -61.63414634\n4770  500.36585366\n4771 -133.68292683\n4772 -198.26829268\n4773 -508.65853659\n4774   35.00000000\n4775  134.60975610\n4776 -365.19512195\n4777  262.17073171\n4778 -195.73170732\n4779  -75.63414634\n4780 -623.09756098\n4781  527.00000000\n4782  222.41463415\n4783  450.46341463\n4784   13.04878049\n4785 -210.75609756\n4786  632.17073171\n4787  571.00000000\n4788 -241.92682927\n4789 -294.60975610\n4790  -28.95121951\n4791   94.36585366\n4792  682.21951220\n4793 -292.85365854\n4794  487.14634146\n4795  227.48780488\n4796   -0.80487805\n4797 -446.36585366\n4798  512.07317073\n4799 -207.58536585\n4800  -27.68292683\n4801 -596.02439024\n4802 -493.87804878\n4803  -21.14634146\n4804 -738.65853659\n4805  118.70731707\n4806  125.92682927\n4807  573.48780488\n4808 -449.14634146\n4809   57.24390244\n4810  498.80487805\n4811  -24.21951220\n4812  631.48780488\n4813 -349.78048780\n4814 -762.70731707\n4815  539.39024390\n4816  118.70731707\n4817  121.43902439\n4818   14.95121951\n4819  776.36585366\n4820 -486.31707317\n4821  158.07317073\n4822   78.21951220\n4823  695.58536585\n4824 -608.95121951\n4825  373.19512195\n4826  -34.41463415\n4827 -603.97560976\n4828 -341.09756098\n4829  457.68292683\n4830  451.14634146\n4831  -58.95121951\n4832    0.31707317\n4833  650.80487805\n4834 -593.58536585\n4835  528.02439024\n4836   43.53658537\n4837  551.43902439\n4838  -63.29268293\n4839 -636.80487805\n4840  277.29268293\n4841  413.53658537\n4842  -22.17073171\n4843 -428.95121951\n4844  170.41463415\n4845  331.73170732\n4846   52.41463415\n4847  560.80487805\n4848 -522.12195122\n4849  776.70731707\n4850 -383.73170732\n4851 -566.85365854\n4852  701.14634146\n4853   47.78048780\n4854 -191.63414634\n4855 -408.51219512\n4856   45.82926829\n4857  107.09756098\n4858   61.58536585\n4859   63.00000000\n4860 -553.63414634\n4861  636.12195122\n4862  562.41463415\n4863 -446.21951220\n4864  428.60975610\n4865  340.80487805\n4866  435.34146341\n4867  752.17073171\n4868 -536.46341463\n4869 -331.24390244\n4870  618.75609756\n4871  456.75609756\n4872  728.60975610\n4873   47.29268293\n4874 -199.92682927\n4875   41.24390244\n4876  762.17073171\n4877  -14.41463415\n4878  343.39024390\n4879   14.70731707\n4880 -163.29268293\n4881  452.36585366\n4882 -380.07317073\n4883  301.97560976\n4884  392.07317073\n4885   59.53658537\n4886 -540.21951220\n4887   81.97560976\n4888 -748.07317073\n4889  -97.24390244\n4890  145.24390244\n4891 -164.60975610\n4892  207.48780488\n4893 -781.43902439\n4894 -439.97560976\n4895  376.75609756\n4896  -80.65853659\n4897 -107.14634146\n4898   82.80487805\n4899 -546.80487805\n4900  -73.97560976\n4901  353.82926829\n4902  -88.75609756\n4903   17.24390244\n4904   70.75609756\n4905  -85.09756098\n4906 -838.51219512\n4907  613.43902439\n4908  729.48780488\n4909 -109.09756098\n4910 -422.31707317\n4911  524.80487805\n4912 -694.41463415\n4913  341.39024390\n4914 -211.09756098\n4915  652.70731707\n4916  149.43902439\n4917   -4.36585366\n4918  -78.75609756\n4919  314.41463415\n4920   37.29268293\n4921 -274.26829268\n4922 -685.82926829\n4923  728.41463415\n4924  526.36585366\n4925  495.53658537\n4926  645.73170732\n4927  -74.56097561\n4928  -23.24390244\n4929 -269.43902439\n4930 -724.17073171\n4931  497.92682927\n4932 -136.60975610\n4933 -627.97560976\n4934  697.48780488\n4935  623.00000000\n4936  115.63414634\n4937  145.24390244\n4938   46.85365854\n4939 -140.12195122\n4940  -61.97560976\n4941  539.68292683\n4942  675.68292683\n4943 -472.65853659\n4944 -150.85365854\n4945  434.26829268\n4946  -94.95121951\n4947  130.21951220\n4948 -209.39024390\n4949  621.24390244\n4950 -259.14634146\n4951 -792.65853659\n4952  -17.29268293\n4953 -371.24390244\n4954 -121.58536585\n4955  613.53658537\n4956   72.26829268\n4957  422.56097561\n4958 -557.53658537\n4959  332.02439024\n4960  443.43902439\n4961  335.73170732\n4962   16.56097561\n4963 -208.90243902\n4964  614.17073171\n4965 -501.43902439\n4966 -647.43902439\n4967  387.19512195\n4968   25.92682927\n4969  -18.31707317\n4970  287.82926829\n4971 -265.29268293\n4972  132.46341463\n4973  313.34146341\n4974 -154.12195122\n4975  686.70731707\n4976 -646.02439024\n4977 -423.19512195\n4978 -672.31707317\n4979   63.53658537\n4980 -416.12195122\n4981  445.34146341\n4982 -674.56097561\n4983  204.95121951\n4984  565.09756098\n4985  -83.29268293\n4986  609.19512195\n4987   27.82926829\n4988  -17.73170732\n4989  461.63414634\n4990  -56.75609756\n4991 -598.90243902\n4992 -520.65853659\n4993 -148.95121951\n4994 -609.43902439\n4995  -66.90243902\n4996  -22.36585366\n4997   77.14634146\n4998 -173.58536585\n4999  424.07317073\n\n\nThe p-value obtained through the permutation test.\n\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_money)\n\nprop_TRUE \n     0.44 \n\n\nVisualization of the observed mean relative to the null distribution:\n\ngf_histogram(data = null_dist_money, ~ diffmean, \n             bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, \n           colour = \"darkred\", linewidth = 1,\n           title = \"Null Distribution by Permutation\", \n           subtitle = \"Histogram\") %&gt;% \n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\nHere we begin by creating a null distribution for the difference in means by performing 4,999 permutations- each engraving and different values among the 2 gender groups. We do this by randomly reassigning the ‘Gender’ variable across all observations\nprop1 calculates the p-value by comparing the observed difference in means (obs_diff_gender) with the null distribution.\nObservations:\n\nThe p-value observed through the permutation test is 0.41. If it would have been a value close to 0, it would indicate that that the observed difference is unlikely to have occurred by chance, leading to a rejection of the null hypothesis. But in this case the higher value i.e. 0.41, suggests that we cannot reject the null hypothesis- There is no difference in the amount of pocket money received by male and female students .\nThe visualization of the histogram of mean differences- null distribution along with the observed mean different suggests the same - I cannot reject the null hypothesis.\n\n\nConclusion:\nIn the assumption that my sample population is a true representation of my population- There is no difference in the amount spent by male and female students on a random day- and so there must be no difference in the pocket money received by both genders."
  },
  {
    "objectID": "posts/Experiment3/index.html#desciptive-analysis",
    "href": "posts/Experiment3/index.html#desciptive-analysis",
    "title": "Experiment 3",
    "section": "Desciptive Analysis:",
    "text": "Desciptive Analysis:\n\nTansforming and inspecting the data:\n\ngrades_modified &lt;- grades_data %&gt;%\n  dplyr::mutate(\n    Gender = as_factor(Gender),\n    Year = factor(Year,\n      levels = c(\"1\",\"2\",\"3\"),\n      labels = c(\"1\",\"2\",\"3\"),\n      ordered = TRUE),\n    Degree = as_factor(Degree),\n    Course = as_factor(Course)\n    )%&gt;%\n  select(Degree , Course, Year, Gender, Score)\ngrades_modified\n\n   Degree Course Year Gender Score\n1   B.Des    CAC    2      F   8.0\n2   B.Des    CAC    2      F   9.6\n3   B.Des   IADP    2      F   9.2\n4   B.Des     CE    2      F   9.8\n5   B.Des   BSSD    2      M   3.0\n6   B.Des    CAC    2      F   9.5\n7   B.Des    PSD    2      F   9.0\n8   B.Des    PSD    2      F   9.0\n9   B.Des    PSD    2      F   9.0\n10  B.Des   BSSD    3      F   9.0\n11  B.Des   VCSB    2      F   8.0\n12  B.Des   VCSB    2      F   8.0\n13  B.Des  IAIDP    3      M   8.0\n14  B.Des  IAIDP    3      M   7.0\n15  B.Des   VCSB    2      F   7.0\n16  B.Des    HCD    3      M   9.0\n17  B.Des    CAC    3      F   9.1\n18  B.Des   VCSB    2      M   8.0\n19  B.Des     CE    2      F   9.0\n20  B.Des    CAC    2      M   9.0\n21  B.Des   IADP    2      F   8.0\n22  B.Des   IADP    3      M   7.0\n23  B.Des   VCSB    3      F   9.0\n24  B.Des     CE    2      F  10.0\n25  B.Des    CAC    3      F   9.0\n26  B.Des    CAC    3      F   9.0\n27  B.Des    PSD    3      F   7.0\n28  B.Des    PSD    3      F   8.0\n29  B.Des    CAC    2      F   9.0\n30  B.Des    HCD    2      F   8.0\n31  B.Voc   UIID    3      F   9.5\n32  B.Voc   UIID    3      F   9.0\n33  B.Voc   UIID    3      M   9.0\n34  B.Voc   UIID    3      M   7.0\n35  B.Voc   UIID    3      F   8.0\n36  B.Voc   UIID    3      F   8.0\n37  B.Voc     ID    3      M   7.0\n38  B.Voc    DMP    3      M   9.5\n39  B.Voc    DMP    3      M   6.5\n40  B.Voc    DMP    3      M   9.5\n41  B.Voc    DMP    3      M   9.5\n42  B.Voc    DMP    3      M   9.0\n43  B.Voc    DMP    3      F   9.5\n44  B.Voc    DMP    3      F   9.5\n45  B.Voc    DMP    3      F   6.5\n46  B.Voc    DMP    3      M   5.0\n47  B.Voc   UIID    1      M   9.0\n48  B.Voc   UIID    1      F   7.0\n49  B.Voc   UIID    1      F   7.0\n50  B.Voc   UIID    1      F   8.0\n51  B.Voc   UIID    1      F   8.0\n52  B.Voc   UIID    1      F   9.0\n53  B.Voc   UIID    2      M   8.0\n54  B.Voc   GADP    1      F   8.0\n55  B.Voc   GADP    2      M   8.0\n56  B.Voc   Film    3      M   8.0\n57  B.Voc   GADP    3      F   9.0\n58  B.Voc   GADP    3      F   9.0\n59  B.Voc    DMP    2      M   8.0\n60  B.Voc    DMP    2      M   8.0\n61   B.FA    CAP    3      F   8.0\n62   B.FA    CAP    3      M   9.0\n63   B.FA    CAP    3      F   9.0\n64   B.FA    DMA    2      M   8.5\n65   B.FA    DMA    2      M   8.0\n66   B.FA    DMA    2      F   8.0\n67   B.FA    CAP    2      F   8.0\n68   B.FA    CAP    2      F   8.0\n69   B.FA   Film    3      M   6.0\n70   B.FA    DMA    2      F   8.0\n71   B.FA    DMA    2      F   7.0\n72   B.FA    DMA    2      F   7.0\n73   B.FA    DMA    2      F   8.0\n74   B.FA    DMA    2      F   7.0\n75   B.FA    DMA    2      M   6.0\n76   B.FA   Film    3      M   8.0\n77   B.FA   Film    3      M   8.0\n78   B.FA    DMA    2      M   8.0\n79   B.FA    DMA    2      M   8.0\n80   B.FA    DMA    2      F   7.0\n81   B.FA    DMA    2      F   7.0\n82   B.FA   Film    2      F   8.0\n83   B.FA   Film    3      F   7.0\n84   B.FA    DMA    2      F   9.0\n85   B.FA    DMA    2      M   8.0\n86   B.FA    DMA    2      F   7.0\n87   B.FA    DMA    2      F   7.0\n88   B.FA    DMA    2      M   7.0\n89   B.FA    DMA    2      F   7.0\n90   B.FA    DMA    2      F   7.0\n\n\n\nglimpse(grades_modified)\n\nRows: 90\nColumns: 5\n$ Degree &lt;fct&gt; B.Des, B.Des, B.Des, B.Des, B.Des, B.Des, B.Des, B.Des, B.Des, …\n$ Course &lt;fct&gt; CAC, CAC, IADP, CE, BSSD, CAC, PSD, PSD, PSD, BSSD, VCSB, VCSB,…\n$ Year   &lt;ord&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, …\n$ Gender &lt;fct&gt; F, F, F, F, M, F, F, F, F, F, F, F, M, M, F, M, F, M, F, M, F, …\n$ Score  &lt;dbl&gt; 8.0, 9.6, 9.2, 9.8, 3.0, 9.5, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0…\n\n\n\nskim(grades_modified)\n\n\nData summary\n\n\nName\ngrades_modified\n\n\nNumber of rows\n90\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDegree\n0\n1\nFALSE\n3\nB.D: 30, B.V: 30, B.F: 30\n\n\nCourse\n0\n1\nFALSE\n15\nDMA: 20, UII: 13, DMP: 11, CAC: 8\n\n\nYear\n0\n1\nTRUE\n3\n2: 46, 3: 37, 1: 7\n\n\nGender\n0\n1\nFALSE\n2\nF: 57, M: 33\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nScore\n0\n1\n8.06\n1.13\n3\n7\n8\n9\n10\n▁▁▆▇▇\n\n\n\n\n\nObservations:\n\nI have an unequal count of levels of genders and Years and although this doesn’t prevent me from testing for significant differences, the statistical power of the tests might be affected, especially if one group is very small compared to the other, which is how it is in both cases. I think it would have been fine if i was looking to find the proportions but since that is not my focus, I think I’ll stick to trying to find if there are significant differences in grades between students enrolled in different degree programs.\nThe lowest grade obtained seems to be 3 while the highest seems to be 10. The mean value is 8.03 which suggests that the distribution of all scores together is left skewed.\n\n\n\nSummeries and Visualisations of the data\nDistribution of overall grades:\n\ngrades_modified %&gt;%\n  gf_histogram(~Score)\n\n\n\n\n\n\n\n\nObservations:\n\nThe distribution seems to be kind of normal expect for an outlier at 3?\nThe most common grade obtained is 8.\n\n\ngf_histogram(~Score, fill = ~Degree, data = grades_modified, alpha=0.5) %&gt;%\n  gf_labs(title = \"Grades faceted by Degree\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ngrades_modified %&gt;%\n  gf_histogram(~Score|Degree, fill = ~Degree, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Grades faceted by Degree\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1 \n    )\n  ))\n\n\n\n\n\n\n\n\n\ngrades_modified %&gt;%\n  gf_density(\n    ~ Score,\n    fill = ~ Degree,\n    alpha = 0.5,\n    title = \"Grades Densities by Degrees \",\n    subtitle = \"B.Des vs B.Voc us B.FA\"\n  )\n\n\n\n\n\n\n\n\nObservations:\n\nThe plot suggests that B.Des students might have more high scores (8 and above) compared to B.FA and B.Voc students.\nThe B.FA program has a wider distribution, with students scoring from as low as 5 to as high as 9.7. The other programs (B.Des and B.Voc) show a denser concentration in the higher score range, with fewer students scoring below 6.\nThe spread and shape differences across these distributions suggest possible variations in grading patterns or student performance across programs.\n\n\nCrosstables:\nTrying to find the difference in means and medians of grades obtained degree vise within the sample population to get a better understanding of what the data is saying.\n\ncrosstable(Score ~ Degree, data = grades_modified) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariableDegreeB.DesB.VocB.FAScoreMin / Max3.0 / 10.05.0 / 9.56.0 / 9.0Med [IQR]9.0 [8.0;9.0]8.0 [8.0;9.0]8.0 [7.0;8.0]Mean (std)8.4 (1.3)8.2 (1.1)7.6 (0.8)N (NA)30 (0)30 (0)30 (0)\n\n\nObservations:\n\nThe mean values indicate that B.Des students, on average, have higher scores, followed by B.Voc and then B.FA.\nThe Standard deviation suggests that B.FA has the lowest variability in scores while B.Des has the most.\n\nVisualizing the median and IQR through a box plot to better understand it.\n\ngrades_modified %&gt;%\n  gf_boxplot(Degree ~ Score, fill = ~Degree, alpha=0.5) %&gt;%\n  gf_labs(title = \"Box plot of Scores by Degree\")\n\n\n\n\n\n\n\n\nObservations:\n\nThe median tip for 2 groups B.Voc and B.FA are 8, Indicating half the participants got more than 8 and half got less than 8. But their IQR ranges are different. B.FA is 7.0-8.0 while B.Voc is 8.0;9.0 which indicated that even through their medians are the same, B.Voc students tend to get more scores among the 2?\nB.Des has a higher median, indicating overall higher scores compared to the other two groups.There is an outlier in B.Des with a score below 4, which likely represents an unusually low performance in this group.\nThe IQR ranges of B.Voc and B.Des overlap perfectly with each other. Therefore 50 percent of all grades obtained by there 2 groups are in the same small range. In this case, is there really a significant enough difference in the grades ontained by the 2 groups?\n\nHypothesis:\nNull Hypothesis: There is no differences in grades between students enrolled in different degree programs.\nAlternative Hypothesis: B.FA students tend get lower grades than students studying for other degrees. Is there a difference in scores achieved by B.Des and B.Voc? I don’t think there would be one significant enough looking at the descriptive analysis\nWe can reasonably hypothesize that there is a difference in scores based on degree, but is this difference statistically significant? We can use ANOVA for a pair-wise comparison and find out!"
  },
  {
    "objectID": "posts/Experiment3/index.html#statistical-analysis",
    "href": "posts/Experiment3/index.html#statistical-analysis",
    "title": "Experiment 3",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nANOVA:\nAssumptions: 1. Data (and standard errors) are normally distributed. 2. Variances between all groups (B.Des, B.FA and B.Voc) are equal. 3. Observations are independent.\n\ngrades_anova &lt;- aov(Score ~ Degree, data = grades_modified) \ngrades_supernova &lt;- supernova::pairwise(grades_anova,\n  correction = \"Bonferroni\",\n  alpha = 0.05, # 95% CI calculation\n  var_equal = TRUE, \n  plot = TRUE\n) \n\n\n\n\n\n\n\n\n\ngrades_supernova\n\n\n\n\n── Pairwise t-tests with Bonferroni correction ─────────────────────────────────\n\n\nModel: Score ~ Degree\n\n\nDegree\n\n\nLevels: 3\n\n\nFamily-wise error-rate: 0.049\n\n\n\n  group_1 group_2   diff pooled_se      t    df  lower  upper  p_adj\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 B.Voc   B.Des   -0.173     0.201 -0.864    87 -0.607  0.261 1.0000\n2 B.FA    B.Des   -0.757     0.201 -3.770    87 -1.191 -0.323  .0009\n3 B.FA    B.Voc   -0.583     0.201 -2.906    87 -1.017 -0.149  .0139\n\n\nObservations:\n\nThe confidence interval for the difference between B.Voc and B.Des includes zero, meaning we cannot conclude to state there is a significant difference in scores between the two. This is supported by the p_adj value of the comparison of these group- 1.00, which suggests that there is no statistically significant difference between the 2 groups, the result we see could be by chance.\nThe confidence interval between B.FA - B.Des and B.FA - B.Voc so no straddle 0 which suggests that there is a statistically significant difference in the mean grades received between B.FA and both groups. This is supported by the p-adj values obtained which are both below 0.05.\nThe computed difference in means are -0.757 and -0.583. This suggests that B.Des students, on average, get 0.75 points more than B.FA students and B.Voc students, on average, get 0.58 points more than B.FA students. The effect is larger for the comparison between B.FA and B.Des compared to B.FA and B.Voc.\nCould any other factor like Year or Course (like too many first years who are still figuring out the ropes compared to the data collected for other degrees) be the reason for the “evidently” lower grades obtained by B.FA students?\n\nWhat if any of my assumptions about the data/groups that wrong? They are crucial to the accuracy of ANOVA! I’ll confirm just to be sure!\n\nTest for Normality:\nThe null hypothesis: All 3 distributions of grades are normal.\n\nshapiro.test(x = grades_modified$Score) %&gt;%\n    broom::tidy()\n\n# A tibble: 1 × 3\n  statistic     p.value method                     \n      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n1     0.885 0.000000920 Shapiro-Wilk normality test\n\n\nObservations:\n\nAlthough the static value (w) is high- 0.88, it’s not high enough to suggest that the we can accept the null hypothesis. Quite the opposite actually, we reject the null hypothesis.\nScience the p-value is less than 0.05, we reject the null hypothesis that the data follows a normal distribution. It’s much smaller than 0.05, indicating that the data significantly deviates from a normal distribution.\n\nFaceted by Groups\n\ngrades_modified %&gt;%\n  group_by(Degree) %&gt;%\n  group_modify(~ .x %&gt;%\n    select(Score) %&gt;%\n    as_vector() %&gt;%\n    shapiro.test() %&gt;%\n    broom::tidy())\n\n# A tibble: 3 × 4\n# Groups:   Degree [3]\n  Degree statistic   p.value method                     \n  &lt;fct&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                      \n1 B.Des      0.753 0.0000101 Shapiro-Wilk normality test\n2 B.Voc      0.887 0.00418   Shapiro-Wilk normality test\n3 B.FA       0.875 0.00211   Shapiro-Wilk normality test\n\n\nVisualizing this:\n\ngrades_modified %&gt;%\n  gf_density( ~ Score,\n              fill = ~ Degree,\n              alpha = 0.5,\n              title = \"Score of different degrees vs normal distribution\") %&gt;%\n  gf_facet_grid(~ Degree) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\nObservations:\nUnsurprisingly, non of the 3 groups are normally distributed. All of them have low w values and really low o-values that indicate that we must reject the null hypothesis- they are not normally distributed.\nThis already makes the inferences we got out of anova invalid. We must use permutation to come to a solid conclusion. But before that, just for kicks, let’s check if the variances within all groups are the same/ similar values.\n\n\nTest for Variance:\n\ngrades_modified %&gt;%\n  group_by(Degree) %&gt;%\n  summarise(variance = var(Score))\n\n# A tibble: 3 × 2\n  Degree variance\n  &lt;fct&gt;     &lt;dbl&gt;\n1 B.Des     1.74 \n2 B.Voc     1.27 \n3 B.FA      0.615\n\n\nAs I has pointed out earlier in the observations of the density graphs, B.Des has the most variance with a value of 1.7 with B.FA with the least- 0.6.\nSince our data is not normally distributed, i can use the fligner test to test the homogeneity of variances.\nNull hypothesis: The variances between the groups are similar.\n\nfligner.test(Score ~ Degree, data = grades_modified)%&gt;%\n    broom::tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                          \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                           \n1      2.27   0.321         2 Fligner-Killeen test of homogeneity of variances\n\n\nSince the p-value is not below 0.05, we cannot reject the null hypothesis of equal variances. This suggests that there is no significant difference in variances between the groups, meaning the assumption of homogeneity of variances is likely satisfied- the data do not have significantly different variances.\n\n\n\nANOVA using Permutation Tests\nPermutation tests are distribution-free, meaning they don’t rely on specific assumptions about the underlying data distribution, including normality and homogeneity of variances, even though we did reject the assumption of similar variances like we did the normality.\nWe want to see if the Grades depends on Degree or if any differences we observe could just be random. The F statistic is a number that tells us how much variation there is between groups (based on degree levels) compared to the variation within each group.\n\nobserved_infer &lt;-\n  grades_modified %&gt;%\n  specify(Score ~ Degree) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  calculate(stat = \"F\")\nobserved_infer\n\nResponse: Score (numeric)\nExplanatory: Degree (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  3.90\n\n\nObservations\n\nNull Hypothesis (independence): There is no statistically significant difference between grades obtained by people studying for different degrees.\nWe see that the observed F-Statistic is 3.900206 which is a high value that suggests that that differences between group means are larger relative to the variation within each group. When the variation within each group is small, but the means are far apart, there is less overlap between the groups, making them more distinct from each other. In simple terms it represents the observed variability in scores among different degrees.\n\nThe null distribution of f-values obtained by permutation\n\nnull_dist_infer &lt;- grades_modified %&gt;%\n  specify(Score ~ Degree) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 4999, type = \"permute\") %&gt;%\n  calculate(stat = \"F\")\n##\nnull_dist_infer\n\nResponse: Score (numeric)\nExplanatory: Degree (factor)\nNull Hypothesis: independence\n# A tibble: 4,999 × 2\n   replicate   stat\n       &lt;int&gt;  &lt;dbl&gt;\n 1         1 0.130 \n 2         2 0.478 \n 3         3 1.73  \n 4         4 1.71  \n 5         5 0.315 \n 6         6 0.0717\n 7         7 1.82  \n 8         8 0.318 \n 9         9 0.102 \n10        10 0.227 \n# ℹ 4,989 more rows\n\n\nVisualizing the presence of observed f value in the null distribution\n\nnull_dist_infer %&gt;%\n  visualise(method = \"simulation\") +\n  shade_p_value(obs_stat = observed_infer$stat, direction = \"right\") +\n  scale_x_continuous(trans = \"log10\", expand = c(0, 0)) +\n  coord_cartesian(xlim = c(0.2, 500), clip = \"off\") +\n  annotation_logticks(outside = FALSE)\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_x_continuous(trans = \"log10\", expand = c(0, 0)): log-10\ntransformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\np_value &lt;- null_dist_infer %&gt;%\n  summarise(p_value = mean(stat &gt;= observed_infer$stat))\np_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0258\n\n\nObservations:\n\nA p-value of 0.019 indicates that there’s only a 1.9% chance of obtaining this result if there were no effect of Degree on Score. With it being so low a value, we can reject the null hypothesis that there is no difference in the grades obtained by students studying for different degrees.\nScores therefore differ significantly by degree but the permutation test evaluates the variability in scores across all groups combined.\nThe results of the permutation suggests that there’s a significant difference among groups, not which specific degrees have higher or lower scores. How do i find this out? Is it just by believing the observed means?\n\n\n\nConclusion:\nWith the permutation test of Anova I was able to conclude that there scores do in fact differ significantly by degree. Taking into account my descriptive analysis of the data, I can positively say that my alternative hypothesis is right- There is a difference in the way B.FA students are graded compared to students in other degrees (B.Des and B.Voc) - They tend to get lower grades. I cannot come to a conclusion regarding it’s status of being statistical significant- because from what I understand, there permutation test shows a low p-value even if there is a statistically significant difference between 2 groups.\nIs there another test i can do to find this?\n\nAmong courses (departments) within B.FA, we can try and find if there is a significant difference in grades received by by any of the groups. Within B.FA, is there a course receiving more or less grades than others and is this difference statistically significant- is it something that happened by chance or not."
  },
  {
    "objectID": "posts/Experiment2/index.html#statistical-analysis-hypothesis-testing",
    "href": "posts/Experiment2/index.html#statistical-analysis-hypothesis-testing",
    "title": "Experiment 2",
    "section": "Statistical Analysis: Hypothesis Testing",
    "text": "Statistical Analysis: Hypothesis Testing\n\nA t-test:\nAssumptions: 1. The distribution for each group - Vegetarians and Non-vegetarians is normal.\n\nmosaic::t_test(Tip ~ Preferance, data = tips_modified)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     2.33      12.3        10     0.503   0.617      46.9    -6.99      11.7\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObservations:\n\nThe estimate suggests that vegetarians, on average, give 2.3rs more tip than non-vegetarians in mu sample data set.\nThe high p-value of 0.6 1, greater than the typical significance level of 0.05, suggests that there is no statistically significant difference in the average tip amounts between vegetarians and non-vegetarians.The result could just be random, so I don’t have strong evidence that something is happening.\n0- which indicates that there is no difference on the average tip, falls within the confidence intervals which suggest the same.\nTherefore we cannot reject the null hypothesis.\n\nWhat if my assumption of these 2 distribution of tip faceted by Food Preference being normal is not right? We could do a Shapiro test to confirm.\n\n\nA Shapiro test:\nShapiro test is one used to determine if a sample comes from a normally distributed population or not.\nFirst we transform our data set to be 2 different ones- one with all the Veg values and the other with all the Non-veg values.\n\nfiltered_Veg &lt;- tips_modified %&gt;%\n  filter(Preferance == \"Veg\")\nfiltered_Non &lt;- tips_modified %&gt;%\n  filter(Preferance == \"Non-veg\")\nfiltered_Veg\n\n       Name Gender Preferance Tip\n1     Aanya Female        Veg   0\n2      Adit   Male        Veg   0\n3     Aditi Female        Veg  20\n4     Anaya Female        Veg  35\n5    Anhuya Female        Veg  40\n6     Anmol   Male        Veg   0\n7      Anna   Male        Veg   0\n8      Ayan   Male        Veg   0\n9      Debu   Male        Veg  15\n10  Dheeman   Male        Veg 100\n11     Diya Female        Veg  30\n12   Khushi Female        Veg   0\n13  Kshraja Female        Veg   0\n14    Mahie Female        Veg   0\n15   Manish   Male        Veg   0\n16 Nanditha Female        Veg   0\n17    Nidhi Female        Veg   0\n18     Ojas   Male        Veg  20\n19   Sadnya Female        Veg   0\n20  Sanjana Female        Veg   0\n21 Shashwat   Male        Veg   0\n22    Shiva   Male        Veg   0\n23   Shreya Female        Veg   0\n24   Srujan   Male        Veg   0\n25   Suhaas   Male        Veg  20\n26 Sushmita Female        Veg  20\n27    Taran   Male        Veg  20\n28    Varad   Male        Veg   0\n29  Vishesh   Male        Veg   0\n30   Zivnya Female        Veg  50\n\nfiltered_Non\n\n        Name Gender Preferance Tip\n1      Akash   Male    Non-veg   0\n2    Akshita Female    Non-veg   0\n3   Anandita Female    Non-veg   0\n4     Ananya Female    Non-veg  20\n5      Ankit   Male    Non-veg   0\n6   Anoushka Female    Non-veg   0\n7      Arnav   Male    Non-veg   0\n8     Arushi Female    Non-veg   0\n9   Ashutosh   Male    Non-veg   0\n10     Asark Female    Non-veg  20\n11   Hardik    Male    Non-veg   0\n12  Ignatius   Male    Non-veg  20\n13  Mahendra   Male    Non-veg  20\n14    Nevaan   Male    Non-veg   0\n15   Nishant   Male    Non-veg  20\n16     Nitya Female    Non-veg   0\n17  Praneeta Female    Non-veg  50\n18 Priyanshu   Male    Non-veg  20\n19     Radha Female    Non-veg  20\n20      Reva Female    Non-veg  20\n21     Rikin   Male    Non-veg  30\n22     Sarah Female    Non-veg   0\n23   Shaivii Female    Non-veg   0\n24    Symran Female    Non-veg  20\n25    Simran Female    Non-veg   0\n26   Sourabh   Male    Non-veg   0\n27    Suhani Female    Non-veg  20\n28    Vedant   Male    Non-veg  20\n29     Vinay   Male    Non-veg   0\n30    Vishnu   Male    Non-veg   0\n\n\nVisualizing the distribution we acquire vs a normal distribution for each group:\n\ntips_modified %&gt;%\n  gf_density( ~ Tip,\n              fill = ~ Preferance,\n              alpha = 0.5,\n              title = \"Tip given\") %&gt;%\n  gf_facet_grid(~ Preferance) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\nThese graphs don’t seem to suggest that these 2 distributions are normally distributed. Does the shapiro test say the same?\n\nFor Vegetarians:\nNull hypothesis: The money given as tip by Vegetarians is normally distributed.\n\nshapiro.test(filtered_Veg$Tip)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic     p.value method                     \n      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n1     0.629 0.000000166 Shapiro-Wilk normality test\n\n\nA high w (statistic) value - one greater than 0.9 indicates that a distribution is normal. With a w value so low : 0.62, we can reject the null hypothesis that it is normally distributed. The p-value suggests the same- it’s less than 0.05 which confirms that the w value we received is statistically significant, it did hot happen by chance.\nTherefore, the distribution of tips given by vegetarians are not normally distributed.\n\n\nFor Non-Vegetarians:\nNull hypothesis: The money given as tip by Non-Vegetarians is normally distributed.\n\nshapiro.test(filtered_Non$Tip)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic    p.value method                     \n      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                      \n1     0.717 0.00000275 Shapiro-Wilk normality test\n\n\nThe conclusions of this Shapiro test is similar to the last- a low w which indicates that the distribution is not normally distributed. Though a little more than the the previous one. We confirm this by assessing the p-value of the test: a very low value confirming that this was not by chance.\nTherefore, the distribution of tips given by Non-vegetarians are not normally distributed.\nAnd so we through out any inferences we pulled from the t test- our assumption has been proven to be false- making it invalid.\n\n\n\nMann-Whitney Test\nThe Mann-Whitney Test is used to compare two independent groups when the data doesn’t follow a normal distribution.\n\nwilcox.test(Tip ~ Preferance, data = tips_modified, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact confidence intervals with ties\n\n\n# A tibble: 1 × 7\n    estimate statistic p.value     conf.low conf.high method         alternative\n       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;      \n1 -0.0000372       437   0.833 -0.000000989 0.0000335 Wilcoxon rank… two.sided  \n\n\nOkay i have a doubt- The test does not directly indicate which group (variable) has higher values. If i look at my skim or even my estimate1 in t-test, Veg is the first group, I also ordered my levels to be Veg first, so that’s how it works here too? Yep, that’s what I’m going with.\n\nThe estimate mean difference is 3.7rs.\nThe p-value is 0.8 (greater than 0.05) suggesting that the estimate result could easily /probably happened by chance, so it’s not considered significant. We therefore, cannot reject the null hypothesis.\n0- which indicates that there is no difference on the average tip, falls within the confidence intervals which suggests the same- we cannot reject the null hypothesis.\n\nI received an error saying “Warning: cannot compute exact p-value with ties Warning: cannot compute exact confidence intervals with ties”. This means that because i have so may repeating values (I’m guessing mainly 0)- there are many values with the same rank and polarity. This could lead to not having a precise p-values and confidence interval result. Therefore, i cannot reject or accept the null hypothesis right away, to make sure, i will go ahead and perform a permutation test to come to a solid inference, one that i cannot question.\n\n\nPermutation Test\nThe permutation test is very flexible and requires minimal assumptions. It doesn’t assume normality or equal variances, making it useful for small or skewed data.\n\nobs_diff_pref &lt;- diffmean(Tip ~ Preferance, \n                            data = tips_modified) \n\nobs_diff_pref\n\n diffmean \n-2.333333 \n\n\nNon-veg mean - Veg mean = -2.3\nIn my sample data, vegetarians, on average, give 2.3rs more tip than non-vegetarians.\nCreating the Null Distribution by Permutation\n\nnull_dist_tips &lt;- \n  do(4999) * diffmean(data = tips_modified, \n                      Tip ~ shuffle(Preferance))\n##null_dist_tips , it appears too long when i render it!\n\nVisualizing this:\n\ngf_histogram(data = null_dist_tips, ~ diffmean, \n             bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_pref, \n           colour = \"darkred\", linewidth = 1,\n           title = \"Null Distribution by Permutation\", \n           subtitle = \"Histogram\") %&gt;% \n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\nAcquiring the p-value:\nThe p-value is calculated by comparing the obs_diff_pref (from the actual data) to the null distribution acquired through permutation. It represents the proportion of times a difference as extreme as the observed one occurs by random chance alone.\n\nprop1(~ diffmean &lt;= obs_diff_pref, data = null_dist_tips)\n\nprop_TRUE \n    0.311 \n\n\nObservations:\n\nThe p-value observed through the permutation test is 0.32. If it would have been a value less than 0.05, it would indicate that that the observed difference is unlikely to have occurred by chance, leading to a rejection of the null hypothesis. But in case of a higher value like 0.3, we cannot reject the null hypothesis: There is no difference in the tipping behavior between vegetarians and non-vegetarians.\nThe visualization of the histogram of mean differences- null distribution along with the observed mean different suggests the same. The observed mean difference is seen around the middle of our null distribution - I can easily mimic nature here - Nothing special is happening - I cannot reject the null hypothesis.\n\n\n\nConclusion:\nIn the assumption that my sample population is a true representation of my population- there appears to be no significant difference in tipping behavior based on dietary preference."
  },
  {
    "objectID": "posts/Experiment2/index.html#descriptive-analysis-based-on-gender",
    "href": "posts/Experiment2/index.html#descriptive-analysis-based-on-gender",
    "title": "Experiment 2",
    "section": "Descriptive Analysis (based on gender):",
    "text": "Descriptive Analysis (based on gender):\n\nVisualizing Tips faceted by Gender\n\ngf_histogram(~Tip, fill = ~Gender, data = tips_modified) %&gt;%\n  gf_labs(title = \"Tips faceted by Gender\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ntips_modified %&gt;%\n  gf_histogram(~Tip|Gender, fill = ~Gender, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Tips faceted by Gender\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1 \n    )\n  ))\n\n\n\n\n\n\n\n\n\ntips_modified %&gt;%\n  gf_density(\n    ~ Tip,\n    fill = ~ Gender,\n    alpha = 0.5,\n    title = \"Money Spent Densities\",\n    subtitle = \"Males vs Females\"\n  )\n\n\n\n\n\n\n\n\nObservations:\n\nIf I ignore the outlier for males at 100, from looking the data, i think females tend to tip more than males.\n\n\n\nCrosstables:\nDoes the summation of the data support my hunch?\n\ncrosstable(Tip ~ Gender, data = tips_modified) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariableGenderFemaleMaleTipMin / Max0 / 50.00 / 100.0Med [IQR]0 [0;20.0]0 [0;20.0]Mean (std)12.2 (16.1)10.2 (19.7)N (NA)30 (0)30 (0)\n\n\nVisualization of median and IQR ranges:\n\ntips_modified %&gt;%\n  gf_boxplot(Gender ~ Tip, fill = ~Gender, alpha=0.5) %&gt;%\n  gf_labs(title = \"Box plot of Tip filled with Gender\")\n\n\n\n\n\n\n\n\nObservations:\n\n50 percent of all tips for both genders lie within the same range 0-20.\nDespite this similarity, the difference in mean by 2 values (which i accept and notice is not too much), the higher value being present for Females, suggest that there are females are more likely to pay more tip than men.\n\n\n\nForming My Hypothesis:\nNull Hypothesis: There is no difference in tipping behavior between males and females.\nAlternative Hypothesis: Females tend give larger values as tips than Males."
  },
  {
    "objectID": "posts/Experiment2/index.html#statistical-analysis-hypothesis-testing-1",
    "href": "posts/Experiment2/index.html#statistical-analysis-hypothesis-testing-1",
    "title": "Experiment 2",
    "section": "Statistical Analysis: Hypothesis Testing",
    "text": "Statistical Analysis: Hypothesis Testing\n\nA t-test:\nAssumptions: 1. The distribution for each group - male and female is normal.\n\nmosaic::t_test(Tip ~ Gender, data = tips_modified)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1        2      12.2      10.2     0.431   0.668      55.8    -7.29      11.3\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObservations:\n\nThe estimate suggests that females, on average, give 2rs more tip than males in our sample data set.\nThe high p-value of 0.66, greater than the typical significance level of 0.05, suggests that there is no statistically significant difference in the average tip amounts between males and females.The result could just be random, so I don’t have strong evidence that something is happening.\n0- which indicates that there is no difference on the average tip, falls within the confidence interval (-7.2 to 11.9) which suggest the same.\nTherefore we cannot reject the null hypothesis.\n\nBut what if my assumption of the tips being distributed normally is not accurate?\nFirst we transform our data set to be 2 different ones- one with all the Male values and the other with all the Female values.\n\nfiltered_Male &lt;- tips_modified %&gt;%\n  filter(Gender == \"Male\")\nfiltered_Female &lt;- tips_modified %&gt;%\n  filter(Gender == \"Female\")\nfiltered_Male\n\n        Name Gender Preferance Tip\n1       Adit   Male        Veg   0\n2      Akash   Male    Non-veg   0\n3      Ankit   Male    Non-veg   0\n4      Anmol   Male        Veg   0\n5       Anna   Male        Veg   0\n6      Arnav   Male    Non-veg   0\n7   Ashutosh   Male    Non-veg   0\n8       Ayan   Male        Veg   0\n9       Debu   Male        Veg  15\n10   Dheeman   Male        Veg 100\n11   Hardik    Male    Non-veg   0\n12  Ignatius   Male    Non-veg  20\n13  Mahendra   Male    Non-veg  20\n14    Manish   Male        Veg   0\n15    Nevaan   Male    Non-veg   0\n16   Nishant   Male    Non-veg  20\n17      Ojas   Male        Veg  20\n18 Priyanshu   Male    Non-veg  20\n19     Rikin   Male    Non-veg  30\n20  Shashwat   Male        Veg   0\n21     Shiva   Male        Veg   0\n22   Sourabh   Male    Non-veg   0\n23    Srujan   Male        Veg   0\n24    Suhaas   Male        Veg  20\n25     Taran   Male        Veg  20\n26     Varad   Male        Veg   0\n27    Vedant   Male    Non-veg  20\n28     Vinay   Male    Non-veg   0\n29   Vishesh   Male        Veg   0\n30    Vishnu   Male    Non-veg   0\n\nfiltered_Female\n\n       Name Gender Preferance Tip\n1     Aanya Female        Veg   0\n2     Aditi Female        Veg  20\n3   Akshita Female    Non-veg   0\n4  Anandita Female    Non-veg   0\n5    Ananya Female    Non-veg  20\n6     Anaya Female        Veg  35\n7    Anhuya Female        Veg  40\n8  Anoushka Female    Non-veg   0\n9    Arushi Female    Non-veg   0\n10    Asark Female    Non-veg  20\n11     Diya Female        Veg  30\n12   Khushi Female        Veg   0\n13  Kshraja Female        Veg   0\n14    Mahie Female        Veg   0\n15 Nanditha Female        Veg   0\n16    Nidhi Female        Veg   0\n17    Nitya Female    Non-veg   0\n18 Praneeta Female    Non-veg  50\n19    Radha Female    Non-veg  20\n20     Reva Female    Non-veg  20\n21   Sadnya Female        Veg   0\n22  Sanjana Female        Veg   0\n23    Sarah Female    Non-veg   0\n24  Shaivii Female    Non-veg   0\n25   Shreya Female        Veg   0\n26   Symran Female    Non-veg  20\n27   Simran Female    Non-veg   0\n28   Suhani Female    Non-veg  20\n29 Sushmita Female        Veg  20\n30   Zivnya Female        Veg  50\n\n\nVisualizing the distribution we acquire vs a normal distribution for each group:\n\ntips_modified %&gt;%\n  gf_density( ~ Tip,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"Tip given\") %&gt;%\n  gf_facet_grid(~ Gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\nThe data does not seem to be normally distributed. Let’s confirm!\n\n\nA Shapiro test:\nShapiro test is one used to determine if a sample comes from a normally distributed population or not.\n\nFor Males:\nNull hypothesis: The money given as tip by Males is normally distributed.\n\nshapiro.test(filtered_Male$Tip)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic      p.value method                     \n      &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                      \n1     0.532 0.0000000119 Shapiro-Wilk normality test\n\n\nWith a w value so low : 0.5 (&lt;0.9), we can reject the null hypothesis that it is normally distributed. The p-value suggests the same- it’s less than 0.05 which confirms that the w value we received is statistically significant, it did not happen by chance.\nTips given by males are not normally distributed.\n\n\nFor Females:\nNull hypothesis: The money given as tip by Females is normally distributed.\n\nshapiro.test(filtered_Female$Tip)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic    p.value method                     \n      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                      \n1     0.748 0.00000832 Shapiro-Wilk normality test\n\n\nWith a w value so low : 0.7 (&lt;0.9), we can reject the null hypothesis that it is normally distributed. The p-value suggests the same- it’s less than 0.05 which confirms that the w value we received is statistically significant, it did not happen by chance.\nTips given by females are not normally distributed.\n\n\n\nMann-Whitney Test\nSince we proved the t-test p-value to be invalid, we can use the Mann-Whitney test to pull inferences. It is used to compare two independent groups when the data doesn’t follow a normal distribution.\n\nwilcox.test(Tip ~ Gender, data = tips_modified, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact confidence intervals with ties\n\n\n# A tibble: 1 × 7\n   estimate statistic p.value   conf.low  conf.high method           alternative\n      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;      \n1 0.0000445       498   0.422 -0.0000134 0.00000497 Wilcoxon rank s… two.sided  \n\n\n\nThe estimate suggests that females, on average, give 4.446004e-05rs more tip than males.\nThe p-value is 0.4(greater than 0.05) suggesting that the estimate result could easily /probably happened by chance, so it’s not considered significant. We therefore, cannot reject the null hypothesis.\n0- which indicates that there is no difference on the average tip, falls within the confidence intervals which suggests the same- we cannot reject the null hypothesis.\n\nI received an error saying “Warning: cannot compute exact p-value with tiesWarning: cannot compute exact confidence intervals with ties”. This means that because i have so may repeating values (I’m guessing mainly 0)- there are many values with the same rank and polarity. This could lead to not having a precise p-values and confidence interval result. Therefore, i cannot reject or accept the null hypothesis right away, to make sure, i will go ahead and perform a permutation test to come to a solid inference, one that i cannot question.\n\n\nPermutation Test\nThe permutation test is very flexible and requires minimal assumptions. It doesn’t assume normality or equal variances, making it useful for small or skewed data.\n\nobs_diff_gender &lt;- diffmean(Tip ~ Gender, \n                            data = tips_modified) \n\nobs_diff_gender\n\ndiffmean \n      -2 \n\n\nMale mean - Female mean = -2\nIn my sample data, females, on average, give 2rs more tip than males.\nCreating the Null Distribution by Permutation\n\nnull_dist_tips2 &lt;- \n  do(4999) * diffmean(data = tips_modified, \n                      Tip ~ shuffle(Gender))\n##null_dist_tips2 , it appears too long when i render it!\n\nVisualizing this:\n\ngf_histogram(data = null_dist_tips2, ~ diffmean, \n             bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, \n           colour = \"darkred\", linewidth = 1,\n           title = \"Null Distribution by Permutation\", \n           subtitle = \"Histogram\") %&gt;% \n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\nAcquiring the p-value:\nThe p-value is calculated by comparing the obs_diff_gender (from the actual data) to the null distribution acquired through permutation. It represents the proportion of times a difference as extreme as the observed one occurs by random chance alone.\n\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_tips2)\n\nprop_TRUE \n   0.3574 \n\n\nObservations:\n\nThe p-value observed through the permutation test is 0.35. If it would have been a value less than 0.05, it would indicate that that the observed difference is unlikely to have occurred by chance, leading to a rejection of the null hypothesis. But in case of a higher value like 0.3, we cannot reject the null hypothesis: There is no difference in the tipping behavior between male and female students .\nThe visualization of the histogram of mean differences- null distribution along with the observed mean different suggests the same. The observed mean difference is observed around the middle of our null distribution - I can easily mimic nature here - Nothing special is happening - I cannot reject the null hypothesis.\n\n\n\nConclusion:\nIn the assumption that my sample population is a true representation of my population-there appears to be no significant difference in tipping behavior based on gender."
  },
  {
    "objectID": "posts/Experiment3/index.html#desicriptive-analysis",
    "href": "posts/Experiment3/index.html#desicriptive-analysis",
    "title": "Experiment 3",
    "section": "Desicriptive analysis",
    "text": "Desicriptive analysis\n\nOf courses (departments) within B.FA. We can try and find if there is a significant difference in grades received by students persuing a B.FA degree based on their course. Within B.FA, is there a course receiving more or less grades than others and is this difference statistically significant- is it something that happend by chance or not.\n\n\nFiltering the data:\n\nfiltered_FA &lt;- grades_modified %&gt;%\n  filter(Degree == \"B.FA\")\nfiltered_FA \n\n   Degree Course Year Gender Score\n1    B.FA    CAP    3      F   8.0\n2    B.FA    CAP    3      M   9.0\n3    B.FA    CAP    3      F   9.0\n4    B.FA    DMA    2      M   8.5\n5    B.FA    DMA    2      M   8.0\n6    B.FA    DMA    2      F   8.0\n7    B.FA    CAP    2      F   8.0\n8    B.FA    CAP    2      F   8.0\n9    B.FA   Film    3      M   6.0\n10   B.FA    DMA    2      F   8.0\n11   B.FA    DMA    2      F   7.0\n12   B.FA    DMA    2      F   7.0\n13   B.FA    DMA    2      F   8.0\n14   B.FA    DMA    2      F   7.0\n15   B.FA    DMA    2      M   6.0\n16   B.FA   Film    3      M   8.0\n17   B.FA   Film    3      M   8.0\n18   B.FA    DMA    2      M   8.0\n19   B.FA    DMA    2      M   8.0\n20   B.FA    DMA    2      F   7.0\n21   B.FA    DMA    2      F   7.0\n22   B.FA   Film    2      F   8.0\n23   B.FA   Film    3      F   7.0\n24   B.FA    DMA    2      F   9.0\n25   B.FA    DMA    2      M   8.0\n26   B.FA    DMA    2      F   7.0\n27   B.FA    DMA    2      F   7.0\n28   B.FA    DMA    2      M   7.0\n29   B.FA    DMA    2      F   7.0\n30   B.FA    DMA    2      F   7.0\n\n\n\nglimpse(filtered_FA)\n\nRows: 30\nColumns: 5\n$ Degree &lt;fct&gt; B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.F…\n$ Course &lt;fct&gt; CAP, CAP, CAP, DMA, DMA, DMA, CAP, CAP, Film, DMA, DMA, DMA, DM…\n$ Year   &lt;ord&gt; 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, …\n$ Gender &lt;fct&gt; F, M, F, M, M, F, F, F, M, F, F, F, F, F, M, M, M, M, M, F, F, …\n$ Score  &lt;dbl&gt; 8.0, 9.0, 9.0, 8.5, 8.0, 8.0, 8.0, 8.0, 6.0, 8.0, 7.0, 7.0, 8.0…\n\n\n\nskim(filtered_FA)\n\n\nData summary\n\n\nName\nfiltered_FA\n\n\nNumber of rows\n30\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDegree\n0\n1\nFALSE\n1\nB.F: 30, B.D: 0, B.V: 0\n\n\nCourse\n0\n1\nFALSE\n3\nDMA: 20, Fil: 5, CAP: 5, CAC: 0\n\n\nYear\n0\n1\nTRUE\n2\n2: 23, 3: 7, 1: 0\n\n\nGender\n0\n1\nFALSE\n2\nF: 19, M: 11\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nScore\n0\n1\n7.62\n0.78\n6\n7\n8\n8\n9\n▁▇▁▇▂\n\n\n\n\n\nThere are 3 courses: DMA, Film and CAP.\nWhat is the count of each of these courses? We will not be able to do an accurate analysis of the data id there is too little or too large of data for any Course.\n\nfiltered_FA %&gt;%\n  group_by(Course) %&gt;%\n  summarize(count =n())\n\n# A tibble: 3 × 2\n  Course count\n  &lt;fct&gt;  &lt;int&gt;\n1 Film       5\n2 CAP        5\n3 DMA       20\n\n\nOkay not possible. There are too little DMA and CAP students - 5 each and too many DMA students in the data set. ut just to be qurious what is the distribution of each?\n\nfiltered_FA %&gt;%\n  gf_density(\n    ~ Score,\n    fill = ~ Course,\n    alpha = 0.5,\n    title = \"Grades Densities by Courses within B.FA\",\n    subtitle = \"Film vs CAP us DMA\"\n  )\n\n\n\n\n\n\n\n\nThat looks so cool! At a fist glance, it seems like among the 3, if this data was good enough, CAP students tend to get more grades than the other 2 while film students tend to get the least. I could be completely wrong here."
  },
  {
    "objectID": "posts/Experiment1/index.html#descriptive-analysis",
    "href": "posts/Experiment1/index.html#descriptive-analysis",
    "title": "Experiment 1",
    "section": "Descriptive Analysis:",
    "text": "Descriptive Analysis:\n\nskim(pockets_modified)\n\n\nData summary\n\n\nName\npockets_modified\n\n\nNumber of rows\n82\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1\n3\n12\n0\n82\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n2\nMal: 41, Fem: 41\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nMoney_spent\n0\n1\n720.96\n1835.72\n0\n100\n264.5\n596.25\n13000\n▇▁▁▁▁\n\n\n\n\n\nObservations:\n\nThere are 41 data entries each for males and females.\nThe lowest money spent on 23rd October is 0 while the highest is 13,000.\nThe mean amount spent is 720.96\n\n\nVisualising Money Spent:\nPlotting a histogram to see a distribution of the money spent:\n\npockets_modified %&gt;%\n  gf_histogram(~Money_spent)\n\n\n\n\n\n\n\n\nPlotting a histogram to see a distribution of the money spent faceted by gender:\n\ngf_histogram(~Money_spent, fill = ~Gender, data = pockets_modified, bins=100) %&gt;%\n  gf_labs(title = \"Money spent faceted by gender\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\npockets_modified %&gt;%\n  gf_histogram(~Money_spent|Gender, fill = ~Gender, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Money spent faceted by gender\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45, ## the angle at which the word should be placed.\n      hjust = 1 ## the incremanting space from the x axis\n    )\n  ))\n\n\n\n\n\n\n\n\n\npockets_modified %&gt;%\n  gf_density(\n    ~ Money_spent,\n    fill = ~ Gender,\n    alpha = 0.5,\n    title = \"Money Spent Densities\",\n    subtitle = \"Boys vs Girls\"\n  )\n\n\n\n\n\n\n\n\nObservations: It seems like the distribution of money spent are similar for boys and girls. Both distributions show a heavy concentration toward the lower end, suggesting that most students spent small amounts of money, regardless of gender. However, there are some cases with much higher expenditures on both sides.\n\n\nSummerising the data:\nTrying to find the difference in means and medians of money spent based on the difference in gender within the sample population:\n\ncrosstable(Money_spent ~ Gender, data = pockets_modified) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariableGenderMaleFemaleMoney_spentMin / Max0 / 1.0e+040 / 1.3e+04Med [IQR]250.0 [150.0;842.0]280.0 [85.0;500.0]Mean (std)748.6 (1636.5)693.3 (2035.8)N (NA)41 (0)41 (0)\n\n\nI can visualize the difference in medians and IQRs with a box plot:\n\npockets_modified %&gt;%\n  gf_boxplot(Gender ~ Money_spent, fill = ~Gender) %&gt;%\n  gf_labs(title = \"Spendinf filled by Gender\")\n\n\n\n\n\n\n\n\nTo understand it better, i take scale the x-axis to log(10):\n\npockets_modified %&gt;%\n  gf_boxplot(Gender ~ Money_spent, fill = ~Gender) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(title = \"Spending filled by Gender\")\n\nWarning in scale_x_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nObservations (regarding mean and sd):\n1. The mean amount spent by males- 748.6 , is slightly higher than that of females- 693.3.\n2. The standard deviation is much larger for females- 2035.8, than for males- 1636.5, indicating more variability in spending among females (or there may be more extreme spending outliers in the female group?).\n3. The median spending is slightly higher for females at 280 compared to that of males which is 250.\n4. The interquartile range (IQR) for males is broader- 150.0 to 842.0 compared to that for females- 85.0 to 500.0, suggesting a wider spread in spending behavior for males.\n5. On descriptive analysis of the data, it appears so that there is no difference in the money spent by both groups- Both show very similar patterns in spending behavior, but if there in fact is a difference, the whiskers in the box plot being longer of males gives me a hunch that males spent more money than females.\n\n\nDefining my Hypothesis:\nNull Hypothesis: There is no difference in the money spent by males and females which indicates that they receive more pocket money\nAlternative Hypothesis: Males tend to spend more money than women which indicates that they receive more pocket money compared to women.\nSince the IQR ranges of both males and females overlap so much, we can only come to a conclusion of our hypothesis with statistical analysis. The first step of it being a t-test."
  },
  {
    "objectID": "posts/Experiment1/index.html#staistical-analysis-hypothesis-testing",
    "href": "posts/Experiment1/index.html#staistical-analysis-hypothesis-testing",
    "title": "Experiment 1",
    "section": "Staistical Analysis: Hypothesis testing",
    "text": "Staistical Analysis: Hypothesis testing\n\nA t-test:\nAssumptions: 1. The distribution for each group - male and female is normal.\n\nmosaic::t_test(Money_spent ~ Gender, data = pockets_modified)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     55.3      749.      693.     0.136   0.893      76.5    -757.      868.\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObservations:\n\nThe estimate suggests that male students, on average, spent 55.29 more than female students.89\nSince the p-value value is as high as 0.8, we fail to reject the null hypothesis. This suggests that there is no statistically significant difference between the spending of male and female students.The confidence interval of -757.07 and 867.66 suggest the same since it straddles 0. Therefore, our null hypothesis that gender does not play a role in the amount of money spent on a random day by a college student is true.\n\nBut is this t-test reliable?.. only if the the money spent by the sample is normally distributed. We test that using a Shapiro test.\n\n\nA shapiro test:\n\nshapiro.test(pockets_modified$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.345 2.34e-17 Shapiro-Wilk normality test\n\n\nWe need to know if the individual distribution of both genders are normally distributed. And so we transform our data set to be 2- one with all the male values and one with all the female values.\nAcquiring the 2 data sets:\n\nfiltered_male &lt;- pockets_modified %&gt;%\n  filter(Gender == \"Male\")\nfiltered_female &lt;- pockets_modified %&gt;%\n  filter(Gender == \"Female\")\nfiltered_male\n\n        Name Gender Money_spent\n1      Aagam   Male         150\n2     Aakash   Male         240\n3    Adithya   Male          68\n4     Aditya   Male         300\n5      Anish   Male           0\n6     Ankush   Male         250\n7      Arjun   Male        1143\n8       Arya   Male         100\n9      Aryan   Male       10000\n10      Aziz   Male        1433\n11       Dan   Male         600\n12 Deborishi   Male         205\n13    Dhiman   Male         910\n14     Dhruv   Male         900\n15     Dhurv   Male         185\n16   Ezhilan   Male         842\n17      Geet   Male          50\n18      Jeff   Male         200\n19   Jeffrey   Male         150\n20    Kartik   Male         145\n21   Koustav   Male         250\n22     Krish   Male          70\n23    Lekith   Male         100\n24    Maahin   Male         310\n25    Madhav   Male        4000\n26     Manav   Male          40\n27    Nivaan   Male         220\n28     Rikit   Male         259\n29     Ritik   Male        1000\n30    Rudraj   Male         478\n31      Ryan   Male         330\n32  Shashank   Male         400\n33  Shashwat   Male         149\n34     Shiva   Male         250\n35    Suhaas   Male         997\n36     Sutej   Male           0\n37     Taran   Male        1399\n38  Tathastu   Male        1535\n39     Varad   Male         154\n40      Veer   Male         566\n41     Viraj   Male         315\n\nfiltered_female\n\n           Name Gender Money_spent\n1       Aarushi Female         382\n2       Abheeta Female          60\n3      Akanksha Female         270\n4        Amruta Female         190\n5        Anaaya Female         300\n6       Anousha Female          85\n7      Anoushka Female         700\n8       Anushka Female         140\n9          Asra Female        1070\n10      Bhumika Female        1200\n11        Daana Female          66\n12     Devanshi Female         700\n13        Eisha Female         300\n14       Harjot Female        3000\n15      Janhavi Female         150\n16      Kalyani Female           0\n17       Kashvi Female         430\n18       Kavana Female         500\n19       Khushi Female         785\n20     Kshirija Female         100\n21       Maanya Female          15\n22      Nandana Female           0\n23        Navya Female          55\n24    Nayantara Female         192\n25        Neeti Female         280\n26       Nithya Female          85\n27       Parisa Female          80\n28        Risha Female           0\n29      Rukaiya Female          20\n30       Shaivi Female         318\n31       Shreya Female         585\n32       Simran Female         340\n33 Simran Anand Female         660\n34      Snigdha Female         100\n35      Suprita Female         500\n36       Tanmay Female         800\n37        Tanya Female           0\n38       Tarini Female         200\n39        Vanya Female         350\n40     Vasantha Female         418\n41   Vasundhara Female       13000\n\n\nVisualizing the distribution we acquire vs a normal distribution:\n\npockets_modified %&gt;%\n  gf_density( ~ Money_spent,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"Money spent on 23rd Oct\") %&gt;%\n  gf_facet_grid(~ Gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\n\nConducting the Shapiro test for each of these distributions:\nFor males:\nNull hypothesis: The money spent by males is normally distributed.\n\nshapiro.test(filtered_male$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.407 1.14e-11 Shapiro-Wilk normality test\n\n\nThe statistic indicates how closely the sample data follows a normal distribution (the closer the value is the 1, the more normal the distribution is) and so the computed value of 0.40 says that the distribution is not normal.\nA p value so low- 1.135947e-11 indicates that we must reject the null hypothesis.Therefore, the data is not normally distributed.\nFor females:\nNull hypothesis: The money spent by females is normally distributed.\n\nshapiro.test(filtered_female$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.296 8.96e-13 Shapiro-Wilk normality test\n\n\nSimilar to the inference for the distribution for males, even in females, we can reject the null hypothesis of the distribution being normal.\nWe go ahead and conduct a Mann-Whitney Test having proven that both distribution aren’t normal, proving the t-test to be invalid.\n\n\n\nMann-Whitney Test\n\nwilcox.test(Money_spent ~ Gender, data = pockets_modified, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact confidence intervals with ties\n\n\n# A tibble: 1 × 7\n  estimate statistic p.value conf.low conf.high method               alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;      \n1     55.0      936.   0.381    -70.0      180. Wilcoxon rank sum t… two.sided  \n\n\nObservations: The estimate of the difference between both groups is 55.00 a high p-value (&gt;0.05) of 0.38 suggests that we cannot reject the null hypothesis. There is statistically no significant difference between the two groups’ median values. 0 falls within the confidence interval which further suggest the same.\nI received an error saying “Warning: cannot compute exact p-value with ties”. This means that because i have so may repeating values (I’m guessing mainly 0)- there are many values with the same rank and polarity. This could lead to not having precise p-values.\nTherefore, i cannot reject the null hypothesis right away. To make sure, i will go ahead and perform a permutation test to come to a solid inference, one that i cannot question.\n\n\nPermutation Test\n\nobs_diff_gender &lt;- diffmean(Money_spent ~ Gender, \n                            data = pockets_modified) \n\nobs_diff_gender\n\n diffmean \n-55.29268 \n\n\nHere we begin by creating a null distribution for the difference in means by performing 4,999 permutations- each engraving and different values among the 2 gender groups. We do this by randomly reassigning the ‘Gender’ variable across all observations\n\nnull_dist_money &lt;- \n  do(4999) * diffmean(data = pockets_modified, \n                      Money_spent ~ shuffle(Gender))\n##null_dist_money , it appears too long when i render it!\n\nVisualization of the observed mean relative to the null distribution:\n\ngf_histogram(data = null_dist_money, ~ diffmean, \n             bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, \n           colour = \"darkred\", linewidth = 1,\n           title = \"Null Distribution by Permutation\", \n           subtitle = \"Histogram\") %&gt;% \n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\nThe p-value obtained through the permutation test:\nprop1 calculates the p-value by comparing the observed difference in means (obs_diff_gender) with the null distribution.\n\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_money)\n\nprop_TRUE \n   0.4408 \n\n\nObservations:\n\nThe p-value observed through the permutation test is 0.41. If it would have been a value less than 0.05, it would indicate that that the observed difference is unlikely to have occurred by chance, leading to a rejection of the null hypothesis. But in this case the higher value i.e. 0.41, we cannot reject the null hypothesis: There is no difference in the amount of pocket money received by male and female students .\nThe visualization of the histogram of mean differences- null distribution along with the observed mean different suggests the same - I cannot reject the null hypothesis.\n\n\nConclusion:\nIn the assumption that my sample population is a true representation of my population- There is no difference in the amount spent by male and female students on a random day- and so there must be no difference in the pocket money received by both genders."
  },
  {
    "objectID": "posts/Experiment4/index.html",
    "href": "posts/Experiment4/index.html",
    "title": "Experiment 4",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\nlibrary(infer) \n\n\nAttaching package: 'infer'\n\nThe following objects are masked from 'package:mosaic':\n\n    prop_test, t_test\n\nlibrary(patchwork) \nlibrary(ggprism)\nlibrary(supernova) \nlibrary(stringr)"
  },
  {
    "objectID": "posts/Experiment4/index.html#defining-the-research-experiment",
    "href": "posts/Experiment4/index.html#defining-the-research-experiment",
    "title": "Experiment 4",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nGoal of the Experiment:\nThe purpose of this research experiment is to understand the nostalgic value of certain childhood cartoons among young adults who have “aged out” of the target demographic. By evaluating three popular cartoons (Doraemon, Chhota Bheem, and Dragon Tales), the experiment aims to identify which show holds the most cherished place in the hearts of these individuals, as reflected by their ratings on a scale of 10.\n\nMethodology\nSampling: The population consists of all people between the ages of 19-22. The sample population is a randomly choosen group of students in a college campus ensuring a hopefully balanced set of participants that is a true representation of the population. Each participant was asked to rate one of the cartoon in a scale of 10. Every participant was not asked to rate all 3 to remove any bias regarding each other- so that their opinion is not clouded by their opinion of another- which would make it a\nMethod Followed- To ensure a lack of bias, a coin was tossed to even determine of data would be collected from a person or not. This lead to random sampling ensure diversity in age and socioeconomic background.\nSample Size: We had a total of 90 participants 30 Chota Bheem and 30 Doremon and 30 Dragon Tales.\nWhy could knowing this be useful?\n\nCompanies in media and entertainment can use the data to gauge the staying power of their content, particularly if they are considering reboots, merchandise, or spin-offs aimed at older audiences\nThe experiment can offer insights into how nostalgia shapes media preferences and emotional connections to shows viewed in childhood, contributing to studies in psychology and media studies.\n\n\n\nReading and Analyzing the data:\n\ncartoon_data &lt;- read.csv(\"../../data/cartoons.csv\")\ncartoon_data\n\n   Participant.ID Gender      Cartoon Rating\n1              P1   Male  Chota Bheem    8.5\n2              P2   Male  Chota Bheem    6.0\n3              P3   Male  Chota Bheem    8.0\n4              P4   Male  Chota Bheem    7.0\n5              P5   Male  Chota Bheem    8.0\n6              P6   Male  Chota Bheem   10.0\n7              P7   Male  Chota Bheem    5.0\n8              P8   Male  Chota Bheem    7.8\n9              P9   Male  Chota Bheem    8.5\n10            P10   Male  Chota Bheem    5.0\n11            P11   Male  Chota Bheem    7.0\n12            P12   Male  Chota Bheem    6.0\n13            P13   Male  Chota Bheem    6.0\n14            P14   Male  Chota Bheem    6.0\n15            P15   Male  Chota Bheem    6.0\n16            P16 Female  Chota Bheem    8.0\n17            P17 Female  Chota Bheem    6.8\n18            P18 Female  Chota Bheem    4.0\n19            P19 Female  Chota Bheem    7.5\n20            P20 Female  Chota Bheem    7.0\n21            P21 Female  Chota Bheem    6.0\n22            P22 Female  Chota Bheem    6.0\n23            P23 Female  Chota Bheem    8.0\n24            P24 Female  Chota Bheem    6.0\n25            P25 Female  Chota Bheem    6.0\n26            P26 Female  Chota Bheem    8.0\n27            P27 Female  Chota Bheem    8.0\n28            P28 Female  Chota Bheem    6.0\n29            P29 Female  Chota Bheem    5.0\n30            P30 Female  Chota Bheem    3.0\n31            P31 Female     Doraemon    8.0\n32            P32 Female     Doraemon    8.0\n33            P33 Female     Doraemon    9.0\n34            P34 Female     Doraemon    7.0\n35            P35 Female     Doraemon    5.0\n36            P36 Female     Doraemon   10.0\n37            P37 Female     Doraemon    9.0\n38            P38 Female     Doraemon    4.0\n39            P39 Female     Doraemon    6.0\n40            P40 Female     Doraemon    6.0\n41            P41 Female     Doraemon   10.0\n42            P42 Female     Doraemon    6.0\n43            P43 Female     Doraemon    7.0\n44            P44 Female     Doraemon    8.0\n45            P45 Female     Doraemon    6.0\n46            P46   Male     Doraemon    8.0\n47            P47   Male     Doraemon    8.0\n48            P48   Male     Doraemon    7.0\n49            P49   Male     Doraemon    8.0\n50            P50   Male     Doraemon   10.0\n51            P51   Male     Doraemon    6.5\n52            P52   Male     Doraemon    9.0\n53            P53   Male     Doraemon    2.0\n54            P54   Male     Doraemon    5.0\n55            P55   Male     Doraemon    6.0\n56            P56   Male     Doraemon   10.0\n57            P57   Male     Doraemon    9.0\n58            P58   Male     Doraemon    9.0\n59            P59   Male     Doraemon    1.0\n60            P60   Male     Doraemon   10.0\n61            P61   Male Dragon Tales   10.0\n62            P62   Male Dragon Tales    4.0\n63            P63   Male Dragon Tales    7.0\n64            P64   Male Dragon Tales   10.0\n65            P65   Male Dragon Tales    7.0\n66            P66   Male Dragon Tales    5.0\n67            P67   Male Dragon Tales    9.0\n68            P68   Male Dragon Tales    8.5\n69            P69   Male Dragon Tales    7.0\n70            P70   Male Dragon Tales    7.0\n71            P71   Male Dragon Tales    6.0\n72            P72   Male Dragon Tales    5.0\n73            P73   Male Dragon Tales    1.0\n74            P74   Male Dragon Tales    7.0\n75            P75   Male Dragon Tales    6.0\n76            P76 Female Dragon Tales    8.0\n77            P77 Female Dragon Tales    9.0\n78            P78 Female Dragon Tales   10.0\n79            P79 Female Dragon Tales    8.0\n80            P80 Female Dragon Tales    9.0\n81            P81 Female Dragon Tales    8.0\n82            P82 Female Dragon Tales   10.0\n83            P83 Female Dragon Tales    6.0\n84            P84 Female Dragon Tales    8.0\n85            P85 Female Dragon Tales    8.0\n86            P86 Female Dragon Tales    6.5\n87            P87 Female Dragon Tales    7.0\n88            P88 Female Dragon Tales    8.0\n89            P89 Female Dragon Tales    7.0\n90            P90 Female Dragon Tales    6.0\n\n\n\nglimpse(cartoon_data)\n\nRows: 90\nColumns: 4\n$ Participant.ID &lt;chr&gt; \"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"P7\", \"P8\", \"P9\", \"…\n$ Gender         &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\",…\n$ Cartoon        &lt;chr&gt; \"Chota Bheem\", \"Chota Bheem\", \"Chota Bheem\", \"Chota Bhe…\n$ Rating         &lt;dbl&gt; 8.5, 6.0, 8.0, 7.0, 8.0, 10.0, 5.0, 7.8, 8.5, 5.0, 7.0,…\n\n\n\nskim(cartoon_data)\n\n\nData summary\n\n\nName\ncartoon_data\n\n\nNumber of rows\n90\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nParticipant.ID\n0\n1\n2\n3\n0\n90\n0\n\n\nGender\n0\n1\n4\n6\n0\n2\n0\n\n\nCartoon\n0\n1\n8\n12\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nRating\n0\n1\n7.06\n1.94\n1\n6\n7\n8\n10\n▁▁▆▇▅\n\n\n\n\n\n\n\nDefining the Data Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nParticipant.ID\na unique identifier for each participant in the study\nQualitative\n\n\nGender\nThis variable records the gender of each participant, typically represented as categorical values\nQualitative\n\n\nCartoon\nIndicates the specific cartoon each participant rated.\nQualitative\n\n\nRating\nRepresents the participant’s rating of the cartoon on a scale of 10. This is an Opinion score.\nQuantitative\n\n\n\nObservations:\n\nParticipant.ID is not a required column- it does not play any role in the what the data says.\nThere are no missing values in the data set.\nGender and Cartoon need to be transformed into factors.\n\n\n\nTransforming the data:\n\ncartoons_modified &lt;- cartoon_data %&gt;%\n  dplyr::mutate(\n    Gender = as_factor(Gender),\n    Cartoon = as_factor(Cartoon)\n    )%&gt;%\n  select(Cartoon , Gender, Rating)\ncartoons_modified\n\n        Cartoon Gender Rating\n1   Chota Bheem   Male    8.5\n2   Chota Bheem   Male    6.0\n3   Chota Bheem   Male    8.0\n4   Chota Bheem   Male    7.0\n5   Chota Bheem   Male    8.0\n6   Chota Bheem   Male   10.0\n7   Chota Bheem   Male    5.0\n8   Chota Bheem   Male    7.8\n9   Chota Bheem   Male    8.5\n10  Chota Bheem   Male    5.0\n11  Chota Bheem   Male    7.0\n12  Chota Bheem   Male    6.0\n13  Chota Bheem   Male    6.0\n14  Chota Bheem   Male    6.0\n15  Chota Bheem   Male    6.0\n16  Chota Bheem Female    8.0\n17  Chota Bheem Female    6.8\n18  Chota Bheem Female    4.0\n19  Chota Bheem Female    7.5\n20  Chota Bheem Female    7.0\n21  Chota Bheem Female    6.0\n22  Chota Bheem Female    6.0\n23  Chota Bheem Female    8.0\n24  Chota Bheem Female    6.0\n25  Chota Bheem Female    6.0\n26  Chota Bheem Female    8.0\n27  Chota Bheem Female    8.0\n28  Chota Bheem Female    6.0\n29  Chota Bheem Female    5.0\n30  Chota Bheem Female    3.0\n31     Doraemon Female    8.0\n32     Doraemon Female    8.0\n33     Doraemon Female    9.0\n34     Doraemon Female    7.0\n35     Doraemon Female    5.0\n36     Doraemon Female   10.0\n37     Doraemon Female    9.0\n38     Doraemon Female    4.0\n39     Doraemon Female    6.0\n40     Doraemon Female    6.0\n41     Doraemon Female   10.0\n42     Doraemon Female    6.0\n43     Doraemon Female    7.0\n44     Doraemon Female    8.0\n45     Doraemon Female    6.0\n46     Doraemon   Male    8.0\n47     Doraemon   Male    8.0\n48     Doraemon   Male    7.0\n49     Doraemon   Male    8.0\n50     Doraemon   Male   10.0\n51     Doraemon   Male    6.5\n52     Doraemon   Male    9.0\n53     Doraemon   Male    2.0\n54     Doraemon   Male    5.0\n55     Doraemon   Male    6.0\n56     Doraemon   Male   10.0\n57     Doraemon   Male    9.0\n58     Doraemon   Male    9.0\n59     Doraemon   Male    1.0\n60     Doraemon   Male   10.0\n61 Dragon Tales   Male   10.0\n62 Dragon Tales   Male    4.0\n63 Dragon Tales   Male    7.0\n64 Dragon Tales   Male   10.0\n65 Dragon Tales   Male    7.0\n66 Dragon Tales   Male    5.0\n67 Dragon Tales   Male    9.0\n68 Dragon Tales   Male    8.5\n69 Dragon Tales   Male    7.0\n70 Dragon Tales   Male    7.0\n71 Dragon Tales   Male    6.0\n72 Dragon Tales   Male    5.0\n73 Dragon Tales   Male    1.0\n74 Dragon Tales   Male    7.0\n75 Dragon Tales   Male    6.0\n76 Dragon Tales Female    8.0\n77 Dragon Tales Female    9.0\n78 Dragon Tales Female   10.0\n79 Dragon Tales Female    8.0\n80 Dragon Tales Female    9.0\n81 Dragon Tales Female    8.0\n82 Dragon Tales Female   10.0\n83 Dragon Tales Female    6.0\n84 Dragon Tales Female    8.0\n85 Dragon Tales Female    8.0\n86 Dragon Tales Female    6.5\n87 Dragon Tales Female    7.0\n88 Dragon Tales Female    8.0\n89 Dragon Tales Female    7.0\n90 Dragon Tales Female    6.0"
  },
  {
    "objectID": "posts/Experiment4/index.html#desciptive-analysis",
    "href": "posts/Experiment4/index.html#desciptive-analysis",
    "title": "Experiment 4",
    "section": "Desciptive Analysis:",
    "text": "Desciptive Analysis:\n\nglimpse(cartoons_modified)\n\nRows: 90\nColumns: 3\n$ Cartoon &lt;fct&gt; Chota Bheem, Chota Bheem, Chota Bheem, Chota Bheem, Chota Bhee…\n$ Gender  &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, Ma…\n$ Rating  &lt;dbl&gt; 8.5, 6.0, 8.0, 7.0, 8.0, 10.0, 5.0, 7.8, 8.5, 5.0, 7.0, 6.0, 6…\n\n\n\nskim(cartoons_modified)\n\n\nData summary\n\n\nName\ncartoons_modified\n\n\nNumber of rows\n90\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nCartoon\n0\n1\nFALSE\n3\nCho: 30, Dor: 30, Dra: 30\n\n\nGender\n0\n1\nFALSE\n2\nMal: 45, Fem: 45\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nRating\n0\n1\n7.06\n1.94\n1\n6\n7\n8\n10\n▁▁▆▇▅\n\n\n\n\n\nObservation:\n\nI noticed that there is only a column called top_counts in the variables declared as factors, i can’t believe I’m just seeing this now!\nThere are 30 entries of each cartoon and 45 for each gender. Was the data collection done in way where there are 15 entries from each gender for every cartoon? If so it could have been with the intent of eliminating gender bias in the analysis of the data. Regardless, it gives me another factor! I can try and find out which gender group remembers cartoon more fondly if i do have to time for it.\n\n\ncartoons_modified %&gt;%\n  group_by(Cartoon, Gender) %&gt;%\n  summarize(count =n())\n\n`summarise()` has grouped output by 'Cartoon'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   Cartoon [3]\n  Cartoon      Gender count\n  &lt;fct&gt;        &lt;fct&gt;  &lt;int&gt;\n1 Chota Bheem  Male      15\n2 Chota Bheem  Female    15\n3 Doraemon     Male      15\n4 Doraemon     Female    15\n5 Dragon Tales Male      15\n6 Dragon Tales Female    15\n\n\nMy guess was right! There are 15 entries each from Males and Females for each Cartoon which removes gender bias from our analysis of this data!\n\nDistribution of all ratings!\n\ncartoons_modified %&gt;%\n  gf_histogram(~Rating)\n\n\n\n\n\n\n\n\nObservations:\n\nThe lowest rating given is 1, showing that at least one participant rated a cartoon at the lowest end of the scale.\np50 from skim is 7, which suggests that The median rating is 7, meaning that half of the ratings are 7 or lower. This suggests a central tendency toward higher ratings.\nThere is a high concentration of ratings around 6 and 8!\n\nRatings faceted by Cartoons\n\ngf_histogram(~Rating, fill = ~Cartoon, data = cartoons_modified, alpha=0.5) %&gt;%\n  gf_labs(title = \"Ratings faceted by Cartoons\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ncartoons_modified %&gt;%\n  gf_histogram(~Rating|Cartoon, fill = ~Cartoon, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Ratings faceted by Cartoons\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1 \n    )\n  ))\n\n\n\n\n\n\n\n\n\ncartoons_modified %&gt;%\n  gf_density(\n    ~ Rating,\n    fill = ~ Cartoon,\n    alpha = 0.5,\n    title = \"Rating Densities by different cartoons\",\n    subtitle = \"Chota Bheem vs Doraemon us Dragon Tales\"\n  )\n\n\n\n\n\n\n\n\nObservations:\n\nThe ratings of Chota Bheem had a bimodial distribution clustered around 6 and 8 approximately.\nFrom what i observe, Doremon and Dragon Tale Ratings have a normal distribution.\nAmong the 3, i think Chota Bheem is th least “liked” among people- It has the lowest distribution of high values, but it has the highest for approximately middle of the distribution.\nDragon Tales’ distribution peaks at approximately 7, while Doremon’s peaks at 8.5.\nWhen it comes to the lows, surprisingly, it’s Chota Bheem that has the least lows while comparatively Doremon has the most. Doremon has the highest highs and the highest lows but the number of lows are very less comapred to the number of highs.\n\n\n\nCrosstables:\nI can get a better idea of the accuracy of my hunches by doing crosstables to better understand my data!\n\ncrosstable(Rating ~ Cartoon, data = cartoons_modified) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariableCartoonChota BheemDoraemonDragon TalesRatingMin / Max3.0 / 10.01.0 / 10.01.0 / 10.0Med [IQR]6.4 [6.0;8.0]8.0 [6.0;9.0]7.0 [6.1;8.4]Mean (std)6.7 (1.5)7.2 (2.3)7.3 (2.0)N (NA)30 (0)30 (0)30 (0)\n\n\nVisualisation of the median and IQR\n\ncartoons_modified %&gt;%\n  gf_boxplot(Cartoon ~ Rating, fill = ~Cartoon, alpha=0.5) %&gt;%\n  gf_labs(title = \"Box plot of Ratings by Cartoon\")\n\n\n\n\n\n\n\n\nObservations:\n\nI think i was right in my inferences on the distribution of data!\nChota Bheem has the least mean (6.4) and median(6.7) with a narrow IQR range.\nWhile Doraemon has a higher median than Dragon tales, it has a slightly lower mean- this aligns with my analysis that while Doraemon has the highest number of highs, it also has the highest number of lows- making it’s median higher and mean lower. Supporting this, while it’s IQR range starts right along with Dragon Tales’, It’s whiskers are much longer to the left(lower side) and the IQR range lasts longer for Doraemon. The distribution of values to the left of the median of doremon is more spread out than the other 2, and more concentrated to the right compared to the other’s.\n\nBasically, Doraemon and Dragon Tales have higher mean and median ratings than Chhota Bheem, suggesting they are more positively regarded by participants overall.\nChota Bheem shows less variability and is clustered around a lower average rating, suggesting a mixed appeal among it’s audience and an “it’s not too great but it’s not bad either” kind of opinion i think.\n\n\n\nForming my Hypothesis:\nNull Hypothesis: There is no difference in the average ratings given by young adults to the three cartoons.\nAlternative Hypothesis: Chota Bheem is the least cherished by young adults compared to the the other 2.\nI cant really say for the most cherished one since I don’t think there is much a difference in the young adults view Dragon Tales and Doremon\nI think my alternate hypothesis is true among the two. Let’s see if my data says the same!"
  },
  {
    "objectID": "posts/Experiment4/index.html#statistical-analysis",
    "href": "posts/Experiment4/index.html#statistical-analysis",
    "title": "Experiment 4",
    "section": "Statistical Analysis:",
    "text": "Statistical Analysis:\n\nANOVA:\nAssumptions: 1. Data (and standard errors) are normally distributed. 2. Variances in rating between all groups (Doraemon, Chota Bheem, and Dragon Tale) are equal. 3. Observations are independent.\nANOVA is used to test hypotheses related to differences in the mean ratings among the three cartoons (Doraemon, Chota Bheem, and Dragon Tales).\n\ncartoons_anova &lt;- aov(Rating ~ Cartoon, data = cartoons_modified) \ncartoons_supernova &lt;- supernova::pairwise(cartoons_anova,\n  correction = \"Bonferroni\",\n  alpha = 0.05, # 95% CI calculation\n  var_equal = TRUE, \n  plot = TRUE\n) \n\n\n\n\n\n\n\n\nObservations:\n\nThe confidence interval for the difference between all the pairs of cartoons include 0 which indicates that we cannot conclude to state there is a significant difference in rating between the 3 cartoons. We cannot reject the null hypothesis.\n\nEvaluating the extent of effect!\n\ncartoons_supernova\n\n\n\n\n── Pairwise t-tests with Bonferroni correction ─────────────────────────────────\n\n\nModel: Rating ~ Cartoon\n\n\nCartoon\n\n\nLevels: 3\n\n\nFamily-wise error-rate: 0.049\n\n\n\n  group_1      group_2      diff pooled_se     t    df  lower upper  p_adj\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Doraemon     Chota Bheem 0.580     0.354 1.636    87 -0.186 1.346  .3161\n2 Dragon Tales Chota Bheem 0.597     0.354 1.683    87 -0.170 1.363  .2877\n3 Dragon Tales Doraemon    0.017     0.354 0.047    87 -0.750 0.783 1.0000\n\n\nObservations:\n1.Non of the 3 p-values are less than 0.05 which suggest that we cannot reject the null hypothesis.\nWhat if any of my assumptions about the data/groups that wrong? They are crucial to the accuracy of ANOVA!\n\nTest for Normality:\nThe null hypothesis: All 3 distributions of ratings are normal- bell-shaped.\nOverall Distribution of ratings\n\nshapiro.test(x = cartoons_modified$Rating) %&gt;%\n    broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.935 0.000227 Shapiro-Wilk normality test\n\n\nObservations: The high statistic value (&gt;0.9) and very low p-value (&lt;0.05) suggest that the overall distribution of ratings are normal.\nDistribution of ratings faceted by cartoon\n\ncartoons_modified %&gt;%\n  group_by(Cartoon) %&gt;%\n  group_modify(~ .x %&gt;%\n    select(Rating) %&gt;%\n    as_vector() %&gt;%\n    shapiro.test() %&gt;%\n    broom::tidy())\n\n# A tibble: 3 × 4\n# Groups:   Cartoon [3]\n  Cartoon      statistic p.value method                     \n  &lt;fct&gt;            &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n1 Chota Bheem      0.951  0.185  Shapiro-Wilk normality test\n2 Doraemon         0.909  0.0139 Shapiro-Wilk normality test\n3 Dragon Tales     0.918  0.0240 Shapiro-Wilk normality test\n\n\nLet’s visualize this:\n\ncartoons_modified %&gt;%\n  gf_density( ~ Rating,\n              fill = ~ Cartoon,\n              alpha = 0.5,\n              title = \"Rating of Cartoon vs normal distribution\") %&gt;%\n  gf_facet_grid(~ Cartoon) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\nObservations:\n\nDoraemon and Dragon Tales’ ratings are normally distributed with a p value &lt;0.05 and a w &gt; 0.9.\nWhile Chota Bheem’s is more confusing, while i has a large w value- 0.93, it’s p-value is not less than 0.05- it’s 0.18. I think it means that this distribution is mostly normal, but there is not enough evidence to confirm it- it could have happen by chance.\n\n\n\nTest for Varaince:\n\ncartoons_modified %&gt;%\n  group_by(Cartoon) %&gt;%\n  summarise(variance = var(Rating))\n\n# A tibble: 3 × 2\n  Cartoon      variance\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Chota Bheem      2.21\n2 Doraemon         5.25\n3 Dragon Tales     3.84\n\n\nWhile there is not too much of a difference between Chota Bheem and Dragon Tales- There is a larger difference in varince for Doremon comapritivly.\nDoes the fligner test say the same?\nnull hypothesis: he variances between the groups are similar.\n\nfligner.test(Rating ~ Cartoon, data = cartoons_modified)%&gt;%\n    broom::tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                          \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                           \n1      1.81   0.404         2 Fligner-Killeen test of homogeneity of variances\n\n\nSince the p-value (0.40) is greater than 0.05, I do not have sufficient evidence to reject the null hypothesis. This means that the variances across the groups (by Cartoons) are not significantly different, so we can reasonably assume homogeneity of variances.\nConsidering we could not prove the presence of a normal distribution when it comes to Chota Bheem’s distribution, it proves our assumption required for ANOVA to be accurate false. Therefore, we will conduct a permutation test to reach a conclusion!\n\n\n\nANOVA using Permutation Tests\nPermutation tests are distribution-free, meaning they don’t rely on specific assumptions about the underlying data distribution, including normality.\n\nobserved_infer &lt;-\n  cartoons_modified %&gt;%\n  specify(Rating ~ Cartoon) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  calculate(stat = \"F\")\nobserved_infer\n\nResponse: Rating (numeric)\nExplanatory: Cartoon (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.919\n\n\nThe observed variability in scores- f value is a small number: 0.9189246. Therefore, the between-group variance is not much larger than the within-group variance for my sample data.\nThe null distribution of f-values obtained by permutation\n\nnull_dist_infer &lt;- cartoons_modified %&gt;%\n  specify(Rating ~ Cartoon) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 4999, type = \"permute\") %&gt;%\n  calculate(stat = \"F\")\n##\nnull_dist_infer\n\nResponse: Rating (numeric)\nExplanatory: Cartoon (factor)\nNull Hypothesis: independence\n# A tibble: 4,999 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 0.124\n 2         2 0.111\n 3         3 0.282\n 4         4 2.38 \n 5         5 1.88 \n 6         6 2.43 \n 7         7 1.39 \n 8         8 1.49 \n 9         9 0.408\n10        10 0.413\n# ℹ 4,989 more rows\n\n\nVisualizing the presence of observed f value in the null distribution\n\nnull_dist_infer %&gt;%\n  visualise(method = \"simulation\") +\n  shade_p_value(obs_stat = observed_infer$stat, direction = \"right\") +\n  scale_x_continuous(trans = \"log10\", expand = c(0, 0)) +\n  coord_cartesian(xlim = c(0.2, 500), clip = \"off\") +\n  annotation_logticks(outside = FALSE)\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_x_continuous(trans = \"log10\", expand = c(0, 0)): log-10\ntransformation introduced infinite values.\n\n\n\n\n\n\n\n\n\nObservations:\n\nSince your observed F statistic (0.91) is at the lower end of this distribution, this suggests that it’s quite common to get an F value around 0.91 (or even higher) due to random chance. Therefore, we don’t have any evidence of to state that there is a difference in people’s reaction to all 3 cartoons.\n\n\n\nConclusion:\nThere is no statistically significant different between the opinion scores for the 3 cartoon! The sample population says they are all the similar when it comes to likability which I assume is the reality of the population as well!\n\nANALYSIS OF OPNION SCORES OF CARTOONS BASED ON GENDER:\nI’m not really sure i can do it for all this because the ratings taken given for 3 different cartoons to answer the question of people of which gender cherish cartoons more than the other. I’m going to go ahead and do it tho, because my statistical analysis based on cartoons concluded saying that there is no significant difference in opinion scores between the 3. Therefore, I assume i can generalise these scores to apply for all cartoons."
  },
  {
    "objectID": "posts/Experiment4/index.html#descriptive-analysis",
    "href": "posts/Experiment4/index.html#descriptive-analysis",
    "title": "Experiment 4",
    "section": "Descriptive Analysis:",
    "text": "Descriptive Analysis:\n\nDistribution of all ratings faceted by gender.\n\ngf_histogram(~Rating, fill = ~Gender, data = cartoons_modified, alpha=0.5) %&gt;%\n  gf_labs(title = \"Ratings faceted by Gender\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ncartoons_modified %&gt;%\n  gf_histogram(~Rating|Gender, fill = ~Gender, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Ratings faceted by Gender\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1 \n    )\n  ))\n\n\n\n\n\n\n\n\n\ncartoons_modified %&gt;%\n  gf_density(\n    ~ Rating,\n    fill = ~ Gender,\n    alpha = 0.5,\n    title = \"Rating Densities by Gender\",\n    subtitle = \"Male vs Female\"\n  )\n\n\n\n\n\n\n\n\nObservations:\n\nBoth of them are left skewed. At first glace, there does not seem to be much a difference in the 2 distributions.\nThe male distribution peaks at approximately 7. It has a number of a score of 10 compared to females and also a higher number of 1 score compared to females.\nThe female distribution peaks at 8 and there are lesser number of lower scores in it compared to the male distribution.\nWhen it comes to the average mean, i think females will have a higher mean than males.\n\n\n\nCrosstables:\nDoes the summation of the data support my hunch?\n\ncrosstable(Rating ~ Gender, data = cartoons_modified) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariableGenderMaleFemaleRatingMin / Max1.0 / 10.03.0 / 10.0Med [IQR]7.0 [6.0;8.5]7.0 [6.0;8.0]Mean (std)7.0 (2.2)7.2 (1.6)N (NA)45 (0)45 (0)\n\n\nVisualizing the median and IQR ranges.\n\ncartoons_modified %&gt;%\n  gf_boxplot(Gender ~ Rating, fill = ~Gender, alpha=0.5) %&gt;%\n  gf_labs(title = \"Box plot of Rating filled with Gender\")\n\n\n\n\n\n\n\n\nObservations:\n\nBoth males and females gave an average (mean) rating close to 7, with females slightly higher at 7.2 compared to 7.0 for males.\nThe median rating for both groups is also 7, suggesting that 7 is a common rating across genders. This similarity in mean and median ratings implies that, overall, both genders rate similarly.\nThe standard deviation for males (2.2) is slightly higher than that for females (1.6). This aligns with my previous hunches that that male ratings are more spread out, with some participants giving ratings much lower or higher than the average. There are outliers present in the male ratings (below 5), as indicated by the dots outside the whiskers. These lower outlier values suggest that a few male participants rated significantly lower than the rest which could explain the larger standard deviation.\nBoth groups have similar IQRs, meaning that the middle 50% of their ratings fall within a close range (6 to 8.5 for males and 6 to 8 for females) aligning with the hunch that both distributions are similar.\n\n\n\nForming My Hypothesis:\nNull Hypothesis: There is no difference in opinion scores for cartoons between males and females.\nAlternative Hypothesis: Females tend give larger values as opinion scores than Males."
  },
  {
    "objectID": "posts/Experiment4/index.html#statistical-analysis-hypothesis-testing",
    "href": "posts/Experiment4/index.html#statistical-analysis-hypothesis-testing",
    "title": "Experiment 4",
    "section": "Statistical Analysis: Hypothesis Testing",
    "text": "Statistical Analysis: Hypothesis Testing\n\nA t-test:\nAssumptions: 1. The distribution for each group - male and female is normal.\n\nmosaic::t_test(Rating ~ Gender, data = cartoons_modified)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   -0.222      6.95      7.17    -0.541   0.590      80.8    -1.04     0.595\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObservations:\n\nThe estimate suggests that females, on average, give 0.22 more value of score than males in our sample data set.\nThe high p-value of 0.5, greater than the typical significance level of 0.05, suggests that there is no statistically significant difference in the average tip amounts between males and females.The result could just be random, so I don’t have strong evidence that something is happening.\n0- which indicates that there is no difference on the average tip, falls within the confidence interval (-7.2 to 11.9) which suggest the same.\nTherefore we cannot reject the null hypothesis.\n\nBut what if my assumption of the tips being distributed normally is not accurate?\nFirst we transform our data set to be 2 different ones- one with all the Male values and the other with all the Female values.\n\nfiltered_Male &lt;- cartoons_modified %&gt;%\n  filter(Gender == \"Male\")\nfiltered_Female &lt;- cartoons_modified %&gt;%\n  filter(Gender == \"Female\")\nfiltered_Male\n\n        Cartoon Gender Rating\n1   Chota Bheem   Male    8.5\n2   Chota Bheem   Male    6.0\n3   Chota Bheem   Male    8.0\n4   Chota Bheem   Male    7.0\n5   Chota Bheem   Male    8.0\n6   Chota Bheem   Male   10.0\n7   Chota Bheem   Male    5.0\n8   Chota Bheem   Male    7.8\n9   Chota Bheem   Male    8.5\n10  Chota Bheem   Male    5.0\n11  Chota Bheem   Male    7.0\n12  Chota Bheem   Male    6.0\n13  Chota Bheem   Male    6.0\n14  Chota Bheem   Male    6.0\n15  Chota Bheem   Male    6.0\n16     Doraemon   Male    8.0\n17     Doraemon   Male    8.0\n18     Doraemon   Male    7.0\n19     Doraemon   Male    8.0\n20     Doraemon   Male   10.0\n21     Doraemon   Male    6.5\n22     Doraemon   Male    9.0\n23     Doraemon   Male    2.0\n24     Doraemon   Male    5.0\n25     Doraemon   Male    6.0\n26     Doraemon   Male   10.0\n27     Doraemon   Male    9.0\n28     Doraemon   Male    9.0\n29     Doraemon   Male    1.0\n30     Doraemon   Male   10.0\n31 Dragon Tales   Male   10.0\n32 Dragon Tales   Male    4.0\n33 Dragon Tales   Male    7.0\n34 Dragon Tales   Male   10.0\n35 Dragon Tales   Male    7.0\n36 Dragon Tales   Male    5.0\n37 Dragon Tales   Male    9.0\n38 Dragon Tales   Male    8.5\n39 Dragon Tales   Male    7.0\n40 Dragon Tales   Male    7.0\n41 Dragon Tales   Male    6.0\n42 Dragon Tales   Male    5.0\n43 Dragon Tales   Male    1.0\n44 Dragon Tales   Male    7.0\n45 Dragon Tales   Male    6.0\n\nfiltered_Female\n\n        Cartoon Gender Rating\n1   Chota Bheem Female    8.0\n2   Chota Bheem Female    6.8\n3   Chota Bheem Female    4.0\n4   Chota Bheem Female    7.5\n5   Chota Bheem Female    7.0\n6   Chota Bheem Female    6.0\n7   Chota Bheem Female    6.0\n8   Chota Bheem Female    8.0\n9   Chota Bheem Female    6.0\n10  Chota Bheem Female    6.0\n11  Chota Bheem Female    8.0\n12  Chota Bheem Female    8.0\n13  Chota Bheem Female    6.0\n14  Chota Bheem Female    5.0\n15  Chota Bheem Female    3.0\n16     Doraemon Female    8.0\n17     Doraemon Female    8.0\n18     Doraemon Female    9.0\n19     Doraemon Female    7.0\n20     Doraemon Female    5.0\n21     Doraemon Female   10.0\n22     Doraemon Female    9.0\n23     Doraemon Female    4.0\n24     Doraemon Female    6.0\n25     Doraemon Female    6.0\n26     Doraemon Female   10.0\n27     Doraemon Female    6.0\n28     Doraemon Female    7.0\n29     Doraemon Female    8.0\n30     Doraemon Female    6.0\n31 Dragon Tales Female    8.0\n32 Dragon Tales Female    9.0\n33 Dragon Tales Female   10.0\n34 Dragon Tales Female    8.0\n35 Dragon Tales Female    9.0\n36 Dragon Tales Female    8.0\n37 Dragon Tales Female   10.0\n38 Dragon Tales Female    6.0\n39 Dragon Tales Female    8.0\n40 Dragon Tales Female    8.0\n41 Dragon Tales Female    6.5\n42 Dragon Tales Female    7.0\n43 Dragon Tales Female    8.0\n44 Dragon Tales Female    7.0\n45 Dragon Tales Female    6.0\n\n\nVisualizing the distribution we acquire vs a normal distribution for each group:\n\ncartoons_modified %&gt;%\n  gf_density( ~ Rating,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"Opinion Score given\") %&gt;%\n  gf_facet_grid(~ Gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\nThe distribution of female ratings seems to be a bimodial distribution while the male one seems mostly normal.\n\n\nA shapiro test:\nShapiro test is one used to determine if a sample comes from a normally distributed population or not.\n\nFor Males:\nNull hypothesis: The Rating by Males is normally distributed.\n\nshapiro.test(filtered_Male$Rating)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic p.value method                     \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n1     0.922 0.00492 Shapiro-Wilk normality test\n\n\nWith a w value as high : 0.92 (&gt;0.9), we can accept the null hypothesis that it is normally distributed. The p-value suggests the same- it’s less than 0.05 which confirms that the w value we received is statistically significant, it did not happen by chance.\nRating given by males is normally distributed.\n\n\nFor Females:\nNull hypothesis: The Rating by Females is normally distributed.\n\nshapiro.test(filtered_Female$Rating)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic p.value method                     \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n1     0.949  0.0474 Shapiro-Wilk normality test\n\n\nWith a w value as high : 0.94 (&gt;0.9), we can accept the null hypothesis that it is normally distributed. The p-value suggests the same- it’s less than 0.05 which confirms that the w value we received is statistically significant, it did not happen by chance.\nRating given by females is normally distributed.\nTherefore, our assumption is right, but just to confirm, I’ll conduct a permutation test do confirm.\n\n\n\nPermutation Test\nThe permutation test is very flexible and requires minimal assumptions. It doesn’t assume normality or equal variances, making it useful for small or skewed data.\n\nobs_diff_gender &lt;- diffmean(Rating ~ Gender, \n                            data = cartoons_modified) \n\nobs_diff_gender\n\n diffmean \n0.2222222 \n\n\nFemale mean - Male mean = 0.2\nIn my sample data, females, on average, give 0.2 more score than males.\nCreating the Null Distribution by Permutation\n\nnull_dist_ratings &lt;- \n  do(4999) * diffmean(data =cartoons_modified, \n                      Rating ~ shuffle(Gender))\n##null_dist_ratings , it appears too long when i render it!\n\nVisualizing this:\n\ngf_histogram(data = null_dist_ratings, ~ diffmean, \n             bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, \n           colour = \"darkred\", linewidth = 1,\n           title = \"Null Distribution by Permutation\", \n           subtitle = \"Histogram\") %&gt;% \n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\nAcquiring the p-value:\nThe p-value is calculated by comparing the obs_diff_gender (from the actual data) to the null distribution acquired through permutation. It represents the proportion of times a difference as extreme as the observed one occurs by random chance alone.\n\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_ratings)\n\nprop_TRUE \n   0.7076 \n\n\nObservations:\n\nThe p-value observed through the permutation test is 0.71. If it would have been a value less than 0.05, it would indicate that that the observed difference is unlikely to have occurred by chance, leading to a rejection of the null hypothesis. But in case of a higher value like 0.7, we cannot reject the null hypothesis: There is no difference in opinion scores for cartoons between males and females.\nThe visualization of the histogram of mean differences- null distribution along with the observed mean different suggests the same. The observed mean difference is observed around the middle of our null distribution - I can easily mimic nature here - Nothing special is happening - I cannot reject the null hypothesis.\n\n\n\nConclusion:\nIn the assumption that my sample population is a true representation of my population-there is no difference in opinion scores for cartoons between males and females."
  },
  {
    "objectID": "posts/Experiment1/index.html#statistical-analysis-hypothesis-testing",
    "href": "posts/Experiment1/index.html#statistical-analysis-hypothesis-testing",
    "title": "Experiment 1",
    "section": "Statistical Analysis: Hypothesis testing",
    "text": "Statistical Analysis: Hypothesis testing\n\nA t-test:\nAssumptions:\n1. The distribution for each group - male and female is normal.\n\nmosaic::t_test(Money_spent ~ Gender, data = pockets_modified)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     55.3      749.      693.     0.136   0.893      76.5    -757.      868.\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nObservations:\n\nThe estimate suggests that male students, on average, spent 55.29 more than female students.89\nSince the p-value value is as high as 0.8, we fail to reject the null hypothesis. This suggests that there is no statistically significant difference between the spending of male and female students.The confidence interval of -757.07 and 867.66 suggest the same since it straddles 0. Therefore, our null hypothesis that gender does not play a role in the amount of money spent on a random day by a college student is true.\n\nBut is this t-test reliable?.. only if the the money spent by the sample is normally distributed. We test that using a Shapiro test.\n\n\nA Shapiro test:\nFor the total distribution:\n\nshapiro.test(pockets_modified$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.345 2.34e-17 Shapiro-Wilk normality test\n\n\nWe need to know if the individual distribution of both genders are normally distributed. And so we transform our data set to into 2- one with all the male values and one with all the female values.\nAcquiring the 2 data sets:\n\nfiltered_male &lt;- pockets_modified %&gt;%\n  filter(Gender == \"Male\")\nfiltered_female &lt;- pockets_modified %&gt;%\n  filter(Gender == \"Female\")\nfiltered_male\n\n        Name Gender Money_spent\n1      Aagam   Male         150\n2     Aakash   Male         240\n3    Adithya   Male          68\n4     Aditya   Male         300\n5      Anish   Male           0\n6     Ankush   Male         250\n7      Arjun   Male        1143\n8       Arya   Male         100\n9      Aryan   Male       10000\n10      Aziz   Male        1433\n11       Dan   Male         600\n12 Deborishi   Male         205\n13    Dhiman   Male         910\n14     Dhruv   Male         900\n15     Dhurv   Male         185\n16   Ezhilan   Male         842\n17      Geet   Male          50\n18      Jeff   Male         200\n19   Jeffrey   Male         150\n20    Kartik   Male         145\n21   Koustav   Male         250\n22     Krish   Male          70\n23    Lekith   Male         100\n24    Maahin   Male         310\n25    Madhav   Male        4000\n26     Manav   Male          40\n27    Nivaan   Male         220\n28     Rikit   Male         259\n29     Ritik   Male        1000\n30    Rudraj   Male         478\n31      Ryan   Male         330\n32  Shashank   Male         400\n33  Shashwat   Male         149\n34     Shiva   Male         250\n35    Suhaas   Male         997\n36     Sutej   Male           0\n37     Taran   Male        1399\n38  Tathastu   Male        1535\n39     Varad   Male         154\n40      Veer   Male         566\n41     Viraj   Male         315\n\nfiltered_female\n\n           Name Gender Money_spent\n1       Aarushi Female         382\n2       Abheeta Female          60\n3      Akanksha Female         270\n4        Amruta Female         190\n5        Anaaya Female         300\n6       Anousha Female          85\n7      Anoushka Female         700\n8       Anushka Female         140\n9          Asra Female        1070\n10      Bhumika Female        1200\n11        Daana Female          66\n12     Devanshi Female         700\n13        Eisha Female         300\n14       Harjot Female        3000\n15      Janhavi Female         150\n16      Kalyani Female           0\n17       Kashvi Female         430\n18       Kavana Female         500\n19       Khushi Female         785\n20     Kshirija Female         100\n21       Maanya Female          15\n22      Nandana Female           0\n23        Navya Female          55\n24    Nayantara Female         192\n25        Neeti Female         280\n26       Nithya Female          85\n27       Parisa Female          80\n28        Risha Female           0\n29      Rukaiya Female          20\n30       Shaivi Female         318\n31       Shreya Female         585\n32       Simran Female         340\n33 Simran Anand Female         660\n34      Snigdha Female         100\n35      Suprita Female         500\n36       Tanmay Female         800\n37        Tanya Female           0\n38       Tarini Female         200\n39        Vanya Female         350\n40     Vasantha Female         418\n41   Vasundhara Female       13000\n\n\nVisualizing the distribution we acquire vs a normal distribution:\n\npockets_modified %&gt;%\n  gf_density( ~ Money_spent,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"Money spent on 23rd Oct\") %&gt;%\n  gf_facet_grid(~ Gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\n\nConducting the Shapiro test for each of these distributions:\nFor males:\nNull hypothesis: The money spent by males is normally distributed.\n\nshapiro.test(filtered_male$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.407 1.14e-11 Shapiro-Wilk normality test\n\n\nThe statistic indicates how closely the sample data follows a normal distribution (the closer the value is the 1, the more normal the distribution is) and so the computed value of 0.40 says that the distribution is not normal.\nA p value so low- 1.135947e-11 indicates that we must reject the null hypothesis.Therefore, the data is not normally distributed.\nFor females:\nNull hypothesis: The money spent by females is normally distributed.\n\nshapiro.test(filtered_female$Money_spent)%&gt;%\n  broom::tidy()\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.296 8.96e-13 Shapiro-Wilk normality test\n\n\nSimilar to the inference for the distribution for males, even in females, we can reject the null hypothesis of the distribution being normal.\nWe go ahead and conduct a Mann-Whitney Test having proven that both distribution aren’t normal, proving the t-test to be invalid.\n\n\n\nMann-Whitney Test\n\nwilcox.test(Money_spent ~ Gender, data = pockets_modified, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact confidence intervals with ties\n\n\n# A tibble: 1 × 7\n  estimate statistic p.value conf.low conf.high method               alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;      \n1     55.0      936.   0.381    -70.0      180. Wilcoxon rank sum t… two.sided  \n\n\nObservations:\nThe estimate of the difference between both groups is 55.00 a high p-value (&gt;0.05) of 0.38 suggests that we cannot reject the null hypothesis. There is statistically no significant difference between the two groups’ median values. 0 falls within the confidence interval which further suggest the same.\nI received an error saying “Warning: cannot compute exact p-value with ties”. This means that because i have so may repeating values (I’m guessing mainly 0)- there are many values with the same rank and polarity. This could lead to not having precise p-values.\nTherefore, i cannot reject the null hypothesis right away. To make sure, i will go ahead and perform a permutation test to come to a solid inference, one that i cannot question.\n\n\nPermutation Test\n\nobs_diff_gender &lt;- diffmean(Money_spent ~ Gender, \n                            data = pockets_modified) \n\nobs_diff_gender\n\n diffmean \n-55.29268 \n\n\nHere we create a null distribution for the difference in means by performing 4,999 permutations- each engraving and different values among the 2 gender groups. We do this by randomly reassigning the ‘Gender’ variable across all observations\n\nnull_dist_money &lt;- \n  do(4999) * diffmean(data = pockets_modified, \n                      Money_spent ~ shuffle(Gender))\n##null_dist_money , it appears too long when i render it!\n\nVisualization of the observed mean relative to the null distribution:\n\ngf_histogram(data = null_dist_money, ~ diffmean, \n             bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, \n           colour = \"darkred\", linewidth = 1,\n           title = \"Null Distribution by Permutation\", \n           subtitle = \"Histogram\") %&gt;% \n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\nThe p-value obtained through the permutation test:\nprop1 calculates the p-value by comparing the observed difference in means (obs_diff_gender) with the null distribution.\n\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_money)\n\nprop_TRUE \n   0.4392 \n\n\nObservations:\n\nThe p-value observed through the permutation test is 0.41. If it would have been a value less than 0.05, it would indicate that that the observed difference is unlikely to have occurred by chance, leading to a rejection of the null hypothesis. But in this case the higher value i.e. 0.41, we cannot reject the null hypothesis: There is no difference in the amount of pocket money received by male and female students .\nThe visualization of the histogram of mean differences- null distribution along with the observed mean different suggests the same - I cannot reject the null hypothesis.\n\n\nConclusion:\nIn the assumption that my sample population is a true representation of my population- There is no difference in the amount spent by male and female students on a random day- and so there must be no difference in the pocket money received by both genders."
  },
  {
    "objectID": "posts/Experiment2/index.html#descriptive-analysis",
    "href": "posts/Experiment2/index.html#descriptive-analysis",
    "title": "Experiment 2",
    "section": "Descriptive Analysis:",
    "text": "Descriptive Analysis:\n\nglimpse(tips_modified)\n\nRows: 60\nColumns: 4\n$ Name       &lt;chr&gt; \"Aanya\", \"Adit\", \"Aditi\", \"Akash\", \"Akshita\", \"Anandita\", \"…\n$ Gender     &lt;fct&gt; Female, Male, Female, Male, Female, Female, Female, Female,…\n$ Preferance &lt;ord&gt; Veg, Veg, Veg, Non-veg, Non-veg, Non-veg, Non-veg, Veg, Veg…\n$ Tip        &lt;int&gt; 0, 0, 20, 0, 0, 0, 20, 35, 40, 0, 0, 0, 0, 0, 0, 0, 20, 0, …\n\n\n\nskim(tips_modified)\n\n\nData summary\n\n\nName\ntips_modified\n\n\nNumber of rows\n60\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1\n4\n9\n0\n60\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n2\nFem: 30, Mal: 30\n\n\nPreferance\n0\n1\nTRUE\n2\nVeg: 30, Non: 30\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTip\n0\n1\n11.17\n17.84\n0\n0\n0\n20\n100\n▇▁▁▁▁\n\n\n\n\n\nObservations:\n\nThere are no missing values in the data-set.\nThere are 30 vegetarian and 30 non-vegetarian entries in the Food Preference factor. There are 30 male and 30 female entries in the Gender factor.Was data collected in a way that there are 15 male and 15 female vegetarians and 15 male and 15 female non-vegetarians? I should summerise and check!\nIt’s interesting to see that the progression of p0 to p100 is a series of 0, 0, 0, 20 and 100. The smallest tip given is 0 while the biggest is 100. This also indicates a large number of 0 -no tip in the data set compared to other values. It’s most likely a very right skewed distribution. It would be interesting to analyse who contributed to more of these 0s or if everyone is equally kanjoos regardless of food preference or gender.\n\n\nSummeries and Visualizations of the data\nDistribution of overall tips:\n\ntips_modified %&gt;%\n  gf_histogram(~Tip)\n\n\n\n\n\n\n\n\nObservations:\n\nLike i had thought inspecting the day, a majority of participants have not given any tip. It as a right skewed distribution.\nAmong people who do tip, 20rs is the most tipped value of money.\nIs tipping value 100 an outlier?\n\n\ntips_modified %&gt;%\n  group_by(Preferance, Gender) %&gt;%\n  summarize(count =n()) \n\n`summarise()` has grouped output by 'Preferance'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Preferance [2]\n  Preferance Gender count\n  &lt;ord&gt;      &lt;fct&gt;  &lt;int&gt;\n1 Veg        Female    15\n2 Veg        Male      15\n3 Non-veg    Female    15\n4 Non-veg    Male      15\n\n\nObservation: My previous hypothesis that there are 15 male and 15 female vegetarians and 15 male and 15 female non-vegetarians. This must have been done to remove any gender bias from our analysis.\nIt gives us the opportunity to also inspect another aspect to tipping - if gender plays a roll or not- it works either ways- on doing it in this way, our analysis has no bias based on dietary decisions. For now, let’s continue with the Food preference tipping analysis.\nTips faceted by Food Preference:\n\ngf_histogram(~Tip, fill = ~Preferance, data = tips_modified, bins=100) %&gt;%\n  gf_labs(title = \"Tips faceted by Food Preference\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ntips_modified %&gt;%\n  gf_histogram(~Tip|Preferance, fill = ~Preferance, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Tips faceted by Food Preference\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1 \n    )\n  ))\n\n\n\n\n\n\n\n\n\ntips_modified %&gt;%\n  gf_density(\n    ~ Tip,\n    fill = ~ Preferance,\n    alpha = 0.5,\n    title = \"Money Spent Densities\",\n    subtitle = \"Vegetarians vs Non-Vegetarians\"\n  )\n\n\n\n\n\n\n\n\nObservations:\n\nBoth groups show a right-skewed distribution with a high count of low tip (0) amount. The count of participants giving no tip appears to be similar for both groups.\nThese graphs suggest that while the count of tips (below 25) appears to be higher for non-vegetarians, The count of higher tips (above 25), while low for both groups, is relatively higher for vegetarians.\nMaybe food preference might not have a strong influence on the tip amounts given by students? Or maybe Vegetarians tip more than non vegetarians in the sense that when they vegetarians do give tips, they tend to give a higher amounts than non- vegetarians do?\n\n\nCrosstables:\nTrying to find the difference in means and medians of tips given based on the difference in food preference within the sample population to get a better understanding of what the data is saying:\n\ncrosstable(Tip ~ Preferance, data = tips_modified) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariablePreferanceVegNon-vegTipMin / Max0 / 100.00 / 50.0Med [IQR]0 [0;20.0]0 [0;20.0]Mean (std)12.3 (21.9)10.0 (12.9)N (NA)30 (0)30 (0)\n\n\nVisualizing the Median and IQR ranges:\n\ntips_modified %&gt;%\n  gf_boxplot(Preferance ~ Tip, fill = ~Preferance, alpha=0.5) %&gt;%\n  gf_labs(title = \"Box plot of Tip filled with Food Preference\")\n\n\n\n\n\n\n\n\nObservations:\n\nThe mean value of the tips given by vegetarians(12.3) is higher than that given by non- vegetarians(10) which makes sense considering our earlier observation of the fact that the giving of higher tips is more common among vegetarians.\nThe higher standard deviation in the Vegetarian group-21.9 suggests that they have more variability in tipping behavior compared to Non-vegetarians-12.9.\nThe median tip for both groups is 0, indicating that at least half of the participants from both groups tipped nothing.\nThe IQR range for both groups is 0 to 20 i.e. 50% of the tips for both groups falls within this range.\nDoes this suggest that most participants, regardless of food preference, tend to give low tips, with the Vegetarian group having one visible outlier that skews its range higher- there is no difference in tipping behavior based on food preference?\n\n\n\n\nForming my Hypothesis:\nNull Hypothesis: There is no difference in the average tip amount between vegetarians and non-vegetarians.\nAlternative Hypothesis: Vegetarians tend give larger values as tips than non-vegetarians.\nIt’s not possible to come to any concrete conclusion with just the descriptive analysis of the data, and so, I move to conduct statistical analysis on this data."
  },
  {
    "objectID": "posts/Experiment3/index.html#descriptive-analysis",
    "href": "posts/Experiment3/index.html#descriptive-analysis",
    "title": "Experiment 3",
    "section": "Descriptive Analysis:",
    "text": "Descriptive Analysis:\n\nglimpse(grades_modified)\n\nRows: 90\nColumns: 5\n$ Degree &lt;fct&gt; B.Des, B.Des, B.Des, B.Des, B.Des, B.Des, B.Des, B.Des, B.Des, …\n$ Course &lt;fct&gt; CAC, CAC, IADP, CE, BSSD, CAC, PSD, PSD, PSD, BSSD, VCSB, VCSB,…\n$ Year   &lt;ord&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, …\n$ Gender &lt;fct&gt; F, F, F, F, M, F, F, F, F, F, F, F, M, M, F, M, F, M, F, M, F, …\n$ Score  &lt;dbl&gt; 8.0, 9.6, 9.2, 9.8, 3.0, 9.5, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0…\n\n\n\nskim(grades_modified)\n\n\nData summary\n\n\nName\ngrades_modified\n\n\nNumber of rows\n90\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDegree\n0\n1\nFALSE\n3\nB.D: 30, B.V: 30, B.F: 30\n\n\nCourse\n0\n1\nFALSE\n15\nDMA: 20, UII: 13, DMP: 11, CAC: 8\n\n\nYear\n0\n1\nTRUE\n3\n2: 46, 3: 37, 1: 7\n\n\nGender\n0\n1\nFALSE\n2\nF: 57, M: 33\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nScore\n0\n1\n8.06\n1.13\n3\n7\n8\n9\n10\n▁▁▆▇▇\n\n\n\n\n\nObservations:\n\nI have an unequal count of levels of genders and Years and although this doesn’t prevent me from testing for significant differences, the statistical power of the tests might be affected, especially if one group is very small compared to the other, which is how it is in both cases. I think it would have been fine if i was looking to find the proportions but since that is not my focus, I think I’ll stick to trying to find if there are significant differences in grades between students enrolled in different degree programs.\nThe lowest grade obtained seems to be 3 while the highest seems to be 10. The mean value is 8.03 which suggests that the distribution of all scores together is left skewed.\n\n\nSummaries and Visualizations of the data\nDistribution of overall grades:\n\ngrades_modified %&gt;%\n  gf_histogram(~Score)\n\n\n\n\n\n\n\n\nObservations:\n\nThe distribution seems to be kind of normal expect for an outlier at 3?\nThe most common grade obtained is 8.\n\n\ngf_histogram(~Score, fill = ~Degree, data = grades_modified, alpha=0.5) %&gt;%\n  gf_labs(title = \"Grades faceted by Degree\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ngrades_modified %&gt;%\n  gf_histogram(~Score|Degree, fill = ~Degree, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Grades faceted by Degree\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1 \n    )\n  ))\n\n\n\n\n\n\n\n\n\ngrades_modified %&gt;%\n  gf_density(\n    ~ Score,\n    fill = ~ Degree,\n    alpha = 0.5,\n    title = \"Grades Densities by Degrees \",\n    subtitle = \"B.Des vs B.Voc us B.FA\"\n  )\n\n\n\n\n\n\n\n\nObservations:\n\nThe plot suggests that B.Des students might have more high scores (8 and above) compared to B.FA and B.Voc students.\nThe B.FA program has a wider distribution, with students scoring from as low as 5 to as high as 9.7. The other programs (B.Des and B.Voc) show a denser concentration in the higher score range, with fewer students scoring below 6.\nThe spread and shape differences across these distributions suggest possible variations in grading patterns or student performance across programs.\n\n\nCrosstables:\nTrying to find the difference in means and medians of grades obtained degree vise within the sample population to get a better understanding of what the data is saying.\n\ncrosstable(Score ~ Degree, data = grades_modified) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariableDegreeB.DesB.VocB.FAScoreMin / Max3.0 / 10.05.0 / 9.56.0 / 9.0Med [IQR]9.0 [8.0;9.0]8.0 [8.0;9.0]8.0 [7.0;8.0]Mean (std)8.4 (1.3)8.2 (1.1)7.6 (0.8)N (NA)30 (0)30 (0)30 (0)\n\n\nObservations:\n\nThe mean values indicate that B.Des students, on average, have higher scores, followed by B.Voc and then B.FA.\nThe Standard deviation suggests that B.FA has the lowest variability in scores while B.Des has the most.\n\nVisualizing the median and IQR through a box plot to better understand it.\n\ngrades_modified %&gt;%\n  gf_boxplot(Degree ~ Score, fill = ~Degree, alpha=0.5) %&gt;%\n  gf_labs(title = \"Box plot of Scores by Degree\")\n\n\n\n\n\n\n\n\nObservations:\n\nThe median tip for 2 groups B.Voc and B.FA are 8, Indicating half the participants got more than 8 and half got less than 8. But their IQR ranges are different. B.FA is 7.0-8.0 while B.Voc is 8.0;9.0 which indicated that even through their medians are the same, B.Voc students tend to get more scores among the 2?\nB.Des has a higher median, indicating overall higher scores compared to the other two groups.There is an outlier in B.Des with a score below 4, which likely represents an unusually low performance in this group.\nThe IQR ranges of B.Voc and B.Des overlap perfectly with each other. Therefore 50 percent of all grades obtained by there 2 groups are in the same small range. In this case, is there really a significant enough difference in the grades obtained by the 2 groups?\n\nHypothesis:\nNull Hypothesis: There is no differences in grades among students enrolled in different degree programs.\nAlternative Hypothesis: B.FA students tend get lower grades than students studying for other degrees. / Is there a difference in scores achieved by B.Des and B.Voc? I don’t think there would be one significant enough looking at the descriptive analysis\nWe can reasonably hypothesize that there is a difference in scores based on degree, but is this difference statistically significant? We can use ANOVA for a pair-wise comparison and find out!"
  },
  {
    "objectID": "posts/Experiment3/index.html#descriptive-analysis-1",
    "href": "posts/Experiment3/index.html#descriptive-analysis-1",
    "title": "Experiment 3",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\nFiltering the data:\n\nfiltered_FA &lt;- grades_modified %&gt;%\n  filter(Degree == \"B.FA\")\nfiltered_FA \n\n   Degree Course Year Gender Score\n1    B.FA    CAP    3      F   8.0\n2    B.FA    CAP    3      M   9.0\n3    B.FA    CAP    3      F   9.0\n4    B.FA    DMA    2      M   8.5\n5    B.FA    DMA    2      M   8.0\n6    B.FA    DMA    2      F   8.0\n7    B.FA    CAP    2      F   8.0\n8    B.FA    CAP    2      F   8.0\n9    B.FA   Film    3      M   6.0\n10   B.FA    DMA    2      F   8.0\n11   B.FA    DMA    2      F   7.0\n12   B.FA    DMA    2      F   7.0\n13   B.FA    DMA    2      F   8.0\n14   B.FA    DMA    2      F   7.0\n15   B.FA    DMA    2      M   6.0\n16   B.FA   Film    3      M   8.0\n17   B.FA   Film    3      M   8.0\n18   B.FA    DMA    2      M   8.0\n19   B.FA    DMA    2      M   8.0\n20   B.FA    DMA    2      F   7.0\n21   B.FA    DMA    2      F   7.0\n22   B.FA   Film    2      F   8.0\n23   B.FA   Film    3      F   7.0\n24   B.FA    DMA    2      F   9.0\n25   B.FA    DMA    2      M   8.0\n26   B.FA    DMA    2      F   7.0\n27   B.FA    DMA    2      F   7.0\n28   B.FA    DMA    2      M   7.0\n29   B.FA    DMA    2      F   7.0\n30   B.FA    DMA    2      F   7.0\n\n\n\nglimpse(filtered_FA)\n\nRows: 30\nColumns: 5\n$ Degree &lt;fct&gt; B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.FA, B.F…\n$ Course &lt;fct&gt; CAP, CAP, CAP, DMA, DMA, DMA, CAP, CAP, Film, DMA, DMA, DMA, DM…\n$ Year   &lt;ord&gt; 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, …\n$ Gender &lt;fct&gt; F, M, F, M, M, F, F, F, M, F, F, F, F, F, M, M, M, M, M, F, F, …\n$ Score  &lt;dbl&gt; 8.0, 9.0, 9.0, 8.5, 8.0, 8.0, 8.0, 8.0, 6.0, 8.0, 7.0, 7.0, 8.0…\n\n\n\nskim(filtered_FA)\n\n\nData summary\n\n\nName\nfiltered_FA\n\n\nNumber of rows\n30\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDegree\n0\n1\nFALSE\n1\nB.F: 30, B.D: 0, B.V: 0\n\n\nCourse\n0\n1\nFALSE\n3\nDMA: 20, Fil: 5, CAP: 5, CAC: 0\n\n\nYear\n0\n1\nTRUE\n2\n2: 23, 3: 7, 1: 0\n\n\nGender\n0\n1\nFALSE\n2\nF: 19, M: 11\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nScore\n0\n1\n7.62\n0.78\n6\n7\n8\n8\n9\n▁▇▁▇▂\n\n\n\n\n\nThere are 3 courses: DMA, Film and CAP.\nWhat is the count of each of these courses? We will not be able to do an accurate analysis of the data id there is too little or too large of data for any Course.\n\nfiltered_FA %&gt;%\n  group_by(Course) %&gt;%\n  summarize(count =n())\n\n# A tibble: 3 × 2\n  Course count\n  &lt;fct&gt;  &lt;int&gt;\n1 Film       5\n2 CAP        5\n3 DMA       20\n\n\nOkay not possible. There are too little DMA and CAP students - 5 each and too many DMA students in the data set. But just to be curious what is the distribution of each?\n\nfiltered_FA %&gt;%\n  gf_density(\n    ~ Score,\n    fill = ~ Course,\n    alpha = 0.5,\n    title = \"Grades Densities by Courses within B.FA\",\n    subtitle = \"Film vs CAP us DMA\"\n  )\n\n\n\n\n\n\n\n\nThat looks so cool! At a fist glance, it seems like among the 3, if this data was good enough, CAP students tend to get more grades than the other 2 while film students tend to get the least. I could be completely wrong here."
  }
]