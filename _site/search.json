[
  {
    "objectID": "posts/Workflow/index.html",
    "href": "posts/Workflow/index.html",
    "title": " Facing the Abyss - WorkFLow",
    "section": "",
    "text": "So you have your shiny new R skills and you’ve successfully loaded a cool dataframe into R… Now what?\nThe best charts come from understanding your data, asking good questions from it, and displaying the answers to those questions as clearly as possible."
  },
  {
    "objectID": "posts/Workflow/index.html#a-data-analytics-process",
    "href": "posts/Workflow/index.html#a-data-analytics-process",
    "title": " Facing the Abyss - WorkFLow",
    "section": "",
    "text": "So you have your shiny new R skills and you’ve successfully loaded a cool dataframe into R… Now what?\nThe best charts come from understanding your data, asking good questions from it, and displaying the answers to those questions as clearly as possible."
  },
  {
    "objectID": "posts/Workflow/index.html#setting-up-r-packages",
    "href": "posts/Workflow/index.html#setting-up-r-packages",
    "title": " Facing the Abyss - WorkFLow",
    "section": "{{< iconify noto-v1 package >}} Setting up R Packages",
    "text": "{{&lt; iconify noto-v1 package &gt;}} Setting up R Packages\n\nInstall packages using install.packages() in your Console.\nLoad up your libraries in a setup chunk:\n\n\nknitr::opts_chunk$set(dev = \"ragg_png\")\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(palmerpenguins)\nlibrary(ggformula)\nlibrary(ggridges)\nlibrary(skimr)\n##\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\n\nGo to https://fonts.google.com/ and choose some professional looking, or funky looking, fonts.\n\nlibrary(extrafont)\n\nRegistering fonts with R\n\nextrafont::loadfonts(quiet = TRUE)\n##\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\n\n\nAttaching package: 'showtextdb'\n\n\nThe following object is masked from 'package:extrafont':\n\n    font_install\n\n## Loading Google fonts (https://fonts.google.com/)\nfont_add_google(name = \"Fira Sans Condensed\", family = \"fira\")\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Montserrat Alternates\", \"montserrat\")\nfont_add_google(\"Roboto Condensed\", \"roboto\")\n### Automatically use showtext to render text\nshowtext_auto()"
  },
  {
    "objectID": "posts/Workflow/index.html#read-data",
    "href": "posts/Workflow/index.html#read-data",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Read Data",
    "text": "Read Data\n\nUse readr::read_csv()"
  },
  {
    "objectID": "posts/Workflow/index.html#examine-data",
    "href": "posts/Workflow/index.html#examine-data",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Examine Data",
    "text": "Examine Data\n\nUse dplyr::glimpse()\nUse mosaic::inspect() or skimr::skim()\nUse dplyr::summarise() and crosstable::crosstable()\nFormat your tables with knitr::kable()\nHighlight any interesting summary stats or data imbalances"
  },
  {
    "objectID": "posts/Workflow/index.html#data-dictionary-and-experiment-description",
    "href": "posts/Workflow/index.html#data-dictionary-and-experiment-description",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Data Dictionary and Experiment Description",
    "text": "Data Dictionary and Experiment Description\n\nA table containing the variable names, their interpretation, and their nature(Qual/Quant/Ord…)\nIf there are wrongly coded variables in the original data, state them in their correct form, so you can munge the in the next step\nDeclare what might be target and predictor variables, based on available information of the experiment, or a description of the data"
  },
  {
    "objectID": "posts/Workflow/index.html#data-munging",
    "href": "posts/Workflow/index.html#data-munging",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Data Munging",
    "text": "Data Munging\n\nConvert variables to factors as needed\nReformat / Rename other variables as needed\nClean badly formatted columns (e.g. text + numbers) using tidyr::separate_**_**()\nSave the data as a modified file\nDo not mess up the original data file"
  },
  {
    "objectID": "posts/Workflow/index.html#form-hypotheses",
    "href": "posts/Workflow/index.html#form-hypotheses",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Form Hypotheses",
    "text": "Form Hypotheses\n\nQuestion-1\n\nState the Question or Hypothesis\n(Temporarily) Drop variables using dplyr::select()\nCreate new variables if needed with dplyr::mutate()\nFilter the data set using dplyr::filter()\nReformat data if needed with tidyr::pivot_longer() or tidyr::pivot_wider()\nAnswer the Question with a Table, a Chart, a Test, using an appropriate Model for Statistical Inference\nUse title, subtitle, legend and scales appropriately in your chart\nPrefer ggformula unless you are using a chart that is not yet supported therein (eg. ggbump() or plot_likert())\n\n\n## Set graph theme\n## Idotic that we have to repeat this every chunk\n## Open issue in Quarto\n\npenguins %&gt;% \n  drop_na() %&gt;% \n  gf_point(body_mass_g ~ flipper_length_mm, \n           colour = ~ species) %&gt;% \n  gf_labs(title = \"My First Penguins Plot\",\n          subtitle = \"Using ggformula with fonts\",\n          x = \"Flipper Length mm\", y = \"Body Mass gms\",\n          caption = \"I love penguins, and R\") %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n  gf_theme(theme(\n      panel.grid.minor = element_blank(),\n      ###\n      text = element_text(family = \"fira\", size = 14),\n      ###\n      plot.title = element_text(\n      family = \"roboto\",\n      face = \"bold\",\n      size = 28, hjust = 0\n    ),\n    plot.subtitle = element_text(\n      family = \"montserrat\",\n      face = \"bold\",\n      size = 18, hjust = 0),\n    \n    plot.margin = margin(2,2,2,2, unit = \"pt\"),\n    \n    axis.title = element_text(size = 20),\n    \n    plot.caption = element_text(family = \"gochi\", size = 14),\n    \n    legend.title = element_text(\n      family = \"bell\",\n      face = \"bold\",\n      size = 20\n    ),\n    legend.text = element_text(family = \"fira\",\n                               size = 12),\n    \n    legend.background = element_rect(fill = \"cornsilk\",\n                                     colour = \"black\"),\n    legend.margin = margin(\n      t = 2,\n      r = 2,\n      b = 2,\n      l = 2,\n      unit = \"pt\"\n    )\n    ))\n\n\n\n\n\n\n\n\n\n\nInference-1\n. . . .\n\n\nQuestion-n\n….\n\n\nInference-n\n…."
  },
  {
    "objectID": "posts/Workflow/index.html#one-most-interesting-graph",
    "href": "posts/Workflow/index.html#one-most-interesting-graph",
    "title": " Facing the Abyss - WorkFLow",
    "section": "One Most Interesting Graph",
    "text": "One Most Interesting Graph"
  },
  {
    "objectID": "posts/Workflow/index.html#conclusion",
    "href": "posts/Workflow/index.html#conclusion",
    "title": " Facing the Abyss - WorkFLow",
    "section": "Conclusion",
    "text": "Conclusion\nDescribe what the graph shows and why it so interesting. What could be done next?"
  },
  {
    "objectID": "posts/Workflow/index.html#references",
    "href": "posts/Workflow/index.html#references",
    "title": " Facing the Abyss - WorkFLow",
    "section": "References",
    "text": "References\n\nhttps://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/"
  },
  {
    "objectID": "posts/Day-5(2)/index.html",
    "href": "posts/Day-5(2)/index.html",
    "title": "Day 5 - Part 2",
    "section": "",
    "text": "For count, both variable are quantitative"
  },
  {
    "objectID": "posts/Day-5(2)/index.html#hollywood-movies-data-set",
    "href": "posts/Day-5(2)/index.html#hollywood-movies-data-set",
    "title": "Day 5 - Part 2",
    "section": "Hollywood movies data-set",
    "text": "Hollywood movies data-set\n\nHollywoodMovies2011 -&gt; movies\nglimpse(movies)\n\nRows: 136\nColumns: 14\n$ Movie             &lt;fct&gt; \"Insidious\", \"Paranormal Activity 3\", \"Bad Teacher\",…\n$ LeadStudio        &lt;fct&gt; Sony, Independent, Independent, Warner Bros, Relativ…\n$ RottenTomatoes    &lt;int&gt; 67, 68, 44, 96, 90, 93, 75, 35, 63, 69, 69, 49, 26, …\n$ AudienceScore     &lt;int&gt; 65, 58, 38, 92, 77, 84, 91, 58, 74, 73, 72, 57, 68, …\n$ Story             &lt;fct&gt; Monster Force, Monster Force, Comedy, Rivalry, Rival…\n$ Genre             &lt;fct&gt; Horror, Horror, Comedy, Fantasy, Comedy, Romance, Dr…\n$ TheatersOpenWeek  &lt;int&gt; 2408, 3321, 3049, 4375, 2918, 944, 2534, 3615, NA, 2…\n$ BOAverageOpenWeek &lt;int&gt; 5511, 15829, 10365, 38672, 8995, 6177, 10278, 23775,…\n$ DomesticGross     &lt;dbl&gt; 54.01, 103.66, 100.29, 381.01, 169.11, 56.18, 169.22…\n$ ForeignGross      &lt;dbl&gt; 43.00, 98.24, 115.90, 947.10, 119.28, 83.00, 30.10, …\n$ WorldGross        &lt;dbl&gt; 97.009, 201.897, 216.196, 1328.111, 288.382, 139.177…\n$ Budget            &lt;dbl&gt; 1.5, 5.0, 20.0, 125.0, 32.5, 17.0, 25.0, 80.0, 0.2, …\n$ Profitability     &lt;dbl&gt; 64.672667, 40.379400, 10.809800, 10.624888, 8.873292…\n$ OpeningWeekend    &lt;dbl&gt; 13.27, 52.57, 31.60, 169.19, 26.25, 5.83, 26.04, 85.…\n\n\n\nMovie (Factor):\n\nThis is likely the title or identifier of the movie. Since it’s a factor, it means there are multiple unique movie titles.\n\nLeadStudio (Factor):\n\nThe production studio that produced or distributed the movie. It could include studios like Warner Bros, Disney, Universal, etc.\n\nRottenTomatoes (Numeric):\n\nThe movie’s rating or score on Rotten Tomatoes, a popular review aggregator. It’s often expressed as a percentage, representing the proportion of positive reviews by critics.\n\nAudienceScore (Numeric):\n\nA score reflecting how audiences rated the movie, also typically on a percentage scale. This is often gathered from surveys or audience feedback platforms.\n\nStory (Factor):\n\nThis could refer to the primary theme or storyline category of the movie (e.g., Hero’s Journey, Revenge, Coming of Age). It might represent narrative tropes or common plot structures.\n\nGenre (Factor):\n\nThe type of movie based on its theme or tone (e.g., Action, Comedy, Drama, Horror). These are pre-defined categories common to film classification.\n\nTheatersOpenWeek (Numeric):\n\nThe number of theaters where the movie was shown during its opening week. A larger number indicates a wide release, while a smaller number could suggest a limited release.\n\nBOAverageOpenWeek (Numeric):\n\nBox Office Average for the opening week, likely representing the average box office earnings per theater during that week.\n\nDomesticGross (Numeric):\n\nThe total amount of money the movie earned in the domestic market (e.g., within the U.S.) during its entire run, expressed in millions of USD.\n\nForeignGross (Numeric):\n\nThe total box office earnings from international markets outside the domestic territory, in millions of USD.\n\nWorldGross (Numeric):\n\nThe combined total of DomesticGross and ForeignGross, representing global earnings for the movie.\n\nBudget (Numeric):\n\nThe total cost of producing the movie, including marketing and production expenses, in millions of USD.\n\nProfitability (Numeric):\n\nrepresent how many times the movie’s global earnings (WorldGross) exceeded the production budget (Budget).\n\nOpeningWeekend (Numeric):\n\nThe amount of money the movie made during its opening weekend, in millions of USD. This figure is often used to predict the overall success of a film.\n\n\nObtaining only the numberic values:\n\nmovies_quant &lt;- movies %&gt;%\n  drop_na() %&gt;%\n  select(where(is.numeric))\nmovies_quant\n\n    RottenTomatoes AudienceScore TheatersOpenWeek BOAverageOpenWeek\n1               67            65             2408              5511\n2               68            58             3321             15829\n3               44            38             3049             10365\n4               96            92             4375             38672\n5               90            77             2918              8995\n6               93            84              944              6177\n7               75            91             2534             10278\n8               35            58             3615             23775\n9               69            73             2756              6860\n10              69            72             3040              9310\n11              49            57             3018              6512\n12              26            68             4061             34012\n13              35            67             4088             23937\n14              56            52             2994              8469\n15              71            73             3826             10252\n16              82            78             3379             10492\n17              83            87             3648             15024\n18              23            31             3328              2615\n19              23            50             3395             10489\n20              93            93             2458              3517\n21              93            79             2886              3929\n22              82            80             3925             12142\n23              55            57             3043              7183\n24              85            76             2199              4761\n25              71            68             2926              6364\n26              34            61             4155             21697\n27              61            56             3155              5715\n28              92            81             2961              5002\n29              93            86             3448              8672\n30              37            54             1719              2955\n31              30            39             2787              3390\n32              60            79             3703             10704\n33              38            55             1552              2470\n34              47            63             3167              7500\n35              47            54             3339              5524\n36              38            55             3122              3860\n37              60            72             2817              5979\n38              35            50             3417             10411\n39              77            80             3955             16618\n40              26            50             3579             10490\n41              78            81             3020              6326\n42              38            56             4115             16072\n43              22            40             3295              3534\n44              19            63             3548              8601\n45              78            75             3715             17512\n46              20            43             2985              4955\n47              71            71             3549              4383\n48              72            67             2840              7450\n49               4            29             2534              5921\n50              46            79             2214              4789\n51              58            81             3440              7942\n52              72            70             2802              4655\n53              36            59             3112             10349\n54              58            57             3305              5656\n55              32            57             3154              6167\n56              84            81             3507              5461\n57               4            46             3118              3504\n58              35            44             2950              4588\n59              10            32             2816              3769\n60              76            70             1826              5427\n61              84            63             3222              6935\n62              87            88             3641             15134\n63              83            76             3952              8623\n64              14            42             3482              5763\n65              71            67             2535              4880\n66              11            41             3030              4622\n67              95            89             2993              6516\n68              38            50             2473              3014\n69              44            47             3584              9335\n70              84            82             2707              4879\n71              88            69             3917              9722\n72              24            48             3017              2875\n73              34            46             2973              4405\n74              84            61                4             93230\n75              19            50             1952              5047\n76              68            61             3367              7135\n77              97            87             3440              8500\n78              16            25             2806              2995\n79              28            55             2614              3982\n80              24            50             3002              1806\n81              43            48             2888              4616\n82              24            53             2913              4645\n83              17            37             2864              5221\n84              53            52             2703              4226\n85              59            37             2760              3089\n86              75            68             3114              2477\n87              26            49             3276              3731\n88              91            79             2405              3267\n89              27            48             3816             13935\n90              23            48             3033              6284\n91              39            43             2296              3782\n92              44            50             3750              9715\n93               4            59             3438              7273\n94              23            31             2940              6060\n95              83            93             1869              2805\n96              83            84              247              7174\n97              41            59             3606              5889\n98               7            38             2661              3055\n99              25            48             2986              3132\n100             36            52             2996              2835\n101             45            38             2290              2265\n102             92            82             3376              3537\n103             56            65              707              4960\n104             22            34             3015              3324\n105             26            36             2769              3380\n106             50            48             2273              2259\n107             46            66              265              3856\n108             66            55              106              6111\n109             62            57               22              4890\n110             36            43             3117              2218\n111             38            62             2150              1513\n    DomesticGross ForeignGross WorldGross Budget Profitability OpeningWeekend\n1           54.01        43.00     97.009    1.5    64.6726667          13.27\n2          103.66        98.24    201.897    5.0    40.3794000          52.57\n3          100.29       115.90    216.196   20.0    10.8098000          31.60\n4          381.01       947.10   1328.111  125.0    10.6248880         169.19\n5          169.11       119.28    288.382   32.5     8.8732923          26.25\n6           56.18        83.00    139.177   17.0     8.1868824           5.83\n7          169.22        30.10    199.324   25.0     7.9729600          26.04\n8          254.46       327.00    581.464   80.0     7.2683000          85.95\n9           79.25        82.60    161.849   27.0     5.9944074          18.91\n10         117.54        92.10    209.638   35.0     5.9896571          28.30\n11          70.60        77.10    147.700   25.0     5.9080000          19.70\n12         260.80       374.00    634.800  110.0     5.7709091         138.12\n13         352.39       770.81   1123.195  195.0     5.7599744          97.85\n14          99.97        94.00    193.967   36.0     5.3879722          25.36\n15         143.62       341.02    484.634   90.0     5.3848222          39.23\n16         127.00       132.71    259.713   50.0     5.1942600          35.45\n17         176.70       304.52    481.226   93.0     5.1744731          54.81\n18          17.69         7.88     25.562    5.0     5.1124000           8.70\n19         142.61       419.54    562.158  110.0     5.1105273          35.61\n20          34.90         1.62     36.511    8.0     4.5638750           8.64\n21          34.68        32.33     67.007   15.0     4.4671333          11.34\n22         165.25       497.78    663.024  150.0     4.4201600          47.66\n23          63.69        67.10    130.786   30.0     4.3595333          21.86\n24          40.49        13.70     54.194   12.5     4.3355200          10.47\n25          55.80        93.74    149.541   35.0     4.2726000          18.62\n26         241.07       802.80   1043.871  250.0     4.1754840          90.15\n27          42.59       115.30    157.887   40.0     3.9471750          18.03\n28          54.71        68.57    123.278   32.0     3.8524375          14.81\n29         197.80       336.70    534.500  145.0     3.6862069          29.55\n30          13.84        41.40     55.241   15.0     3.6827333           5.08\n31          23.20        85.40    108.600   30.0     3.6200000           9.40\n32         179.04       261.00    440.040  125.0     3.5203200          39.63\n33           8.31       149.63    157.939   45.0     3.5097556           3.83\n34          52.70        19.72     72.416   21.0     3.4483810          23.75\n35          68.22       119.14    187.355   55.0     3.4064545          18.45\n36          36.49        90.90    127.393   40.0     3.1848250          12.05\n37          58.71        58.39    117.094   38.0     3.0814211          16.84\n38          83.55       128.27    211.818   70.0     3.0259714          35.57\n39         181.03       267.48    448.512  150.0     2.9900800          65.72\n40         108.09        75.87    183.953   63.0     2.9198889          37.54\n41          84.34        58.50    142.841   50.0     2.8568200          19.10\n42         191.45       360.40    551.850  200.0     2.7592500          66.14\n43          38.54        35.54     74.080   27.0     2.7437037          11.64\n44         103.03       111.92    214.945   80.0     2.6868125          30.51\n45         176.65       191.75    368.404  140.0     2.6314571          65.06\n46          33.00        63.00     96.000   37.0     2.5945946          14.80\n47          51.16        10.90     62.053   24.0     2.5855417          15.56\n48          62.50        65.37    127.868   50.2     2.5471713          21.16\n49          37.30         3.19     40.492   16.0     2.5307500          15.00\n50          43.85         0.41     44.267   18.0     2.4592778          10.60\n51          83.61       186.20    269.811  110.0     2.4528273          27.32\n52          37.41        60.57     97.983   40.0     2.4495750          13.04\n53          80.49       102.00    182.485   75.0     2.4331333          32.21\n54          38.18        58.96     97.137   40.0     2.4284250          18.69\n55          55.10        89.40    144.500   60.0     2.4083333          19.45\n56          71.08        16.86     87.947   37.0     2.3769459          19.15\n57          28.07        54.00     82.069   35.0     2.3448286          10.93\n58          45.06        38.10     83.160   36.0     2.3100000          13.54\n59          24.80        66.80     91.600   40.0     2.2900000          10.60\n60          31.18        14.25     45.429   20.0     2.2714500           9.91\n61          75.64        59.80    135.443   60.0     2.2573833          22.40\n62         146.41       207.22    353.623  160.0     2.2101437          55.10\n63         142.09       142.30    284.386  130.0     2.1875846          34.08\n64          80.36        89.94    170.301   80.0     2.1287625          20.07\n65          40.26        23.52     63.781   30.0     2.1260333          12.37\n66          37.66        51.50     89.162   42.0     2.1229048          14.01\n67          74.21        27.90    102.109   50.0     2.0421800          19.50\n68          23.18        16.48     39.664   20.0     1.9832000           7.45\n69          98.80       129.00    227.800  120.0     1.8983333          33.50\n70          58.01        17.00     75.009   40.0     1.8752250          13.21\n71         123.26       121.90    245.154  135.0     1.8159556          38.08\n72          20.25       111.90    132.147   75.0     1.7619600           8.67\n73          35.61        16.80     52.408   30.0     1.7469333          13.10\n74          13.30        41.00     54.303   32.0     1.6969687           0.37\n75          27.87         0.97     28.833   17.0     1.6960588           9.85\n76          74.50        47.00    121.504   75.0     1.6200533          24.03\n77          66.63         5.80     72.426   45.0     1.6094667          29.24\n78          18.88        19.83     38.702   25.0     1.5480800           8.40\n79          36.67        24.30     60.965   40.0     1.5241250          10.41\n80          14.01        16.42     30.426   20.0     1.5213000           5.42\n81          37.05         3.49     40.546   28.0     1.4480714          13.33\n82          37.08        33.75     70.833   52.0     1.3621731          13.53\n83          29.14        49.17     78.308   60.0     1.3051333          14.95\n84          29.20        22.00     51.200   40.0     1.2800000          11.40\n85          24.05         7.50     31.546   25.0     1.2618400           8.53\n86          18.30        18.80     37.102   30.0     1.2367333           7.71\n87          33.04        12.70     45.735   40.0     1.1433750          12.22\n88          26.69         6.46     33.152   30.0     1.1050667           7.86\n89         116.60       103.25    219.851  200.0     1.0992550          53.17\n90          36.39        53.40     89.792   82.0     1.0950244          19.06\n91          19.49         7.63     27.121   25.0     1.0848400           8.68\n92         100.24        74.58    174.821  163.0     1.0725215          36.43\n93          68.91        15.00     83.911   79.0     1.0621646          25.00\n94          48.50        21.20     69.700   70.0     0.9957143          17.80\n95          13.66         9.40     23.057   25.0     0.9222800           5.24\n96           5.31         2.95      8.258   10.0     0.8258000           1.75\n97          57.31        49.20    106.507  135.0     0.7889407          21.24\n98          21.30        17.20     38.502   50.0     0.7700400           8.13\n99          25.12        27.84     52.961   70.0     0.7565857           9.35\n100         16.93        10.50     27.428   38.0     0.7217895           8.49\n101         10.72        18.21     28.931   45.0     0.6429111           5.19\n102         33.70        57.50     91.203  150.0     0.6080200          12.07\n103         11.54         2.67     14.211   25.0     0.5684400           3.51\n104         21.30        27.50     48.795   90.0     0.5421667          10.02\n105         21.60         3.26     24.856   49.9     0.4981162           9.36\n106         13.07         8.48     21.552   45.0     0.4789333           5.14\n107          4.46         9.73     14.190   30.0     0.4730000           1.02\n108          4.40         0.40      4.800   15.0     0.3200000           0.64\n109          0.97         5.40      6.370   21.0     0.3033333           0.11\n110         21.39        17.60     38.992  150.0     0.2599467           6.91\n111          7.17         0.24      7.410   41.0     0.1807317           3.25\n\n\n\nCorrelation between domestic gross and world gross.\n\nmovies %&gt;%\n  gf_point(DomesticGross ~ WorldGross) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Gross Earnings: Domestics vs World\"\n  )\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_lm()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nThere appears to be a +ve correlation between Domestic gross and world gross.\n\n\nCorrelation between Profitability and Opening Week.\n\nmovies %&gt;%\n  gf_point(Profitability ~ OpeningWeekend) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\"\n  )\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_lm()`).\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nOpeningWeek and Profitability are also related in a linear way. There are just two movies which have been extremely profitable\n\n\nCorrelation between RottenTomatoes and AudienceScore.\n\nmovies %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Tomatoes vs Audience\"\n  )\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_lm()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nRotten tomatoes and Audience score have a +ve correlation.\n\nWe can split some of the scatter plots using one or other of the Qual variables.\n\n\n\nCorrelation between RottenTomatoes and AudienceScore seprated by Genre.\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore,\n    color = ~Genre\n  ) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Trends by Genre\"\n  )\n\n\n\n\n\n\n\n\n\n\nQuantifying Correlation\nGGally::ggpairs() : This function creates a matrix of plots for pairwise comparisons of the selected variables.\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"RottenTomatoes\", \"AudienceScore\", \"DomesticGross\", \"ForeignGross\"\n  ),\n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n\n  progress = FALSE,\n  # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  title = \"Movies Data Correlations Plot #1\"\n)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAudienceScore and RottenTomatoes are well correlated with a score of 8.3. Domestic Gross and Foreign Gross is also well correlated with a score of 8.73. Nothing else is as strongly related.\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"Budget\", \"Profitability\", \"DomesticGross\", \"ForeignGross\"\n  ),\n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n\n  progress = FALSE,\n  # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  title = \"Movies Data Correlations Plot #2\"\n)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAlthough not too much, you do see a correlation score of 0.65 between budget and Domestic gross and 0.67 between budget and Foreign Gross.\nBudget and Profitability seem to have a slight negative correlation.\n\n\nDoing a Correlation Test\nWe try and find out how certain we can be about the correlation score by doing this.\n\nWhat does this data tell me?\nestimate: This is the correlation coefficient (often denoted as “r”) between Profitability and Budget in your dataset.\nconfidence level low and confidence level high: These are the lower and upper bounds of uncertainity for the correlation coefficient.\np.value: The p-value shows the probability that the observed correlation happened by chance, assuming that there is no true correlation.\nMovie Profitability vs Budget\n\nmosaic::cor_test(Profitability ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Profitability vs Budget\"\n  )\n\n\nMovie Profitability vs Budget\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-0.08\n-0.96\n0.34\n132\n-0.25\n0.09\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nHere, the correlation coefficient is -0.08 - a slight negative correlation, but we observe that the uncertainty levels lie within a large range. We also see that that p-value is high. In other words, there is no clear linear relationship between Profitability and Budget in your data. We, I don’t have enough evidence to make strong conclusions about a consistent negative (or positive) relationship. Increasing or decreasing a movie’s budget does not reliably predict its profitability.\nMovie’s Domestic Gross vs Budget\n\nmosaic::cor_test(DomesticGross ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Domestic Gross vs Budget\"\n  )\n\n\nMovie Domestic Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n0.7\n11.06\n0\n131\n0.6\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nHere, the correlation coefficient is 0.7 - a positive correlation, and we observe that the uncertainty levels lie within a short range i.e. there’s low uncertainty around the estimate. We also see that that p-value is 0, concluding that this inference could not have been by chance making it statistically significant. Therefore, Budget has a strong positive effect on Profitability. Movies with higher budgets tend to have significantly higher Domestic Gross numbers, based on this data. This relationship is statistically significant, reliable, and consistent.\nMovie Foreign Gross vs Budget\n\nmosaic::cor_test(ForeignGross ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Foreign Gross vs Budget\"\n  )\n\n\nMovie Foreign Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n0.69\n10.22\n0\n118\n0.58\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nHas similar inferences as the previous test.\n\n\n\nThe ErrorBar Plot for Correlations"
  },
  {
    "objectID": "posts/day-4/index.html",
    "href": "posts/day-4/index.html",
    "title": "Day 4",
    "section": "",
    "text": "Quant and Qual Variable Graphs and their Siblings (god knows what that means).\nA histogram is a graphical representation of the distribution of continuous numerical data, where data is grouped into ranges (or bins), and the frequency of data points in each bin is displayed using bars. Quant variables will be present on the x-axis and the histogram shows us how frequently different values occur for that variable by showing counts/frequencies on the y-axis. we use “gf_histogram” for this.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\n##\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact"
  },
  {
    "objectID": "posts/day-4/index.html#introduction",
    "href": "posts/day-4/index.html#introduction",
    "title": "Day 4",
    "section": "",
    "text": "Quant and Qual Variable Graphs and their Siblings (god knows what that means).\nA histogram is a graphical representation of the distribution of continuous numerical data, where data is grouped into ranges (or bins), and the frequency of data points in each bin is displayed using bars. Quant variables will be present on the x-axis and the histogram shows us how frequently different values occur for that variable by showing counts/frequencies on the y-axis. we use “gf_histogram” for this.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\n##\nlibrary(crosstable)\n\n\nAttaching package: 'crosstable'\n\nThe following object is masked from 'package:purrr':\n\n    compact"
  },
  {
    "objectID": "posts/day-4/index.html#diamonds-data-set",
    "href": "posts/day-4/index.html#diamonds-data-set",
    "title": "Day 4",
    "section": "Diamonds data-set:",
    "text": "Diamonds data-set:\n\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\nConsidering the fact that all qualitative data is already ordered and factored, we don’t mutate any values here.\n\n\nskim(diamonds)\n\n\nData summary\n\n\nName\ndiamonds\n\n\nNumber of rows\n53940\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncut\n0\n1\nTRUE\n5\nIde: 21551, Pre: 13791, Ver: 12082, Goo: 4906\n\n\ncolor\n0\n1\nTRUE\n7\nG: 11292, E: 9797, F: 9542, H: 8304\n\n\nclarity\n0\n1\nTRUE\n8\nSI1: 13065, VS2: 12258, SI2: 9194, VS1: 8171\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncarat\n0\n1\n0.80\n0.47\n0.2\n0.40\n0.70\n1.04\n5.01\n▇▂▁▁▁\n\n\ndepth\n0\n1\n61.75\n1.43\n43.0\n61.00\n61.80\n62.50\n79.00\n▁▁▇▁▁\n\n\ntable\n0\n1\n57.46\n2.23\n43.0\n56.00\n57.00\n59.00\n95.00\n▁▇▁▁▁\n\n\nprice\n0\n1\n3932.80\n3989.44\n326.0\n950.00\n2401.00\n5324.25\n18823.00\n▇▂▁▁▁\n\n\nx\n0\n1\n5.73\n1.12\n0.0\n4.71\n5.70\n6.54\n10.74\n▁▁▇▃▁\n\n\ny\n0\n1\n5.73\n1.14\n0.0\n4.72\n5.71\n6.54\n58.90\n▇▁▁▁▁\n\n\nz\n0\n1\n3.54\n0.71\n0.0\n2.91\n3.53\n4.04\n31.80\n▇▁▁▁▁\n\n\n\n\n\n\n\ncarat: weight of the diamond 0.2-5.01\ndepth: depth total depth percentage 43-79\ntable: width of top of diamond relative to widest point 43-95\nprice: price in US dollars $326-$18,823\nx: length in mm 0-10.74\ny: width in mm 0-58.9\nz(dbl): depth in mm 0-31.8\n\n\n\nThere are no missing values for any variable, all are complete with 54K entries.\n\n\nMy first histogram!!!!\n\nPlotting diamond prices.\n\n## if i want i can not specigy the bins too. \ngf_histogram(~price,\n  data = diamonds,\n  bins = 100\n) %&gt;%\n  gf_labs(\n    title = \"Plot 1A: Diamond Prices\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\nWe can infer that while a large number of diamonds are priced relatively low, there are also a significant number of diamonds that are priced very high.\n\n\nWhat is the distribution of the predictor variable carat?\n\ndiamonds %&gt;%\n  gf_histogram(~carat,\n    bins = 100\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2B: Carats of Diamonds\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\nWe can infer that there must be some, very few, diamonds of very high carat value while there a few carat values that appear to be more than common! People very commonly buy diamonds of 1 carat and a little less frequently 0.5, 1.5 and 2.\n\n\nDoes a price distribution vary based upon type of cut?\n\nfrom what i observe, the fill acts as a stack here, although it is prices that are represented in the historgram, we able to observe what portion of each bin is occupied by each of the cuts\n\n\ngf_histogram(~price, fill = ~cut, data = diamonds, bins=100) %&gt;%\n  gf_labs(title = \"Plot 3A: Diamond Prices\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price|cut, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Plot 3C: Prices by Filled and Facetted by Cut\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45, ## the angle at which the word should be placed.\n      hjust = 1 ## the incremanting space from the x axis\n    )\n  ))\n\n\n\n\n\n\n\n\nusing this, we can observe the price range of each individual cut as different graphs, but very low values (in comparison to high values of ideal) such as those in fair and good. We can make them have ranges of values in y axis based on their individual values by setting scales to be free y.\n\n## the nrow, defines the number of rows \ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~cut, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Prices Filled and Facetted by Cut\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nPrice ranges are the same regardless of cut and so that must not be the only parameter in dermeining the price\n\n\nDoes a price distribution vary based upon type of clarity?\n\ngf_histogram(~price, fill = ~clarity, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot 3A: Diamond Prices spereated by clarity\")\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~clarity, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~clarity) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Prices Filled and Facetted by Clarity\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\n\n## the nrow, defines the number of rows \ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~clarity, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~clarity, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Prices Filled and Facetted by Clarity\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nPrice ranges are the same regardless of clarity and so that must not be the only parameter in determining the price but SI1 appease to have the most in high prices\n\n\nDoes a price distribution vary based upon type of colour?\n\ngf_histogram(~price, fill = ~color, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot: Diamond Prices spereated by color\")\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~color, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~color) %&gt;%\n  gf_labs(\n    title = \"Plot: Prices Filled and Facetted by Color\",\n    subtitle = \"Free y-scale\",\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~color, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~color, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot: Prices Filled and Facetted by Color\",\n    subtitle = \"Free y-scale\",\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))"
  },
  {
    "objectID": "posts/day-4/index.html#the-race-data-set",
    "href": "posts/day-4/index.html#the-race-data-set",
    "title": "Day 4",
    "section": "The Race data-set",
    "text": "The Race data-set\n\nrace_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/race.csv\")\n\nRows: 1207 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): event, race, city, country, participation\ndbl  (6): race_year_id, distance, elevation_gain, elevation_loss, aid_statio...\ndate (1): date\ntime (1): start_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrace_df\n\n# A tibble: 1,207 × 13\n   race_year_id event    race  city  country date       start_time participation\n          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;     &lt;time&gt;     &lt;chr&gt;        \n 1        68140 Peak Di… Mill… Cast… United… 2021-09-03 19:00      solo         \n 2        72496 UTMB®    UTMB® Cham… France  2021-08-27 17:00      Solo         \n 3        69855 Grand R… Ultr… viel… France  2021-08-20 05:00      solo         \n 4        67856 Persenk… PERS… Asen… Bulgar… 2021-08-20 18:00      solo         \n 5        70469 Runfire… 100 … uluk… Turkey  2021-08-20 18:00      solo         \n 6        66887 Swiss A… 160KM Müns… Switze… 2021-08-15 17:00      solo         \n 7        67851 Salomon… Salo… Foll… Norway  2021-08-14 07:00      solo         \n 8        68241 Ultra T… 160KM Spa   Belgium 2021-08-14 07:00      solo         \n 9        70241 Québec … QMT-… Beau… Canada  2021-08-13 22:00      solo         \n10        69945 Bunketo… BBUT… LIND… Sweden  2021-08-07 10:00      solo         \n# ℹ 1,197 more rows\n# ℹ 5 more variables: distance &lt;dbl&gt;, elevation_gain &lt;dbl&gt;,\n#   elevation_loss &lt;dbl&gt;, aid_stations &lt;dbl&gt;, participants &lt;dbl&gt;\n\nrank_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/ultra_rankings.csv\")\n\nRows: 137803 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): runner, time, gender, nationality\ndbl (4): race_year_id, rank, age, time_in_seconds\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrank_df\n\n# A tibble: 137,803 × 8\n   race_year_id  rank runner      time    age gender nationality time_in_seconds\n          &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;\n 1        68140     1 VERHEUL Ja… 26H …    30 M      GBR                   95725\n 2        68140     2 MOULDING J… 27H …    43 M      GBR                   97229\n 3        68140     3 RICHARDSON… 28H …    38 M      GBR                  103747\n 4        68140     4 DYSON Fiona 30H …    55 W      GBR                  111217\n 5        68140     5 FRONTERAS … 32H …    48 W      GBR                  117981\n 6        68140     6 THOMAS Lei… 32H …    31 M      GBR                  118000\n 7        68140     7 SHORT Debo… 33H …    55 W      GBR                  120601\n 8        68140     8 CROSSLEY C… 33H …    40 W      GBR                  120803\n 9        68140     9 BUTCHER Ke… 34H …    47 M      GBR                  125656\n10        68140    10 Hendry Bill 34H …    29 M      GBR                  125979\n# ℹ 137,793 more rows\n\n\n\nglimpse(race_df)\n\nRows: 1,207\nColumns: 13\n$ race_year_id   &lt;dbl&gt; 68140, 72496, 69855, 67856, 70469, 66887, 67851, 68241,…\n$ event          &lt;chr&gt; \"Peak District Ultras\", \"UTMB®\", \"Grand Raid des Pyréné…\n$ race           &lt;chr&gt; \"Millstone 100\", \"UTMB®\", \"Ultra Tour 160\", \"PERSENK UL…\n$ city           &lt;chr&gt; \"Castleton\", \"Chamonix\", \"vielle-Aure\", \"Asenovgrad\", \"…\n$ country        &lt;chr&gt; \"United Kingdom\", \"France\", \"France\", \"Bulgaria\", \"Turk…\n$ date           &lt;date&gt; 2021-09-03, 2021-08-27, 2021-08-20, 2021-08-20, 2021-0…\n$ start_time     &lt;time&gt; 19:00:00, 17:00:00, 05:00:00, 18:00:00, 18:00:00, 17:0…\n$ participation  &lt;chr&gt; \"solo\", \"Solo\", \"solo\", \"solo\", \"solo\", \"solo\", \"solo\",…\n$ distance       &lt;dbl&gt; 166.9, 170.7, 167.0, 164.0, 159.9, 159.9, 163.8, 163.9,…\n$ elevation_gain &lt;dbl&gt; 4520, 9930, 9980, 7490, 100, 9850, 5460, 4630, 6410, 31…\n$ elevation_loss &lt;dbl&gt; -4520, -9930, -9980, -7500, -100, -9850, -5460, -4660, …\n$ aid_stations   &lt;dbl&gt; 10, 11, 13, 13, 12, 15, 5, 8, 13, 23, 13, 5, 12, 15, 0,…\n$ participants   &lt;dbl&gt; 150, 2300, 600, 150, 0, 300, 0, 200, 120, 100, 300, 50,…\n\n\n\nglimpse(rank_df)\n\nRows: 137,803\nColumns: 8\n$ race_year_id    &lt;dbl&gt; 68140, 68140, 68140, 68140, 68140, 68140, 68140, 68140…\n$ rank            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, NA, NA, NA,…\n$ runner          &lt;chr&gt; \"VERHEUL Jasper\", \"MOULDING JON\", \"RICHARDSON Phill\", …\n$ time            &lt;chr&gt; \"26H 35M 25S\", \"27H 0M 29S\", \"28H 49M 7S\", \"30H 53M 37…\n$ age             &lt;dbl&gt; 30, 43, 38, 55, 48, 31, 55, 40, 47, 29, 48, 47, 52, 49…\n$ gender          &lt;chr&gt; \"M\", \"M\", \"M\", \"W\", \"W\", \"M\", \"W\", \"W\", \"M\", \"M\", \"M\",…\n$ nationality     &lt;chr&gt; \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\"…\n$ time_in_seconds &lt;dbl&gt; 95725, 97229, 103747, 111217, 117981, 118000, 120601, …\n\n\n\nmosaic::favstats returns a data frame with several common summary statistics, such as:\n\nmin: Minimum value\nQ1: First quartile (25th percentile)\nmedian: Median (50th percentile)\nQ3: Third quartile (75th percentile)\nmax: Maximum value\nmean: Arithmetic mean\nsd: Standard deviation\nn: Number of non-missing observations\nmissing: Number of missing values\nIQR: Interquartile range (Q3 - Q1)\n\n\n\nrace_df %&gt;%\n  favstats(~distance, data = .)\n\n min    Q1 median     Q3   max     mean       sd    n missing\n   0 160.1  161.5 165.15 179.1 152.6187 39.87864 1207       0\n\n\n\nrace_df %&gt;%\n  favstats(~participants, data = .)\n\n min Q1 median  Q3  max     mean       sd    n missing\n   0  0     21 150 2900 120.4872 281.8337 1207       0\n\n\n\nrank_df %&gt;%\n  drop_na() %&gt;%\n  favstats(time_in_seconds ~ gender, data = .)\n\n  gender  min      Q1 median       Q3    max     mean       sd      n missing\n1      M 3600 96536.5 115845 149761.5 288000 123271.1 37615.42 101643       0\n2      W 9191 96695.0 107062 131464.0 296806 117296.5 34604.26  18341       0\n\n\n\nOn occasion we may need to see summaries of several Quant variables, over levels of Qual variables. This is where the package crosstable is so effective. Therefore, crosstable is useful when you need to generate summary statistics of quantitative (numeric) variables, broken down by levels of qualitative (categorical) variables.\n\n\ncrosstable(time_in_seconds + age ~ gender, data = rank_df) %&gt;%\n  crosstable::as_flextable()\n\nlabelvariablegenderMWNAtime_in_secondsMin / Max3600.0 / 2.9e+059191.0 / 3.0e+058131.0 / 2.2e+05Med [IQR]1.2e+05 [9.7e+04;1.5e+05]1.1e+05 [9.7e+04;1.3e+05]1.2e+05 [9.9e+04;1.5e+05]Mean (std)1.2e+05 (3.8e+04)1.2e+05 (3.5e+04)1.2e+05 (4.4e+04)N (NA)101643 (15073)18341 (2716)28 (2)ageMin / Max0 / 133.00 / 81.029.0 / 59.0Med [IQR]47.0 [40.0;53.0]45.0 [39.0;52.0]40.5 [36.0;50.5]Mean (std)46.4 (10.2)45.3 (9.7)41.7 (9.0)N (NA)116716 (0)21057 (0)30 (0)\n\n\nMen participating are generally older than women. When it comes to time in seconds, while the overall central tendency (mean) remains consistent, the distribution of the data has some variation as shown by (std) and (median) but not the average - it’s the same.\n\nWhich countries host the maximum number of races? Which countries send the maximum number of participants??\n\nrace_df %&gt;%\n  count(country) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 61 × 2\n   country            n\n   &lt;chr&gt;          &lt;int&gt;\n 1 United States    438\n 2 United Kingdom   110\n 3 France            56\n 4 Australia         46\n 5 Sweden            46\n 6 China             45\n 7 Canada            32\n 8 Spain             27\n 9 Japan             24\n10 Poland            23\n# ℹ 51 more rows\n\n\n\nrank_df %&gt;%\n  count(nationality) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 133 × 2\n   nationality     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 USA         47259\n 2 FRA         28905\n 3 GBR         11076\n 4 JPN          6729\n 5 ESP          5478\n 6 CHN          4744\n 7 CAN          2822\n 8 ITA          2794\n 9 SWE          2293\n10 AUS          1683\n# ℹ 123 more rows\n\n\nThe United states hosts the most number of games and send the most number of players.\n\n\nWhich country wins the most?\n\nrank_df %&gt;%\n  filter(rank %in% c(1, 2, 3)) %&gt;%\n  count(nationality) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 69 × 2\n   nationality     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 USA          1240\n 2 GBR           347\n 3 FRA           210\n 4 AUS           140\n 5 CAN           132\n 6 CHN           128\n 7 SWE           124\n 8 ESP           113\n 9 JPN            94\n10 ITA            79\n# ℹ 59 more rows\n\n\nTo no surprise, United states wins the most too. Would it be the same case if compared the ratio of players to wins?\n\n\nAnalyze the nationality of the top 10 participants in the longest races\n\n\nslice() allows you to select, remove, and duplicate rows. slice_min() and slice_max() select rows with the smallest or largest values of a variable.\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions.\nleft_join() is a function from the dplyr package used to combine two data frames by joining them based on a common column (or columns).\n\n\n\nlongest_races &lt;- race_df %&gt;%\n  slice_max(n = 5, order_by = distance) # Longest distance races\nlongest_races\n\n# A tibble: 6 × 13\n  race_year_id event     race  city  country date       start_time participation\n         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;     &lt;time&gt;     &lt;chr&gt;        \n1        68776 Ultra To… Ut4M… Gren… France  2021-07-16 18:00      Solo         \n2        55551 Ultra Tr… Inth… Chom… Thaila… 2020-02-14 10:00      solo         \n3         7484 Le TREG®… LE T… Fada  Chad    2015-02-06 00:00      solo         \n4         7594 THE GREA… 100 … Pato… Austra… 2014-09-13 00:00      Solo         \n5        71066 ULTRA 01  Ultr… Oyon… France  2021-07-09 18:00      solo         \n6        23565 EstrelAç… Estr… Penh… Portug… 2017-10-06 18:00      Solo         \n# ℹ 5 more variables: distance &lt;dbl&gt;, elevation_gain &lt;dbl&gt;,\n#   elevation_loss &lt;dbl&gt;, aid_stations &lt;dbl&gt;, participants &lt;dbl&gt;\n\nlongest_races %&gt;%\n  left_join(., rank_df, by = \"race_year_id\") %&gt;% # total participants in longest 4 races\n  filter(rank %in% c(1:10)) %&gt;% # Top 10 ranks\n  count(nationality) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 9 × 2\n  nationality     n\n  &lt;chr&gt;       &lt;int&gt;\n1 FRA            26\n2 AUS             9\n3 POR             8\n4 THA             8\n5 BEL             1\n6 BRA             1\n7 ESP             1\n8 MAS             1\n9 RUS             1\n\n\n\nWe get 2 tables - one with the joined data set, the other with the nationality and count of the amount of wins (top 10 rank among the longest races).\n\nThese quantities show that even though USA has the most number of wins, for the longest races, france has the most participents among everyone the top 10 rank.\n\n\nWhat is the distribution of the finishing times?\n\nrank_df %&gt;%\n  gf_histogram(~time_in_seconds, bins = 75) %&gt;%\n  gf_labs(title = \"Histogram of Race Times\")\n\nWarning: Removed 17791 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nMost people finished the race at 1e+05. The histogram shows three bumps (is this a result of the difference in distance?)\n\n\nWhat is the distribution of race distances?\n\nrace_df %&gt;%\n  gf_histogram(~distance, bins = 50) %&gt;%\n  gf_labs(title = \"Histogram of Race Distances\")\n\n\n\n\n\n\n\n\nHow are there multiple races at 0 distance, is this a glitch in the data? There are very few races between 0-150. Most races seem to be set at a distance from 150 -180.\n\nrace_df %&gt;%\n  filter(distance == 0)\n\n# A tibble: 74 × 13\n   race_year_id event    race  city  country date       start_time participation\n          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;     &lt;time&gt;     &lt;chr&gt;        \n 1        64771 The Old… 100m… Hanm… New Ze… 2021-05-14 10:00      solo         \n 2        71220 Run Lov… 100M  &lt;NA&gt;  United… 2021-02-26 00:00      solo         \n 3        67160 IDAHO M… 100 … &lt;NA&gt;  United… 2020-09-12 00:00      solo         \n 4        67713 Pine cr… 100M… Well… PA, Un… 2020-09-12 00:00      solo         \n 5        51777 Chiemga… 100 … Berg… Germany 2020-07-31 13:00      Solo         \n 6        66413 Palisad… Moos… Irwin United… 2020-07-17 05:00      solo         \n 7        62593 Run Lov… 100M  &lt;NA&gt;  United… 2020-02-28 00:00      solo         \n 8        50097 The Gre… The … Hanm… New Ze… 2020-01-17 07:00      solo         \n 9        65861 Loup Ga… 100M  Vill… LA, Un… 2019-12-14 00:00      solo         \n10        59415 RIO DEL… 100 … &lt;NA&gt;  United… 2019-11-07 00:00      solo         \n# ℹ 64 more rows\n# ℹ 5 more variables: distance &lt;dbl&gt;, elevation_gain &lt;dbl&gt;,\n#   elevation_loss &lt;dbl&gt;, aid_stations &lt;dbl&gt;, participants &lt;dbl&gt;\n\n\nEven though there are so many races registerd as 100 mile races, non of the distances are 100 in the histogram.\n\n\nWhat is the distribution of finishing times for race distance around 150 faceted by time of day?\nA count of start times:\n\nrace_times &lt;- race_df %&gt;%\n  count(start_time) %&gt;%\n  arrange(desc(n))\nrace_times\n\n# A tibble: 39 × 2\n   start_time     n\n   &lt;time&gt;     &lt;int&gt;\n 1 00:00        513\n 2 06:00        114\n 3 08:00         63\n 4 10:00         60\n 5 07:00         58\n 6 18:00         50\n 7 05:00         48\n 8 12:00         38\n 9 04:00         30\n10 09:00         27\n# ℹ 29 more rows\n\n\nWe section a day into different groups of time: example morning, noon, evening and create a new column start_day_time with this information; since might and post midnight can be categorized as the same, we use fact_collapse to combine it to be the same. left_join() is used to merge race_start_factor with another data frame rank_df based on the race_year_id column, which is common between both. drop_na() removes any rows where time_in_seconds is NA (i.e., missing values). This ensures the plot only uses races with valid time data. hms- hour minute second.\n\nrace_start_factor &lt;- race_df %&gt;%\n  filter(distance == 0) %&gt;% # Races that actually took place\n  mutate(\n    ## start day time is a new column you are creating based on the values in\n    start_day_time =\n      case_when(\n        start_time &gt; hms(\"02:00:00\") &\n          start_time &lt;= hms(\"06:00:00\") ~ \"early_morning\",\n        start_time &gt; hms(\"06:00:01\") &\n          start_time &lt;= hms(\"10:00:00\") ~ \"late_morning\",\n        start_time &gt; hms(\"10:00:01\") &\n          start_time &lt;= hms(\"14:00:00\") ~ \"mid_day\",\n        start_time &gt; hms(\"14:00:01\") &\n          start_time &lt;= hms(\"18:00:00\") ~ \"afternoon\",\n        start_time &gt; hms(\"18:00:01\") &\n          start_time &lt;= hms(\"22:00:00\") ~ \"evening\",\n        start_time &gt; hms(\"22:00:01\") &\n          start_time &lt;= hms(\"23:59:59\") ~ \"night\",\n        start_time &gt;= hms(\"00:00:00\") &\n          start_time &lt;= hms(\"02:00:00\") ~ \"postmidnight\",\n        .default = \"other\"\n      )\n  ) %&gt;%\n  mutate(\n    start_day_time =\n      as_factor(start_day_time) %&gt;%\n        fct_collapse(\n          .f = .,\n          night = c(\"night\", \"postmidnight\")\n        )\n  )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `start_day_time = `%&gt;%`(...)`.\nCaused by warning:\n! Unknown levels in `f`: night\n\n##\n# Join with rank_df\nrace_start_factor %&gt;%\n  left_join(rank_df, by = \"race_year_id\") %&gt;%\n  drop_na(time_in_seconds) %&gt;%\n  gf_histogram(\n    ~time_in_seconds,\n    bins = 75,\n    fill = ~start_day_time,\n    color = ~start_day_time,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(start_day_time), ncol = 2, scales = \"free_y\") %&gt;%\n  gf_labs(title = \"Race Times by Start-Time\")\n\n\n\n\n\n\n\n\nWe see that finish times tend to be longer for afternoon and evening start races"
  },
  {
    "objectID": "posts/day-4/index.html#populations-data-set",
    "href": "posts/day-4/index.html#populations-data-set",
    "title": "Day 4",
    "section": "Populations data-set",
    "text": "Populations data-set\n\npop &lt;- read_delim(\"../../data/populations.csv\")\n\nRows: 16400 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country_code, country_name\ndbl (2): year, value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npop\n\n# A tibble: 16,400 × 4\n   country_code country_name  year value\n   &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 ABW          Aruba         1960 54608\n 2 ABW          Aruba         1961 55811\n 3 ABW          Aruba         1962 56682\n 4 ABW          Aruba         1963 57475\n 5 ABW          Aruba         1964 58178\n 6 ABW          Aruba         1965 58782\n 7 ABW          Aruba         1966 59291\n 8 ABW          Aruba         1967 59522\n 9 ABW          Aruba         1968 59471\n10 ABW          Aruba         1969 59330\n# ℹ 16,390 more rows\n\ninspect(pop)\n\n\ncategorical variables:  \n          name     class levels     n missing\n1 country_code character    265 16400       0\n2 country_name character    265 16400       0\n                                   distribution\n1 ABW (0.4%), AFE (0.4%), AFG (0.4%) ...       \n2 Afghanistan (0.4%) ...                       \n\nquantitative variables:  \n   name   class  min       Q1  median       Q3        max         mean\n1  year numeric 1960   1975.0    1991     2006       2021 1.990529e+03\n2 value numeric 2646 986302.5 6731400 46024452 7888408686 2.140804e+08\n            sd     n missing\n1 1.789551e+01 16400       0\n2 7.040554e+08 16400       0\n\n\nThere are many countries with small populations and a few countries with very large populations. Such distributions are also called “long tailed” distributions.\n\ngf_histogram(~value, data = pop, title = \"Long Tailed Histogram\")\n\n\n\n\n\n\n\n##\ngf_density(~value, data = pop, title = \"Long Tailed Density\")\n\n\n\n\n\n\n\n\nTo develop better insights with this data, we should transform the variable concerned, using say a “log” transformation:\n\ngf_histogram(~ log10(value), data = pop, title = \"Histogram with Log transformed x-variable\")\n\n\n\n\n\n\n\n##\ngf_density(~ log10(value), data = pop, title = \"Density with Log transformed x-variable\")"
  },
  {
    "objectID": "posts/day-4/index.html#what-does-each-distribution-signify",
    "href": "posts/day-4/index.html#what-does-each-distribution-signify",
    "title": "Day 4",
    "section": "What does each distribution signify?",
    "text": "What does each distribution signify?\n\n\n\nBimodal: There could be two different underlying processes or populations contributing to the data.\nComb: The data might have been processed in a way that grouped values together in regular intervals.A comb distribution could appear in data where ages are rounded to the nearest five years (e.g., 20, 25, 30).\nEdge Peak: Could even be a data entry artifact!! All unknown / unrecorded observations are recorded as 999 !!🙀\nNormal: The data follows a typical pattern where most values are close to the mean, and extremes are rare. This distribution occurs frequently in nature and in many datasets due to the Central Limit Theorem.\nSkewed: Right skew suggests that most values are clustered at the lower end, but there are some extreme high values. Left skew suggests that most values are clustered at the higher end, but there are some extreme low values.\nUniform: This can suggest random or non-preferential selection of values."
  },
  {
    "objectID": "posts/Day-2/index.html",
    "href": "posts/Day-2/index.html",
    "title": "Day 2",
    "section": "",
    "text": "Throwing away data to grasp it. I am trying to reduce data into a summarized form."
  },
  {
    "objectID": "posts/Day-2/index.html#introduction",
    "href": "posts/Day-2/index.html#introduction",
    "title": "Day 2",
    "section": "",
    "text": "Throwing away data to grasp it. I am trying to reduce data into a summarized form."
  },
  {
    "objectID": "posts/Day-2/index.html#r-code",
    "href": "posts/Day-2/index.html#r-code",
    "title": "Day 2",
    "section": "R-code:",
    "text": "R-code:\n\nnote to self: don’t forget to put the label!!!!!\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows"
  },
  {
    "objectID": "posts/Day-2/index.html#the-mpg-data-set-car-stuff",
    "href": "posts/Day-2/index.html#the-mpg-data-set-car-stuff",
    "title": "Day 2",
    "section": "The mpg data-set (car stuff):",
    "text": "The mpg data-set (car stuff):\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\n\nmaking it look fancy. Arvind wrote this, i only added it here cause i wanted the full form of all columns cause i don’t know car stuff.\n\n\nmpg %&gt;%\n  head(10) %&gt;%\n  kbl(\n    # add Human Readable column names\n    col.names = c(\n      \"Manufacturer\", \"Model\", \"Engine\\nDisplacement\",\n      \"Model\\n Year\", \"Cylinders\", \"Transmission\",\n      \"Drivetrain\", \"City\\n Mileage\", \"Highway\\n Mileage\",\n      \"Fuel\", \"Class\\nOf\\nVehicle\"\n    ),\n    caption = \"MPG Dataset\"\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\n      \"striped\", \"hover\",\n      \"condensed\", \"responsive\"\n    ),\n    full_width = F, position = \"center\"\n  )\n\n\n\nMPG Dataset\n\n\nManufacturer\nModel\nEngine Displacement\nModel Year\nCylinders\nTransmission\nDrivetrain\nCity Mileage\nHighway Mileage\nFuel\nClass Of Vehicle\n\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n\n\n\n\n\n\n\nGlimpse:\n\nmpg %&gt;% dplyr::glimpse()\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\n\nTable of Variables\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nmanufacturer\nRepresents the name of the company that produces the vehicle.\nQualitative\n\n\nmodel\nThis variable indicates the specific model of the vehicle produced by the manufacturer.\nQualitative\n\n\ndispl\nEngine Displacement: measures the engine’s size or capacity\nQuantitative\n\n\nyear\nIndicates the year in which the vehicle model was manufactured.\nQualitative\n\n\ncyl\nSpecifies the number of cylinders in the vehicle’s engine.\nQualitative\n\n\ntrans\nIndicates the type of transmission system used in the vehicle\nQualitative\n\n\ndrv\nDescribes how power is delivered to the wheels (e.g., front-wheel drive, rear-wheel drive, all-wheel drive)\nQualitative\n\n\ncty\nMeasures the fuel efficiency of the vehicle when driving in urban conditions\nQuantitative\n\n\nhwy\nMeasures fuel efficiency on highways\nQuantitative\n\n\nfl\nIndicates the type of fuel used by the vehicle\nQualitative\n\n\nclass\nCategorizes vehicles into different classes based on their size, purpose, or design\nQualitative\n\n\n\n\nThrough this glimpse, i find that cylinder is encoded as an int, and so it is considered a quantitative variable. i need to change it.\n\n\nmpg_modified &lt;- mpg %&gt;%\n  dplyr::mutate(\n    cyl = as_factor(cyl),\n    fl = as_factor(fl),\n    drv = as_factor(drv),\n    class = as_factor(class),\n    trans = as_factor(trans)\n  )\nglimpse(mpg_modified)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;fct&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;fct&gt; auto(l5), manual(m5), manual(m6), auto(av), auto(l5), man…\n$ drv          &lt;fct&gt; f, f, f, f, f, f, f, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, r, …\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;fct&gt; p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, r, …\n$ class        &lt;fct&gt; compact, compact, compact, compact, compact, compact, com…\n\n\n\nInspecting the data-set with mosaic and skimr\nA mosaic inspect of the mgp data-set only to see the difference between this and the newly created mpg_modified data set\n\nmpg %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3        trans character     10 234       0\n4          drv character      3 234       0\n5           fl character      5 234       0\n6        class character      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n4 f (45.3%), 4 (44%), r (10.7%)                \n5 r (71.8%), p (22.2%), e (3.4%) ...           \n6 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cyl integer    4.0    4.0    6.0    8.0    8    5.888889 1.611534 234\n4   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n5   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n\n\n\nmpg_modified %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3          cyl    factor      4 234       0\n4        trans    factor     10 234       0\n5          drv    factor      3 234       0\n6           fl    factor      5 234       0\n7        class    factor      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 4 (34.6%), 6 (33.8%), 8 (29.9%) ...          \n4 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n5 f (45.3%), 4 (44%), r (10.7%)                \n6 r (71.8%), p (22.2%), e (3.4%) ...           \n7 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n4   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\n\nmpg_modified %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n234\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n5\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmanufacturer\n0\n1\n4\n10\n0\n15\n0\n\n\nmodel\n0\n1\n2\n22\n0\n38\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncyl\n0\n1\nFALSE\n4\n4: 81, 6: 79, 8: 70, 5: 4\n\n\ntrans\n0\n1\nFALSE\n10\naut: 83, man: 58, aut: 39, man: 19\n\n\ndrv\n0\n1\nFALSE\n3\nf: 106, 4: 103, r: 25\n\n\nfl\n0\n1\nFALSE\n5\nr: 168, p: 52, e: 8, d: 5\n\n\nclass\n0\n1\nFALSE\n7\nsuv: 62, com: 47, mid: 41, sub: 35\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndispl\n0\n1\n3.47\n1.29\n1.6\n2.4\n3.3\n4.6\n7\n▇▆▆▃▁\n\n\nyear\n0\n1\n2003.50\n4.51\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n▇▁▁▁▇\n\n\ncty\n0\n1\n16.86\n4.26\n9.0\n14.0\n17.0\n19.0\n35\n▆▇▃▁▁\n\n\nhwy\n0\n1\n23.44\n5.95\n12.0\n18.0\n24.0\n27.0\n44\n▅▅▇▁▁\n\n\n\n\n\n\n\n\nGroups within columns -\nWe can group a quantitative data based on different qualitative data-sets.\n\nHere i am finding the separate means of highway mileage for every separate level of cyl.\n\nmpg_modified %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(average_hwy = mean(hwy), count = n())\n\n# A tibble: 4 × 3\n  cyl   average_hwy count\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 4            28.8    81\n2 5            28.8     4\n3 6            22.8    79\n4 8            17.6    70\n\n\nThe most highway mileage is for 4 cylinders while the lowest is for 8 cylinders.\n\nYou can find mean based on 2 or more factors i.e. permutations and combinations of all factors\n\n\n\nSeparate means of highway mileage for every separate level of cyl and fuel.\n\nmpg_modified %&gt;%\n  group_by(cyl, fl) %&gt;%\n  summarize(average_hwy = mean(hwy), count = n())\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 13 × 4\n# Groups:   cyl [4]\n   cyl   fl    average_hwy count\n   &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n 1 4     p            27.8    22\n 2 4     r            28.3    55\n 3 4     d            43       3\n 4 4     c            36       1\n 5 5     r            28.8     4\n 6 6     p            25.3    17\n 7 6     r            22.2    60\n 8 6     e            17       1\n 9 6     d            22       1\n10 8     p            20.8    13\n11 8     r            17.5    49\n12 8     e            12.7     7\n13 8     d            17       1\n\n\nThe highest mean highway mileage is for a cars with 4 cylinders with diesel fuel while the lowest is for electric cars with 6 cylinders.\n\n\nSeparate means of city mileage for every separate level of cyl and fuel.\n\nmpg_modified %&gt;%\n  group_by(cyl, fl) %&gt;%\n  summarize(average_cty = mean(cty), count = n())\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 13 × 4\n# Groups:   cyl [4]\n   cyl   fl    average_cty count\n   &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n 1 4     p           19.9     22\n 2 4     r           20.8     55\n 3 4     d           32.3      3\n 4 4     c           24        1\n 5 5     r           20.5      4\n 6 6     p           16.8     17\n 7 6     r           16.1     60\n 8 6     e           11        1\n 9 6     d           17        1\n10 8     p           13.8     13\n11 8     r           12.7     49\n12 8     e            9.57     7\n13 8     d           14        1\n\n\nThe highest mean city mileage is for a cars with 4 cylinders with diesel fuel while the lowest is for cars with 8 cylinders and petrol fuel.\n\n\nSeparate means of city mileage for every separate level of cyl and fuel.\n\nmpg_modified %&gt;%\n  group_by(manufacturer) %&gt;%\n  summarize(average_cty = mean(cty))\n\n# A tibble: 15 × 2\n   manufacturer average_cty\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 audi                17.6\n 2 chevrolet           15  \n 3 dodge               13.1\n 4 ford                14  \n 5 honda               24.4\n 6 hyundai             18.6\n 7 jeep                13.5\n 8 land rover          11.5\n 9 lincoln             11.3\n10 mercury             13.2\n11 nissan              18.1\n12 pontiac             17  \n13 subaru              19.3\n14 toyota              18.5\n15 volkswagen          20.9\n\n\nThe highest mean city mileage is for a cars with 4 cylinders with diesel fuel while the lowest is for electric cars with 6 cylinders."
  },
  {
    "objectID": "posts/Day-2/index.html#math-anxiety-data-set",
    "href": "posts/Day-2/index.html#math-anxiety-data-set",
    "title": "Day 2",
    "section": "Math Anxiety Data-Set:",
    "text": "Math Anxiety Data-Set:\n\nmath_anx &lt;- read_delim(\"../../data/MathAnxiety.csv\", delim=\";\")\n\nRows: 599 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (2): Gender, Grade\ndbl (3): AMAS, RCMAS, Arith\nnum (1): Age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmath_anx\n\n# A tibble: 599 × 6\n     Age Gender Grade      AMAS RCMAS Arith\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1378 Boy    Secondary     9    20     6\n 2  1407 Boy    Secondary    18     8     6\n 3  1379 Girl   Secondary    23    26     5\n 4  1428 Girl   Secondary    19    18     7\n 5  1356 Boy    Secondary    23    20     1\n 6  1350 Girl   Secondary    27    33     1\n 7  1336 Boy    Secondary    22    23     4\n 8  1393 Boy    Secondary    17    11     7\n 9  1317 Girl   Secondary    28    32     2\n10  1348 Boy    Secondary    20    30     6\n# ℹ 589 more rows\n\n\n###Glimpse the data to understand which we have to change to factors.\n\nmath_anx %&gt;% dplyr::glimpse()\n\nRows: 599\nColumns: 6\n$ Age    &lt;dbl&gt; 1378, 1407, 1379, 1428, 1356, 1350, 1336, 1393, 1317, 1348, 141…\n$ Gender &lt;chr&gt; \"Boy\", \"Boy\", \"Girl\", \"Girl\", \"Boy\", \"Girl\", \"Boy\", \"Boy\", \"Gir…\n$ Grade  &lt;chr&gt; \"Secondary\", \"Secondary\", \"Secondary\", \"Secondary\", \"Secondary\"…\n$ AMAS   &lt;dbl&gt; 9, 18, 23, 19, 23, 27, 22, 17, 28, 20, 16, 20, 21, 36, 16, 27, …\n$ RCMAS  &lt;dbl&gt; 20, 8, 26, 18, 20, 33, 23, 11, 32, 30, 10, 4, 23, 26, 24, 21, 3…\n$ Arith  &lt;dbl&gt; 6, 6, 5, 7, 1, 1, 4, 7, 2, 6, 2, 5, 2, 6, 2, 7, 2, 4, 7, 3, 8, …\n\n\n\nTable of Variables\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nAge\nRepresents the age of the individual\nQualitative\n\n\nGender\nRepresents the gender of the individual(male/female)\nQualitative\n\n\nGrade\nRepresents the grade they study in\nQualitative\n\n\nAMAS\nAdult Manifest Anxiety Scale\nQuantitative\n\n\nRCMAS\nRevised Children’s Manifest Anxiety Scale\nQuantitative\n\n\nArith\nArithmetic Subtest of Intelligence Tests\nQuantitative\n\n\n\nTarget Variable: AMAS, RCMAS and Arith Predictor Variables: Age, Gender and Grade\n\n\nMutate to facors:\n\nAmong all variables, Gender, Grade and Arith must be turned into factors.\n\n\nmath_anx_modified &lt;- math_anx %&gt;%\n  dplyr::mutate(\n    Gender = as_factor(Gender),\n    Grade = as_factor(Grade),\n    Arith = as_factor(Arith)\n  )\nglimpse(math_anx_modified)\n\nRows: 599\nColumns: 6\n$ Age    &lt;dbl&gt; 1378, 1407, 1379, 1428, 1356, 1350, 1336, 1393, 1317, 1348, 141…\n$ Gender &lt;fct&gt; Boy, Boy, Girl, Girl, Boy, Girl, Boy, Boy, Girl, Boy, Boy, Boy,…\n$ Grade  &lt;fct&gt; Secondary, Secondary, Secondary, Secondary, Secondary, Secondar…\n$ AMAS   &lt;dbl&gt; 9, 18, 23, 19, 23, 27, 22, 17, 28, 20, 16, 20, 21, 36, 16, 27, …\n$ RCMAS  &lt;dbl&gt; 20, 8, 26, 18, 20, 33, 23, 11, 32, 30, 10, 4, 23, 26, 24, 21, 3…\n$ Arith  &lt;fct&gt; 6, 6, 5, 7, 1, 1, 4, 7, 2, 6, 2, 5, 2, 6, 2, 7, 2, 4, 7, 3, 8, …\n\n\n\n\nSummerising the data with skim:\n\nmath_anx_modified %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n599\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n2\nBoy: 323, Gir: 276\n\n\nGrade\n0\n1\nFALSE\n2\nPri: 401, Sec: 198\n\n\nArith\n0\n1\nFALSE\n9\n7: 112, 6: 109, 8: 94, 5: 93\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n0\n1\n1246.49\n223.11\n37\n1061.5\n1208\n1418.5\n1875\n▁▁▇▇▃\n\n\nAMAS\n0\n1\n21.98\n6.60\n4\n18.0\n22\n26.5\n45\n▂▆▇▃▁\n\n\nRCMAS\n0\n1\n19.24\n7.57\n1\n14.0\n19\n25.0\n41\n▂▇▇▅▁\n\n\n\n\n\n\nAnxiety based on Gender:\n\nmath_anx_modified %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(average_AMAS = mean(AMAS), average_RCMAS = mean(RCMAS), count =n())\n\n# A tibble: 2 × 4\n  Gender average_AMAS average_RCMAS count\n  &lt;fct&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Boy            21.2          18.1   323\n2 Girl           22.9          20.6   276\n\n\nHypothesis to Evaluate: Girls have a higher anxiety level.\n\n\nAnxiety based on Grade:\n\nmath_anx_modified %&gt;%\n  group_by(Grade) %&gt;%\n  summarize(average_AMAS = mean(AMAS), average_RCMAS = mean(RCMAS), count =n())\n\n# A tibble: 2 × 4\n  Grade     average_AMAS average_RCMAS count\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Secondary         22.3          18.5   198\n2 Primary           21.8          19.6   401\n\n\nHypothesis to Evaluate: Primary school students experience more anxiety than secondary school students.\n\n\nAnxiety based on Grade and Gender:\n\nmath_anx_modified %&gt;%\n  group_by(Grade, Gender) %&gt;%\n  summarize(average_AMAS = mean(AMAS), average_RCMAS = mean(RCMAS), count =n())\n\n`summarise()` has grouped output by 'Grade'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 5\n# Groups:   Grade [2]\n  Grade     Gender average_AMAS average_RCMAS count\n  &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Secondary Boy            21.5          17.4   124\n2 Secondary Girl           23.5          20.3    74\n3 Primary   Boy            20.9          18.6   199\n4 Primary   Girl           22.7          20.6   202\n\n\nHypothesis to Evaluate: Girls studying in secondary school have the highest anxiety and Secondary School boys have the least amount of anxiety.\n\n\nAnxiety based on Arithmetic Subtest of Intelligence Tests:\n\nmath_anx_modified %&gt;%\n  group_by(Arith) %&gt;%\n  summarize(average_AMAS = mean(AMAS), average_RCMAS = mean(RCMAS), count =n())\n\n# A tibble: 9 × 4\n  Arith average_AMAS average_RCMAS count\n  &lt;fct&gt;        &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 0             22.5          20.9    15\n2 1             24.5          23.9    25\n3 2             23.6          21.0    26\n4 3             24.2          18.2    56\n5 4             21.7          19.3    69\n6 5             21.2          19.7    93\n7 6             22.5          19.4   109\n8 7             21.4          18.1   112\n9 8             20.5          18.5    94\n\n\nI don’t really know how to read this last data."
  },
  {
    "objectID": "posts/cs2/index.html",
    "href": "posts/cs2/index.html",
    "title": "Case Study 2",
    "section": "",
    "text": "This dataset pertains to scores obtained by students in diverse subjects. Family Income is also part of this dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(easystats) \n\n# Attaching packages: easystats 0.7.3\n✔ bayestestR  0.14.0   ✔ correlation 0.8.5 \n✔ datawizard  0.13.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.5   ✔ modelbased  0.8.8 \n✔ performance 0.12.3   ✔ parameters  0.22.2\n✔ report      0.5.9    ✔ see         0.9.0 \n\nlibrary(correlation)"
  },
  {
    "objectID": "posts/cs2/index.html#introduction",
    "href": "posts/cs2/index.html#introduction",
    "title": "Case Study 2",
    "section": "",
    "text": "This dataset pertains to scores obtained by students in diverse subjects. Family Income is also part of this dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(easystats) \n\n# Attaching packages: easystats 0.7.3\n✔ bayestestR  0.14.0   ✔ correlation 0.8.5 \n✔ datawizard  0.13.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.5   ✔ modelbased  0.8.8 \n✔ performance 0.12.3   ✔ parameters  0.22.2\n✔ report      0.5.9    ✔ see         0.9.0 \n\nlibrary(correlation)"
  },
  {
    "objectID": "posts/cs2/index.html#playing-with-and-understanding-the-data",
    "href": "posts/cs2/index.html#playing-with-and-understanding-the-data",
    "title": "Case Study 2",
    "section": "Playing with and understanding the data",
    "text": "Playing with and understanding the data\n\nAcquiring the data:\n\nscores &lt;- read_csv(\"../../data/school-scores-data.csv\")\n\nRows: 577 Columns: 99\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): State.Code, State.Name\ndbl (97): Year, Total.Math, Total.Test-takers, Total.Verbal, Academic Subjec...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nscores\n\n# A tibble: 577 × 99\n    Year State.Code State.Name       Total.Math `Total.Test-takers` Total.Verbal\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1  2005 AL         Alabama                 559                3985          567\n 2  2005 AK         Alaska                  519                3996          523\n 3  2005 AZ         Arizona                 530               18184          526\n 4  2005 AR         Arkansas                552                1600          563\n 5  2005 CA         California              522              186552          504\n 6  2005 CO         Colorado                560               11990          560\n 7  2005 CT         Connecticut             517               34313          517\n 8  2005 DE         Delaware                502                6257          503\n 9  2005 DC         District Of Col…        478                3622          490\n10  2005 FL         Florida                 498               93505          498\n# ℹ 567 more rows\n# ℹ 93 more variables: `Academic Subjects.Arts/Music.Average GPA` &lt;dbl&gt;,\n#   `Academic Subjects.Arts/Music.Average Years` &lt;dbl&gt;,\n#   `Academic Subjects.English.Average GPA` &lt;dbl&gt;,\n#   `Academic Subjects.English.Average Years` &lt;dbl&gt;,\n#   `Academic Subjects.Foreign Languages.Average GPA` &lt;dbl&gt;,\n#   `Academic Subjects.Foreign Languages.Average Years` &lt;dbl&gt;, …\n\n\n\nglimpse(scores)\n\nRows: 577\nColumns: 99\n$ Year                                                      &lt;dbl&gt; 2005, 2005, …\n$ State.Code                                                &lt;chr&gt; \"AL\", \"AK\", …\n$ State.Name                                                &lt;chr&gt; \"Alabama\", \"…\n$ Total.Math                                                &lt;dbl&gt; 559, 519, 53…\n$ `Total.Test-takers`                                       &lt;dbl&gt; 3985, 3996, …\n$ Total.Verbal                                              &lt;dbl&gt; 567, 523, 52…\n$ `Academic Subjects.Arts/Music.Average GPA`                &lt;dbl&gt; 3.92, 3.76, …\n$ `Academic Subjects.Arts/Music.Average Years`              &lt;dbl&gt; 2.2, 1.9, 2.…\n$ `Academic Subjects.English.Average GPA`                   &lt;dbl&gt; 3.53, 3.35, …\n$ `Academic Subjects.English.Average Years`                 &lt;dbl&gt; 3.9, 3.9, 3.…\n$ `Academic Subjects.Foreign Languages.Average GPA`         &lt;dbl&gt; 3.54, 3.34, …\n$ `Academic Subjects.Foreign Languages.Average Years`       &lt;dbl&gt; 2.6, 2.1, 2.…\n$ `Academic Subjects.Mathematics.Average GPA`               &lt;dbl&gt; 3.41, 3.06, …\n$ `Academic Subjects.Mathematics.Average Years`             &lt;dbl&gt; 4.0, 3.5, 3.…\n$ `Academic Subjects.Natural Sciences.Average GPA`          &lt;dbl&gt; 3.52, 3.25, …\n$ `Academic Subjects.Natural Sciences.Average Years`        &lt;dbl&gt; 3.9, 3.2, 3.…\n$ `Academic Subjects.Social Sciences/History.Average GPA`   &lt;dbl&gt; 3.59, 3.39, …\n$ `Academic Subjects.Social Sciences/History.Average Years` &lt;dbl&gt; 3.9, 3.4, 3.…\n$ `Family Income.Between 20-40k.Math`                       &lt;dbl&gt; 513, 492, 49…\n$ `Family Income.Between 20-40k.Test-takers`                &lt;dbl&gt; 324, 401, 21…\n$ `Family Income.Between 20-40k.Verbal`                     &lt;dbl&gt; 527, 500, 49…\n$ `Family Income.Between 40-60k.Math`                       &lt;dbl&gt; 539, 517, 52…\n$ `Family Income.Between 40-60k.Test-takers`                &lt;dbl&gt; 442, 539, 22…\n$ `Family Income.Between 40-60k.Verbal`                     &lt;dbl&gt; 551, 522, 51…\n$ `Family Income.Between 60-80k.Math`                       &lt;dbl&gt; 550, 513, 52…\n$ `Family Income.Between 60-80k.Test-takers`                &lt;dbl&gt; 473, 603, 23…\n$ `Family Income.Between 60-80k.Verbal`                     &lt;dbl&gt; 564, 519, 52…\n$ `Family Income.Between 80-100k.Math`                      &lt;dbl&gt; 566, 528, 53…\n$ `Family Income.Between 80-100k.Test-takers`               &lt;dbl&gt; 475, 444, 18…\n$ `Family Income.Between 80-100k.Verbal`                    &lt;dbl&gt; 577, 534, 53…\n$ `Family Income.Less than 20k.Math`                        &lt;dbl&gt; 462, 464, 48…\n$ `Family Income.Less than 20k.Test-takers`                 &lt;dbl&gt; 175, 191, 89…\n$ `Family Income.Less than 20k.Verbal`                      &lt;dbl&gt; 474, 467, 47…\n$ `Family Income.More than 100k.Math`                       &lt;dbl&gt; 588, 541, 55…\n$ `Family Income.More than 100k.Test-takers`                &lt;dbl&gt; 980, 540, 30…\n$ `Family Income.More than 100k.Verbal`                     &lt;dbl&gt; 590, 544, 54…\n$ `GPA.A minus.Math`                                        &lt;dbl&gt; 569, 544, 54…\n$ `GPA.A minus.Test-takers`                                 &lt;dbl&gt; 724, 673, 33…\n$ `GPA.A minus.Verbal`                                      &lt;dbl&gt; 575, 546, 53…\n$ `GPA.A plus.Math`                                         &lt;dbl&gt; 622, 600, 60…\n$ `GPA.A plus.Test-takers`                                  &lt;dbl&gt; 563, 173, 16…\n$ `GPA.A plus.Verbal`                                       &lt;dbl&gt; 623, 604, 59…\n$ GPA.A.Math                                                &lt;dbl&gt; 600, 580, 57…\n$ `GPA.A.Test-takers`                                       &lt;dbl&gt; 1032, 671, 3…\n$ GPA.A.Verbal                                              &lt;dbl&gt; 608, 578, 56…\n$ GPA.B.Math                                                &lt;dbl&gt; 514, 492, 49…\n$ `GPA.B.Test-takers`                                       &lt;dbl&gt; 1253, 1622, …\n$ GPA.B.Verbal                                              &lt;dbl&gt; 525, 499, 49…\n$ GPA.C.Math                                                &lt;dbl&gt; 436, 466, 45…\n$ `GPA.C.Test-takers`                                       &lt;dbl&gt; 188, 418, 11…\n$ GPA.C.Verbal                                              &lt;dbl&gt; 451, 472, 46…\n$ `GPA.D or lower.Math`                                     &lt;dbl&gt; 0, 424, 439,…\n$ `GPA.D or lower.Test-takers`                              &lt;dbl&gt; 0, 12, 16, 0…\n$ `GPA.D or lower.Verbal`                                   &lt;dbl&gt; 0, 466, 435,…\n$ `GPA.No response.Math`                                    &lt;dbl&gt; 0, 0, 0, 0, …\n$ `GPA.No response.Test-takers`                             &lt;dbl&gt; 225, 427, 91…\n$ `GPA.No response.Verbal`                                  &lt;dbl&gt; 0, 0, 0, 0, …\n$ Gender.Female.Math                                        &lt;dbl&gt; 538, 505, 51…\n$ `Gender.Female.Test-takers`                               &lt;dbl&gt; 2072, 2161, …\n$ Gender.Female.Verbal                                      &lt;dbl&gt; 561, 521, 52…\n$ Gender.Male.Math                                          &lt;dbl&gt; 582, 535, 54…\n$ `Gender.Male.Test-takers`                                 &lt;dbl&gt; 1913, 1835, …\n$ Gender.Male.Verbal                                        &lt;dbl&gt; 574, 526, 53…\n$ `Score Ranges.Between 200 to 300.Math.Females`            &lt;dbl&gt; 22, 30, 119,…\n$ `Score Ranges.Between 200 to 300.Math.Males`              &lt;dbl&gt; 10, 20, 72, …\n$ `Score Ranges.Between 200 to 300.Math.Total`              &lt;dbl&gt; 32, 50, 191,…\n$ `Score Ranges.Between 200 to 300.Verbal.Females`          &lt;dbl&gt; 14, 26, 115,…\n$ `Score Ranges.Between 200 to 300.Verbal.Males`            &lt;dbl&gt; 17, 26, 86, …\n$ `Score Ranges.Between 200 to 300.Verbal.Total`            &lt;dbl&gt; 31, 52, 201,…\n$ `Score Ranges.Between 300 to 400.Math.Females`            &lt;dbl&gt; 173, 233, 88…\n$ `Score Ranges.Between 300 to 400.Math.Males`              &lt;dbl&gt; 93, 153, 450…\n$ `Score Ranges.Between 300 to 400.Math.Total`              &lt;dbl&gt; 266, 386, 13…\n$ `Score Ranges.Between 300 to 400.Verbal.Females`          &lt;dbl&gt; 123, 218, 73…\n$ `Score Ranges.Between 300 to 400.Verbal.Males`            &lt;dbl&gt; 84, 171, 613…\n$ `Score Ranges.Between 300 to 400.Verbal.Total`            &lt;dbl&gt; 207, 389, 13…\n$ `Score Ranges.Between 400 to 500.Math.Females`            &lt;dbl&gt; 514, 696, 32…\n$ `Score Ranges.Between 400 to 500.Math.Males`              &lt;dbl&gt; 293, 485, 19…\n$ `Score Ranges.Between 400 to 500.Math.Total`              &lt;dbl&gt; 807, 1181, 5…\n$ `Score Ranges.Between 400 to 500.Verbal.Females`          &lt;dbl&gt; 430, 656, 30…\n$ `Score Ranges.Between 400 to 500.Verbal.Males`            &lt;dbl&gt; 332, 552, 23…\n$ `Score Ranges.Between 400 to 500.Verbal.Total`            &lt;dbl&gt; 762, 1208, 5…\n$ `Score Ranges.Between 500 to 600.Math.Females`            &lt;dbl&gt; 722, 813, 35…\n$ `Score Ranges.Between 500 to 600.Math.Males`              &lt;dbl&gt; 614, 616, 31…\n$ `Score Ranges.Between 500 to 600.Math.Total`              &lt;dbl&gt; 1336, 1429, …\n$ `Score Ranges.Between 500 to 600.Verbal.Females`          &lt;dbl&gt; 690, 729, 36…\n$ `Score Ranges.Between 500 to 600.Verbal.Males`            &lt;dbl&gt; 617, 596, 31…\n$ `Score Ranges.Between 500 to 600.Verbal.Total`            &lt;dbl&gt; 1307, 1325, …\n$ `Score Ranges.Between 600 to 700.Math.Females`            &lt;dbl&gt; 485, 342, 16…\n$ `Score Ranges.Between 600 to 700.Math.Males`              &lt;dbl&gt; 611, 445, 21…\n$ `Score Ranges.Between 600 to 700.Math.Total`              &lt;dbl&gt; 1096, 787, 3…\n$ `Score Ranges.Between 600 to 700.Verbal.Females`          &lt;dbl&gt; 596, 423, 18…\n$ `Score Ranges.Between 600 to 700.Verbal.Males`            &lt;dbl&gt; 613, 375, 16…\n$ `Score Ranges.Between 600 to 700.Verbal.Total`            &lt;dbl&gt; 1209, 798, 3…\n$ `Score Ranges.Between 700 to 800.Math.Females`            &lt;dbl&gt; 156, 47, 327…\n$ `Score Ranges.Between 700 to 800.Math.Males`              &lt;dbl&gt; 292, 116, 63…\n$ `Score Ranges.Between 700 to 800.Math.Total`              &lt;dbl&gt; 448, 163, 95…\n$ `Score Ranges.Between 700 to 800.Verbal.Females`          &lt;dbl&gt; 219, 109, 41…\n$ `Score Ranges.Between 700 to 800.Verbal.Males`            &lt;dbl&gt; 250, 115, 50…\n$ `Score Ranges.Between 700 to 800.Verbal.Total`            &lt;dbl&gt; 469, 224, 91…\n\n\nThe data doesn’t seem right. How are there 99 columns?\n\nUse the janitor package here to clean up the variable names. Try to use the big_camel case name format for variables.\n\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following object is masked from 'package:insight':\n\n    clean_names\n\n\nThe following objects are masked from 'package:datawizard':\n\n    remove_empty, remove_empty_rows\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n\nscores_cleaned &lt;- clean_names(scores, case = \"big_camel\")\nscores_cleaned\n\n# A tibble: 577 × 99\n    Year StateCode StateName            TotalMath TotalTestTakers TotalVerbal\n   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                    &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1  2005 AL        Alabama                    559            3985         567\n 2  2005 AK        Alaska                     519            3996         523\n 3  2005 AZ        Arizona                    530           18184         526\n 4  2005 AR        Arkansas                   552            1600         563\n 5  2005 CA        California                 522          186552         504\n 6  2005 CO        Colorado                   560           11990         560\n 7  2005 CT        Connecticut                517           34313         517\n 8  2005 DE        Delaware                   502            6257         503\n 9  2005 DC        District Of Columbia       478            3622         490\n10  2005 FL        Florida                    498           93505         498\n# ℹ 567 more rows\n# ℹ 93 more variables: AcademicSubjectsArtsMusicAverageGpa &lt;dbl&gt;,\n#   AcademicSubjectsArtsMusicAverageYears &lt;dbl&gt;,\n#   AcademicSubjectsEnglishAverageGpa &lt;dbl&gt;,\n#   AcademicSubjectsEnglishAverageYears &lt;dbl&gt;,\n#   AcademicSubjectsForeignLanguagesAverageGpa &lt;dbl&gt;,\n#   AcademicSubjectsForeignLanguagesAverageYears &lt;dbl&gt;, …\n\n\n\nglimpse(scores_cleaned)\n\nRows: 577\nColumns: 99\n$ Year                                              &lt;dbl&gt; 2005, 2005, 2005, 20…\n$ StateCode                                         &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"A…\n$ StateName                                         &lt;chr&gt; \"Alabama\", \"Alaska\",…\n$ TotalMath                                         &lt;dbl&gt; 559, 519, 530, 552, …\n$ TotalTestTakers                                   &lt;dbl&gt; 3985, 3996, 18184, 1…\n$ TotalVerbal                                       &lt;dbl&gt; 567, 523, 526, 563, …\n$ AcademicSubjectsArtsMusicAverageGpa               &lt;dbl&gt; 3.92, 3.76, 3.85, 3.…\n$ AcademicSubjectsArtsMusicAverageYears             &lt;dbl&gt; 2.2, 1.9, 2.1, 2.2, …\n$ AcademicSubjectsEnglishAverageGpa                 &lt;dbl&gt; 3.53, 3.35, 3.45, 3.…\n$ AcademicSubjectsEnglishAverageYears               &lt;dbl&gt; 3.9, 3.9, 3.9, 4.0, …\n$ AcademicSubjectsForeignLanguagesAverageGpa        &lt;dbl&gt; 3.54, 3.34, 3.41, 3.…\n$ AcademicSubjectsForeignLanguagesAverageYears      &lt;dbl&gt; 2.6, 2.1, 2.6, 2.6, …\n$ AcademicSubjectsMathematicsAverageGpa             &lt;dbl&gt; 3.41, 3.06, 3.25, 3.…\n$ AcademicSubjectsMathematicsAverageYears           &lt;dbl&gt; 4.0, 3.5, 3.9, 4.1, …\n$ AcademicSubjectsNaturalSciencesAverageGpa         &lt;dbl&gt; 3.52, 3.25, 3.43, 3.…\n$ AcademicSubjectsNaturalSciencesAverageYears       &lt;dbl&gt; 3.9, 3.2, 3.4, 3.7, …\n$ AcademicSubjectsSocialSciencesHistoryAverageGpa   &lt;dbl&gt; 3.59, 3.39, 3.55, 3.…\n$ AcademicSubjectsSocialSciencesHistoryAverageYears &lt;dbl&gt; 3.9, 3.4, 3.3, 3.6, …\n$ FamilyIncomeBetween20_40KMath                     &lt;dbl&gt; 513, 492, 498, 513, …\n$ FamilyIncomeBetween20_40KTestTakers               &lt;dbl&gt; 324, 401, 2121, 180,…\n$ FamilyIncomeBetween20_40KVerbal                   &lt;dbl&gt; 527, 500, 495, 526, …\n$ FamilyIncomeBetween40_60KMath                     &lt;dbl&gt; 539, 517, 520, 543, …\n$ FamilyIncomeBetween40_60KTestTakers               &lt;dbl&gt; 442, 539, 2270, 245,…\n$ FamilyIncomeBetween40_60KVerbal                   &lt;dbl&gt; 551, 522, 518, 555, …\n$ FamilyIncomeBetween60_80KMath                     &lt;dbl&gt; 550, 513, 524, 553, …\n$ FamilyIncomeBetween60_80KTestTakers               &lt;dbl&gt; 473, 603, 2372, 227,…\n$ FamilyIncomeBetween60_80KVerbal                   &lt;dbl&gt; 564, 519, 523, 570, …\n$ FamilyIncomeBetween80_100KMath                    &lt;dbl&gt; 566, 528, 534, 570, …\n$ FamilyIncomeBetween80_100KTestTakers              &lt;dbl&gt; 475, 444, 1866, 147,…\n$ FamilyIncomeBetween80_100KVerbal                  &lt;dbl&gt; 577, 534, 533, 580, …\n$ FamilyIncomeLessThan20KMath                       &lt;dbl&gt; 462, 464, 485, 489, …\n$ FamilyIncomeLessThan20KTestTakers                 &lt;dbl&gt; 175, 191, 891, 107, …\n$ FamilyIncomeLessThan20KVerbal                     &lt;dbl&gt; 474, 467, 474, 486, …\n$ FamilyIncomeMoreThan100KMath                      &lt;dbl&gt; 588, 541, 554, 572, …\n$ FamilyIncomeMoreThan100KTestTakers                &lt;dbl&gt; 980, 540, 3083, 314,…\n$ FamilyIncomeMoreThan100KVerbal                    &lt;dbl&gt; 590, 544, 546, 589, …\n$ GpaAMinusMath                                     &lt;dbl&gt; 569, 544, 541, 559, …\n$ GpaAMinusTestTakers                               &lt;dbl&gt; 724, 673, 3334, 298,…\n$ GpaAMinusVerbal                                   &lt;dbl&gt; 575, 546, 535, 572, …\n$ GpaAPlusMath                                      &lt;dbl&gt; 622, 600, 605, 629, …\n$ GpaAPlusTestTakers                                &lt;dbl&gt; 563, 173, 1684, 273,…\n$ GpaAPlusVerbal                                    &lt;dbl&gt; 623, 604, 593, 639, …\n$ GpaAMath                                          &lt;dbl&gt; 600, 580, 571, 579, …\n$ GpaATestTakers                                    &lt;dbl&gt; 1032, 671, 3854, 457…\n$ GpaAVerbal                                        &lt;dbl&gt; 608, 578, 563, 583, …\n$ GpaBMath                                          &lt;dbl&gt; 514, 492, 498, 492, …\n$ GpaBTestTakers                                    &lt;dbl&gt; 1253, 1622, 7193, 43…\n$ GpaBVerbal                                        &lt;dbl&gt; 525, 499, 499, 511, …\n$ GpaCMath                                          &lt;dbl&gt; 436, 466, 458, 419, …\n$ GpaCTestTakers                                    &lt;dbl&gt; 188, 418, 1184, 57, …\n$ GpaCVerbal                                        &lt;dbl&gt; 451, 472, 464, 436, …\n$ GpaDOrLowerMath                                   &lt;dbl&gt; 0, 424, 439, 0, 419,…\n$ GpaDOrLowerTestTakers                             &lt;dbl&gt; 0, 12, 16, 0, 240, 1…\n$ GpaDOrLowerVerbal                                 &lt;dbl&gt; 0, 466, 435, 0, 408,…\n$ GpaNoResponseMath                                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ GpaNoResponseTestTakers                           &lt;dbl&gt; 225, 427, 919, 78, 1…\n$ GpaNoResponseVerbal                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ GenderFemaleMath                                  &lt;dbl&gt; 538, 505, 513, 536, …\n$ GenderFemaleTestTakers                            &lt;dbl&gt; 2072, 2161, 9806, 85…\n$ GenderFemaleVerbal                                &lt;dbl&gt; 561, 521, 522, 558, …\n$ GenderMaleMath                                    &lt;dbl&gt; 582, 535, 549, 570, …\n$ GenderMaleTestTakers                              &lt;dbl&gt; 1913, 1835, 8378, 74…\n$ GenderMaleVerbal                                  &lt;dbl&gt; 574, 526, 531, 570, …\n$ ScoreRangesBetween200To300MathFemales             &lt;dbl&gt; 22, 30, 119, 12, 297…\n$ ScoreRangesBetween200To300MathMales               &lt;dbl&gt; 10, 20, 72, 7, 1453,…\n$ ScoreRangesBetween200To300MathTotal               &lt;dbl&gt; 32, 50, 191, 19, 443…\n$ ScoreRangesBetween200To300VerbalFemales           &lt;dbl&gt; 14, 26, 115, 9, 3382…\n$ ScoreRangesBetween200To300VerbalMales             &lt;dbl&gt; 17, 26, 86, 3, 2433,…\n$ ScoreRangesBetween200To300VerbalTotal             &lt;dbl&gt; 31, 52, 201, 12, 581…\n$ ScoreRangesBetween300To400MathFemales             &lt;dbl&gt; 173, 233, 881, 68, 1…\n$ ScoreRangesBetween300To400MathMales               &lt;dbl&gt; 93, 153, 450, 31, 71…\n$ ScoreRangesBetween300To400MathTotal               &lt;dbl&gt; 266, 386, 1331, 99, …\n$ ScoreRangesBetween300To400VerbalFemales           &lt;dbl&gt; 123, 218, 739, 46, 1…\n$ ScoreRangesBetween300To400VerbalMales             &lt;dbl&gt; 84, 171, 613, 42, 10…\n$ ScoreRangesBetween300To400VerbalTotal             &lt;dbl&gt; 207, 389, 1352, 88, …\n$ ScoreRangesBetween400To500MathFemales             &lt;dbl&gt; 514, 696, 3215, 210,…\n$ ScoreRangesBetween400To500MathMales               &lt;dbl&gt; 293, 485, 1948, 137,…\n$ ScoreRangesBetween400To500MathTotal               &lt;dbl&gt; 807, 1181, 5163, 347…\n$ ScoreRangesBetween400To500VerbalFemales           &lt;dbl&gt; 430, 656, 3048, 183,…\n$ ScoreRangesBetween400To500VerbalMales             &lt;dbl&gt; 332, 552, 2398, 141,…\n$ ScoreRangesBetween400To500VerbalTotal             &lt;dbl&gt; 762, 1208, 5446, 324…\n$ ScoreRangesBetween500To600MathFemales             &lt;dbl&gt; 722, 813, 3576, 316,…\n$ ScoreRangesBetween500To600MathMales               &lt;dbl&gt; 614, 616, 3152, 244,…\n$ ScoreRangesBetween500To600MathTotal               &lt;dbl&gt; 1336, 1429, 6728, 56…\n$ ScoreRangesBetween500To600VerbalFemales           &lt;dbl&gt; 690, 729, 3661, 302,…\n$ ScoreRangesBetween500To600VerbalMales             &lt;dbl&gt; 617, 596, 3101, 236,…\n$ ScoreRangesBetween500To600VerbalTotal             &lt;dbl&gt; 1307, 1325, 6762, 53…\n$ ScoreRangesBetween600To700MathFemales             &lt;dbl&gt; 485, 342, 1688, 204,…\n$ ScoreRangesBetween600To700MathMales               &lt;dbl&gt; 611, 445, 2126, 239,…\n$ ScoreRangesBetween600To700MathTotal               &lt;dbl&gt; 1096, 787, 3814, 443…\n$ ScoreRangesBetween600To700VerbalFemales           &lt;dbl&gt; 596, 423, 1831, 242,…\n$ ScoreRangesBetween600To700VerbalMales             &lt;dbl&gt; 613, 375, 1679, 226,…\n$ ScoreRangesBetween600To700VerbalTotal             &lt;dbl&gt; 1209, 798, 3510, 468…\n$ ScoreRangesBetween700To800MathFemales             &lt;dbl&gt; 156, 47, 327, 49, 54…\n$ ScoreRangesBetween700To800MathMales               &lt;dbl&gt; 292, 116, 630, 83, 8…\n$ ScoreRangesBetween700To800MathTotal               &lt;dbl&gt; 448, 163, 957, 132, …\n$ ScoreRangesBetween700To800VerbalFemales           &lt;dbl&gt; 219, 109, 412, 77, 5…\n$ ScoreRangesBetween700To800VerbalMales             &lt;dbl&gt; 250, 115, 501, 93, 4…\n$ ScoreRangesBetween700To800VerbalTotal             &lt;dbl&gt; 469, 224, 913, 170, …\n\n\n\nskim(scores_cleaned)\n\n\nData summary\n\n\nName\nscores_cleaned\n\n\nNumber of rows\n577\n\n\nNumber of columns\n99\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n97\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nStateCode\n0\n1\n2\n2\n0\n53\n0\n\n\nStateName\n0\n1\n4\n20\n0\n53\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1\n2010.02\n3.17\n2005.00\n2007.00\n2010.00\n2013.00\n2015.00\n▇▅▅▆▆\n\n\nTotalMath\n0\n1\n535.68\n46.17\n383.00\n504.00\n527.00\n571.00\n619.00\n▁▁▇▆▅\n\n\nTotalTestTakers\n0\n1\n27914.24\n45602.11\n134.00\n2536.00\n6468.00\n35799.00\n241553.00\n▇▁▁▁▁\n\n\nTotalVerbal\n0\n1\n531.33\n44.32\n401.00\n496.00\n522.00\n572.00\n612.00\n▁▂▇▃▅\n\n\nAcademicSubjectsArtsMusicAverageGpa\n0\n1\n3.82\n0.09\n3.43\n3.76\n3.85\n3.90\n3.96\n▁▁▂▆▇\n\n\nAcademicSubjectsArtsMusicAverageYears\n0\n1\n2.29\n0.32\n1.20\n2.10\n2.30\n2.50\n3.10\n▁▂▇▅▂\n\n\nAcademicSubjectsEnglishAverageGpa\n0\n1\n3.50\n0.19\n3.03\n3.35\n3.51\n3.67\n3.88\n▂▆▇▇▃\n\n\nAcademicSubjectsEnglishAverageYears\n0\n1\n3.93\n0.09\n3.50\n3.90\n3.90\n4.00\n4.10\n▁▁▂▇▇\n\n\nAcademicSubjectsForeignLanguagesAverageGpa\n0\n1\n3.45\n0.19\n3.03\n3.30\n3.46\n3.63\n3.79\n▃▇▇▇▇\n\n\nAcademicSubjectsForeignLanguagesAverageYears\n0\n1\n2.85\n0.34\n1.80\n2.60\n2.80\n3.10\n3.60\n▁▅▆▇▃\n\n\nAcademicSubjectsMathematicsAverageGpa\n0\n1\n3.31\n0.22\n2.85\n3.12\n3.30\n3.51\n3.76\n▂▇▅▇▃\n\n\nAcademicSubjectsMathematicsAverageYears\n0\n1\n3.94\n0.17\n3.20\n3.80\n3.90\n4.10\n4.40\n▁▁▇▆▂\n\n\nAcademicSubjectsNaturalSciencesAverageGpa\n0\n1\n3.42\n0.20\n2.87\n3.25\n3.42\n3.60\n3.82\n▁▆▇▇▅\n\n\nAcademicSubjectsNaturalSciencesAverageYears\n0\n1\n3.63\n0.20\n2.80\n3.50\n3.60\n3.80\n4.20\n▁▁▇▇▁\n\n\nAcademicSubjectsSocialSciencesHistoryAverageGpa\n0\n1\n3.52\n0.18\n3.05\n3.38\n3.53\n3.68\n3.88\n▁▆▆▇▅\n\n\nAcademicSubjectsSocialSciencesHistoryAverageYears\n0\n1\n3.62\n0.18\n3.00\n3.50\n3.60\n3.70\n4.00\n▁▃▇▇▂\n\n\nFamilyIncomeBetween20_40KMath\n0\n1\n500.24\n48.46\n0.00\n471.00\n495.00\n533.00\n643.00\n▁▁▁▇▅\n\n\nFamilyIncomeBetween20_40KTestTakers\n0\n1\n3234.18\n5935.05\n5.00\n214.00\n580.00\n3179.00\n35446.00\n▇▁▁▁▁\n\n\nFamilyIncomeBetween20_40KVerbal\n0\n1\n501.33\n43.81\n387.00\n466.00\n496.00\n536.00\n634.00\n▁▇▇▅▁\n\n\nFamilyIncomeBetween40_60KMath\n0\n1\n522.84\n43.08\n381.00\n493.00\n519.00\n554.00\n629.00\n▁▂▇▆▂\n\n\nFamilyIncomeBetween40_60KTestTakers\n0\n1\n2847.20\n4638.14\n10.00\n236.00\n636.00\n3386.00\n28124.00\n▇▁▁▁▁\n\n\nFamilyIncomeBetween40_60KVerbal\n0\n1\n523.08\n41.95\n414.00\n489.00\n517.00\n558.00\n628.00\n▁▇▇▇▂\n\n\nFamilyIncomeBetween60_80KMath\n0\n1\n533.61\n42.50\n249.00\n506.00\n531.00\n564.00\n630.00\n▁▁▁▇▅\n\n\nFamilyIncomeBetween60_80KTestTakers\n0\n1\n2269.76\n3535.33\n8.00\n199.00\n523.00\n2791.00\n17937.00\n▇▁▁▁▁\n\n\nFamilyIncomeBetween60_80KVerbal\n0\n1\n533.85\n43.42\n232.00\n501.00\n527.00\n571.00\n645.00\n▁▁▁▇▅\n\n\nFamilyIncomeBetween80_100KMath\n0\n1\n547.06\n38.37\n398.00\n519.00\n539.00\n575.00\n646.00\n▁▁▇▅▂\n\n\nFamilyIncomeBetween80_100KTestTakers\n0\n1\n1803.63\n2855.65\n5.00\n164.00\n441.00\n2130.00\n15358.00\n▇▁▁▁▁\n\n\nFamilyIncomeBetween80_100KVerbal\n0\n1\n544.58\n38.07\n433.00\n514.00\n536.00\n580.00\n651.00\n▁▇▇▇▁\n\n\nFamilyIncomeLessThan20KMath\n0\n1\n461.73\n79.25\n0.00\n438.00\n465.00\n510.00\n589.00\n▁▁▁▇▇\n\n\nFamilyIncomeLessThan20KTestTakers\n0\n1\n2433.32\n5254.03\n1.00\n124.00\n347.00\n2129.00\n42551.00\n▇▁▁▁▁\n\n\nFamilyIncomeLessThan20KVerbal\n0\n1\n458.43\n67.02\n0.00\n429.00\n464.00\n501.00\n579.00\n▁▁▁▇▇\n\n\nFamilyIncomeMoreThan100KMath\n0\n1\n565.83\n42.14\n0.00\n548.00\n565.00\n587.00\n637.00\n▁▁▁▁▇\n\n\nFamilyIncomeMoreThan100KTestTakers\n0\n1\n4141.35\n7004.94\n2.00\n427.00\n1000.00\n4405.00\n46127.00\n▇▁▁▁▁\n\n\nFamilyIncomeMoreThan100KVerbal\n0\n1\n560.49\n42.59\n0.00\n538.00\n555.00\n590.00\n637.00\n▁▁▁▁▇\n\n\nGpaAMinusMath\n0\n1\n550.03\n39.00\n0.00\n532.00\n552.00\n569.00\n619.00\n▁▁▁▁▇\n\n\nGpaAMinusTestTakers\n0\n1\n4954.84\n8126.75\n0.00\n460.00\n1217.00\n6372.00\n45869.00\n▇▁▁▁▁\n\n\nGpaAMinusVerbal\n0\n1\n543.72\n36.54\n0.00\n526.00\n547.00\n566.00\n609.00\n▁▁▁▁▇\n\n\nGpaAPlusMath\n0\n1\n621.70\n42.97\n0.00\n607.00\n624.00\n644.00\n683.00\n▁▁▁▁▇\n\n\nGpaAPlusTestTakers\n0\n1\n1592.20\n2384.09\n0.00\n274.00\n524.00\n1792.00\n12184.00\n▇▁▁▁▁\n\n\nGpaAPlusVerbal\n0\n1\n613.05\n41.44\n0.00\n595.00\n616.00\n637.00\n672.00\n▁▁▁▁▇\n\n\nGpaAMath\n0\n1\n584.99\n41.79\n0.00\n565.00\n588.00\n605.00\n655.00\n▁▁▁▁▇\n\n\nGpaATestTakers\n0\n1\n4925.42\n7645.59\n0.00\n680.00\n1390.00\n6112.00\n42656.00\n▇▁▁▁▁\n\n\nGpaAVerbal\n0\n1\n576.85\n39.54\n0.00\n556.00\n580.00\n600.00\n637.00\n▁▁▁▁▇\n\n\nGpaBMath\n0\n1\n490.50\n37.77\n0.00\n472.00\n492.00\n511.00\n564.00\n▁▁▁▁▇\n\n\nGpaBTestTakers\n0\n1\n11728.67\n19924.44\n0.00\n676.00\n2282.00\n14745.00\n104693.00\n▇▁▁▁▁\n\n\nGpaBVerbal\n0\n1\n490.98\n36.59\n0.00\n470.00\n493.00\n517.00\n562.00\n▁▁▁▁▇\n\n\nGpaCMath\n0\n1\n426.22\n70.60\n0.00\n413.00\n436.00\n457.00\n553.00\n▁▁▁▇▆\n\n\nGpaCTestTakers\n0\n1\n2619.74\n4602.10\n0.00\n93.00\n445.00\n3060.00\n22802.00\n▇▂▁▁▁\n\n\nGpaCVerbal\n0\n1\n431.55\n71.62\n0.00\n415.00\n440.00\n464.00\n548.00\n▁▁▁▇▇\n\n\nGpaDOrLowerMath\n0\n1\n266.64\n209.25\n0.00\n0.00\n389.00\n428.00\n648.00\n▆▁▂▇▁\n\n\nGpaDOrLowerTestTakers\n0\n1\n90.76\n235.37\n0.00\n2.00\n12.00\n90.00\n2061.00\n▇▁▁▁▁\n\n\nGpaDOrLowerVerbal\n0\n1\n272.36\n211.13\n0.00\n0.00\n394.00\n432.00\n632.00\n▆▁▁▇▁\n\n\nGpaNoResponseMath\n0\n1\n304.01\n236.34\n0.00\n0.00\n446.00\n496.00\n589.00\n▇▁▁▆▇\n\n\nGpaNoResponseTestTakers\n0\n1\n1953.19\n3595.06\n0.00\n107.00\n399.00\n2206.00\n26744.00\n▇▁▁▁▁\n\n\nGpaNoResponseVerbal\n0\n1\n315.56\n245.70\n0.00\n0.00\n462.00\n510.00\n616.00\n▇▁▁▆▇\n\n\nGenderFemaleMath\n0\n1\n518.42\n44.22\n368.00\n488.00\n510.00\n551.00\n611.00\n▁▁▇▅▃\n\n\nGenderFemaleTestTakers\n0\n1\n15011.61\n24667.80\n73.00\n1357.00\n3428.00\n18698.00\n133217.00\n▇▁▁▁▁\n\n\nGenderFemaleVerbal\n0\n1\n528.35\n43.47\n399.00\n493.00\n519.00\n569.00\n611.00\n▁▂▇▃▅\n\n\nGenderMaleMath\n0\n1\n553.91\n48.39\n394.00\n521.00\n546.00\n592.00\n640.00\n▁▁▇▆▅\n\n\nGenderMaleTestTakers\n0\n1\n12911.25\n20959.55\n61.00\n1177.00\n2979.00\n16718.00\n108336.00\n▇▁▁▁▁\n\n\nGenderMaleVerbal\n0\n1\n534.88\n45.41\n403.00\n499.00\n525.00\n577.00\n635.00\n▁▃▇▆▃\n\n\nScoreRangesBetween200To300MathFemales\n0\n1\n441.30\n779.45\n0.00\n12.00\n102.00\n518.00\n4294.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300MathMales\n0\n1\n308.42\n509.50\n0.00\n8.00\n60.00\n468.00\n3034.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300MathTotal\n0\n1\n705.21\n1281.79\n0.00\n20.00\n162.00\n628.00\n6772.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300VerbalFemales\n0\n1\n387.74\n793.35\n0.00\n12.00\n46.00\n357.00\n5111.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300VerbalMales\n0\n1\n651.10\n1830.08\n0.00\n14.00\n83.00\n570.00\n20348.00\n▇▁▁▁▁\n\n\nScoreRangesBetween200To300VerbalTotal\n0\n1\n752.66\n1514.44\n0.00\n26.00\n74.00\n790.00\n10603.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400MathFemales\n0\n1\n2180.50\n3961.85\n1.00\n95.00\n599.00\n1972.00\n24977.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400MathMales\n0\n1\n1278.97\n2396.51\n1.00\n57.00\n144.00\n1389.00\n13740.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400MathTotal\n0\n1\n3450.22\n6353.18\n0.00\n149.00\n937.00\n3288.00\n38161.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400VerbalFemales\n0\n1\n2017.92\n4037.56\n1.00\n52.00\n206.00\n1827.00\n22544.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400VerbalMales\n0\n1\n1956.98\n3578.36\n1.00\n74.00\n368.00\n2081.00\n26188.00\n▇▁▁▁▁\n\n\nScoreRangesBetween300To400VerbalTotal\n0\n1\n3669.70\n7235.21\n2.00\n110.00\n394.00\n3530.00\n41262.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500MathFemales\n0\n1\n4597.65\n8104.31\n0.00\n333.00\n670.00\n5262.00\n43758.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500MathMales\n0\n1\n3142.77\n5642.54\n0.00\n125.00\n442.00\n3412.00\n29254.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500MathTotal\n0\n1\n7737.70\n13748.63\n0.00\n493.00\n1096.00\n8698.00\n73012.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500VerbalFemales\n0\n1\n4538.66\n8240.61\n1.00\n198.00\n595.00\n5082.00\n45918.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500VerbalMales\n0\n1\n5540.67\n13135.33\n2.00\n223.00\n892.00\n5351.00\n164622.00\n▇▁▁▁▁\n\n\nScoreRangesBetween400To500VerbalTotal\n0\n1\n8190.33\n14833.61\n0.00\n354.00\n1063.00\n9280.00\n80535.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600MathFemales\n0\n1\n4332.81\n6939.66\n4.00\n369.00\n1006.00\n5593.00\n35778.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600MathMales\n0\n1\n3790.29\n6212.88\n3.00\n292.00\n798.00\n5163.00\n31702.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600MathTotal\n0\n1\n8125.04\n13139.57\n6.00\n651.00\n1830.00\n10753.00\n67480.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600VerbalFemales\n0\n1\n4350.62\n6945.72\n4.00\n356.00\n995.00\n5835.00\n37455.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600VerbalMales\n0\n1\n3760.08\n6010.78\n4.00\n318.00\n871.00\n5341.00\n31449.00\n▇▁▁▁▁\n\n\nScoreRangesBetween500To600VerbalTotal\n0\n1\n8112.57\n12954.97\n1.00\n663.00\n1888.00\n11266.00\n68869.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700MathFemales\n0\n1\n2888.34\n5908.86\n10.00\n284.00\n699.00\n3242.00\n66431.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700MathMales\n0\n1\n3166.88\n5530.24\n15.00\n328.00\n732.00\n4178.00\n49941.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700MathTotal\n0\n1\n6055.63\n11358.02\n26.00\n609.00\n1462.00\n7308.00\n116372.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700VerbalFemales\n0\n1\n2914.30\n5949.39\n13.00\n319.00\n718.00\n3329.00\n71360.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700VerbalMales\n0\n1\n2677.58\n5129.55\n10.00\n302.00\n672.00\n3215.00\n56513.00\n▇▁▁▁▁\n\n\nScoreRangesBetween600To700VerbalTotal\n0\n1\n5592.36\n11069.17\n23.00\n617.00\n1383.00\n6521.00\n127873.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800MathFemales\n0\n1\n792.63\n1787.20\n2.00\n83.00\n223.00\n821.00\n24126.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800MathMales\n0\n1\n1306.64\n2557.59\n1.00\n163.00\n406.00\n1475.00\n30815.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800MathTotal\n0\n1\n2099.26\n4334.33\n1.00\n251.00\n645.00\n2301.00\n54941.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800VerbalFemales\n0\n1\n849.82\n1665.76\n2.00\n123.00\n295.00\n987.00\n21826.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800VerbalMales\n0\n1\n847.27\n1625.23\n2.00\n121.00\n295.00\n970.00\n20460.00\n▇▁▁▁▁\n\n\nScoreRangesBetween700To800VerbalTotal\n0\n1\n1697.12\n3289.67\n4.00\n246.00\n605.00\n1971.00\n42286.00\n▇▁▁▁▁\n\n\n\n\n\n\n\nData Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nYear\nRefers to the year the data was collected\nQualitative\n\n\nStateCode\nRepresent the standardized code for U.S. states\nQualitative\n\n\nTotalTestTakers\nThis column likely shows the total number of students who took the standardized test in each state for that year.\nQuantitative\n\n\nTotal”Subject”\nThis likely represents the average “Subject” score of students in each state, each year.\nQuantitative\n\n\nAcademicSubjects”Subject”AverageGpa\nThis likely represents the average GPA (Grade Point Average) of students in a “subject” for each state, each year.\nQuantitative\n\n\nAcademicSubjects“Subject”AverageYears\nThis column could represent the average number of years students spent studying “subject”, possibly during high school for each state, each year.\nQuantitative\n\n\nFamilyIncomeBetween“Number1_Number2”K“Subject”\nThe average “Subject” score of students whose family income falls between $“Number1” thousand and $“Number2” thousand annually each state, each year.\nQuantitative\n\n\nFamilyIncomeBetween“Number1_Number2”KTestTakers\nThe number of students from families earning between $“Number1” thousand and $“Number2” thousand who took the test.\nQuantitative\n\n\nFamilyIncome“LessThan/MoreThan”“Number”KTestTakers\nThe number of students from families earning Less than or more than $“Number” thousand anually\nQuantitative\n\n\nFamilyIncome”LessThan/MoreThan”“Number”K“Subject”\nThe average “Subject” score of students whose family income falls is less than or more than $“Number” thousand annually\nQuantitative\n\n\nGpa“Grade”“Subject”\nThe average “Subject” score of students with a GPA grade of “Grade”.\nQuantitative\n\n\nGpa“Grade”TestTakers\nThe number of students with a GPA of “Grade” who took the test.\nQuantitative\n\n\nGpaDOrLower“Subject”\nThe average “Subject” score of students with a GPA grade of D or lower.\nQuantitative\n\n\nGpaDOrLowerTestTakers\nThe number of students with a GPA of D or lower who took the test.\nQuantitative\n\n\nGpaNoResponse“Subject”\nThe average “Subject” score of students who did not report their GPA.\nQuantitative\n\n\nGpaNoResponseTestTakers\nThe number of students who did not report their GPA and took the test.\nQuantitative\n\n\nGender“Gender”“Subject”\nThe average “Subject” score of “Gender” students.\nQuantitative\n\n\nGender“Gender”TestTakers\nThe number of “Gender” students who took the test.\nQuantitative\n\n\nScoreRangesBetween“Number1”To“Number2”“Subject”“Gender”\nThe number of “Gender” students whose “Subject” scores fall between “Number1” and “Number2”.\nQuantitative\n\n\nScoreRangesBetween“Number1”To“Number2”“Subject”Total\nThe total number of students (both female and male) whose “Subject” scores fall between “Number1” and “Number2”.\nQuantitative\n\n\n\nI will admit i was over whelmed by the amount of data (in particular the amount of columns in this data set. But i think I’ve gotten a hand of it)\n\nObservations:\n\nThe variables Year, StateCode, and StateName are qualitative (categorical) data but they have a large number of unique levels, meaning they can’t be treated as factors. Maybe Year can be? It only have 10 different levels.\nRest all variables are quantitative either representing average GPA, number of students or average score.\nThere are no missing values for any variable.\n\n\n\n\nTarget and Predictor Variables:\nThe target variables could be either the Math score (TotalMath) or the Verbal score (TotalVerbal). You could treat them as separate target variables or focus on just one. Leaving the rest of the variables to be predictor variables."
  },
  {
    "objectID": "posts/cs2/index.html#defining-the-research-experiment",
    "href": "posts/cs2/index.html#defining-the-research-experiment",
    "title": "Case Study 2",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nThe primary goal of this research could be to predict test scores (in both math and verbal sections) based on various demographic, socioeconomic, and academic factors.\nDemographics: To understand regional and gender-based differences in education outcomes.\n\nStateCode, StateName (location-related factors)\nGenderFemaleTestTakers, GenderMaleTestTakers (gender of the test-takers)\n\nSocioeconomic Variables: Assess how financial background impacts test performance.\n\nFamilyIncomeLessThan20KTestTakers, FamilyIncomeBetween20_40KTestTakers, etc. (family income)\nFamilyIncomeLessThan20KMath, FamilyIncomeBetween20_40KMath, etc. (scores for different income levels)\n\nAcademic Background: To understand the relationship between day-to-day academic success and standardized testing success.\n\nGpaAMinusTestTakers, GpaBTestTakers, etc. (number of test-takers with specific GPAs)\nAcademicSubjectsArtsMusicAverageGpa (average GPA in academic subjects)\n\nWhy do it? To identify which factors have the most significant influence on standardized test scores.\nUse: By understanding the factors influencing standardized test performance could help education policymakers identify gaps in achievement and support more equitable access to resources.\nQuestions:\n\nDoes a higher family income correlate with better test scores.\nDo students with higher GPAs perform better on standardized tests.\nDo certain states consistently outperform others. If yes, does this relate to it’s socioeconomic status or educational policies.\nDoes gender plays a role in performance differences."
  },
  {
    "objectID": "posts/cs2/index.html#graph-1",
    "href": "posts/cs2/index.html#graph-1",
    "title": "Case Study 2",
    "section": "Graph 1:",
    "text": "Graph 1:\n\nType of graph: a Scatter Plot analyzing the visible relationships between different Quantitative variables using GGally.\n\nDefining the question:\n\nAre there patterns of performance consistency or variation between subjects?\nWhat is the Correlation between the average GPA of different subjects? Is being good / bad in one subject mean they are good/bad at all?\n\n\n\nAnalyse the Data:\nIdentifying the variables used: The quantitative values under all AcademicSubjects”Subject”AverageGpa’s quantitative data are plotted against each other. There is no particular data transformation required other than changing the variable names and cleaning the data-set a little more to only have these variables (although neither of these are really necessary)\n\nscores_modified &lt;- scores_cleaned %&gt;%\n  rename(\n    ArtsMusic = AcademicSubjectsArtsMusicAverageGpa,\n    English = AcademicSubjectsEnglishAverageGpa,\n    ForeignLanguages = AcademicSubjectsForeignLanguagesAverageGpa,\n    Mathematics = AcademicSubjectsMathematicsAverageGpa,\n    NaturalSciences = AcademicSubjectsNaturalSciencesAverageGpa,\n    SocialSciencesHistory = AcademicSubjectsSocialSciencesHistoryAverageGpa\n    )%&gt;%\n  select(ArtsMusic, English, ForeignLanguages, Mathematics,NaturalSciences, SocialSciencesHistory)\nscores_modified\n\n# A tibble: 577 × 6\n   ArtsMusic English ForeignLanguages Mathematics NaturalSciences\n       &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1      3.92    3.53             3.54        3.41            3.52\n 2      3.76    3.35             3.34        3.06            3.25\n 3      3.85    3.45             3.41        3.25            3.43\n 4      3.9     3.61             3.64        3.46            3.55\n 5      3.76    3.32             3.29        3.05            3.2 \n 6      3.88    3.49             3.41        3.33            3.43\n 7      3.66    3.13             3.03        3               3.07\n 8      3.71    3.21             3.18        3.07            3.19\n 9      3.54    3.03             3.04        2.91            2.99\n10      3.77    3.29             3.3         3.07            3.27\n# ℹ 567 more rows\n# ℹ 1 more variable: SocialSciencesHistory &lt;dbl&gt;\n\n\n\n\nReplicating the graph:\n\nGGally::ggpairs(\n  scores_modified %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"ArtsMusic\", \"English\", \"ForeignLanguages\", \"Mathematics\", \"NaturalSciences\", \"SocialSciencesHistory\"\n  ),\n  switch = \"both\",\n  progress = FALSE,\n  diag = list(continuous = \"densityDiag\"),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.03, se = FALSE)),\n  title = \"Academic Scores in Different Subjects\"\n)\n\n\n\n\n\n\n\n\n\n\nDoing a Correlation Test:\n\ncor_results &lt;- correlation::correlation(scores_modified)\ncor_results\n\n# Correlation Matrix (pearson-method)\n\nParameter1       |            Parameter2 |    r |       95% CI | t(575) |         p\n-----------------------------------------------------------------------------------\nArtsMusic        |               English | 0.91 | [0.90, 0.93] |  53.42 | &lt; .001***\nArtsMusic        |      ForeignLanguages | 0.90 | [0.88, 0.91] |  49.12 | &lt; .001***\nArtsMusic        |           Mathematics | 0.88 | [0.86, 0.90] |  43.96 | &lt; .001***\nArtsMusic        |       NaturalSciences | 0.93 | [0.91, 0.94] |  58.45 | &lt; .001***\nArtsMusic        | SocialSciencesHistory | 0.94 | [0.93, 0.95] |  66.89 | &lt; .001***\nEnglish          |      ForeignLanguages | 0.96 | [0.96, 0.97] |  87.33 | &lt; .001***\nEnglish          |           Mathematics | 0.96 | [0.95, 0.97] |  81.67 | &lt; .001***\nEnglish          |       NaturalSciences | 0.98 | [0.98, 0.98] | 119.60 | &lt; .001***\nEnglish          | SocialSciencesHistory | 0.98 | [0.98, 0.98] | 124.66 | &lt; .001***\nForeignLanguages |           Mathematics | 0.94 | [0.93, 0.95] |  68.27 | &lt; .001***\nForeignLanguages |       NaturalSciences | 0.97 | [0.96, 0.97] |  88.59 | &lt; .001***\nForeignLanguages | SocialSciencesHistory | 0.96 | [0.95, 0.97] |  82.20 | &lt; .001***\nMathematics      |       NaturalSciences | 0.98 | [0.97, 0.98] | 109.57 | &lt; .001***\nMathematics      | SocialSciencesHistory | 0.96 | [0.95, 0.96] |  78.08 | &lt; .001***\nNaturalSciences  | SocialSciencesHistory | 0.99 | [0.98, 0.99] | 136.99 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 577\n\n\n\nObservations:\nAll combination of subjects have a very small range of confidence intervals. Therefore, all correlation scores are valid i.e. all relationships are statistically significant, reliable, and consistent.\n\n\n\nObservations:\n\nThere is a high positive correlation between all subjects.\nThis consistent positive correlation might indicate that students who perform well in one subject tend to perform well in others. This could suggest a general academic competence or factors like study habits, discipline, or access to resources influencing performance across all subjects.\nThe correlation between ArtsMusic and Mathematics is relatively lower (0.878) compared to the other subject pairs but still strong. This could highlight different skill sets, as Arts/Music may engage creativity, whereas Mathematics is more focused on logical reasoning\nEnglish and Foreign Languages (0.964): These subjects may require similar skills in language comprehension, grammar, and communication.\nSocial Sciences/History and English (0.982) & Natural Science and Social Sciences/History(0.985) & English and Natural Science(0.980): All 3 subjects subjects rely heavily on reading, writing, and critical analysis, which might explain the very high correlation.\nWith mathematics, the subject with highest correlation is NaturalSciences with a score of 0.977. Both subjects share analytical and logical reasoning skills, which could explain the strong link."
  },
  {
    "objectID": "posts/cs2/index.html#graph-2",
    "href": "posts/cs2/index.html#graph-2",
    "title": "Case Study 2",
    "section": "Graph 2:",
    "text": "Graph 2:\n\nType of graph: A box plot.\n\nDefining the question:\nDoes a higher family income correlate with better test scores?\n\n\nAnalyse the Data:\nType of variables: Where a quantitative data (math scores) is plotted as a box plot separated by “levels” of income earned by the family. In this existing data-set, all math scores categorized by income are in different rows (FamilyIncomeBetween”Number1-Number2”KMath). We need to make a new column called IncomeCategory with the different levels of income and another that corresponds with it called Values where the math score data is stored.\n\nincome_data &lt;- scores_cleaned %&gt;%\n  select(\n    FamilyIncomeBetween20_40KMath,\n    FamilyIncomeLessThan20KMath,\n    FamilyIncomeBetween40_60KMath,\n    FamilyIncomeBetween60_80KMath,\n    FamilyIncomeBetween80_100KMath,\n    FamilyIncomeMoreThan100KMath\n  ) %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"IncomeCategory\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(IncomeCategory = case_when(\n    IncomeCategory == \"FamilyIncomeLessThan20KMath\" ~ \"LessThan20K\",\n    IncomeCategory == \"FamilyIncomeBetween20_40KMath\" ~ \"Between20_40K\",\n    IncomeCategory == \"FamilyIncomeBetween40_60KMath\" ~ \"Between40_60K\",\n    IncomeCategory == \"FamilyIncomeBetween60_80KMath\" ~ \"Between60_80K\",\n    IncomeCategory == \"FamilyIncomeBetween80_100KMath\" ~ \"Between80_100K\",\n    IncomeCategory == \"FamilyIncomeMoreThan100KMath\" ~ \"MoreThan100K\",\n    TRUE ~ NA_character_,\n  )) %&gt;%\n  select(IncomeCategory, Value)%&gt;%\n  dplyr::mutate(\n    IncomeCategory = factor(IncomeCategory,\n      levels = c(\"LessThan20K\", \"Between20_40K\", \"Between40_60K\",\"Between60_80K\", \"Between80_100K\", \"MoreThan100K\" ),\n      labels = c(\"LessThan20K\", \"Between20_40K\", \"Between40_60K\",\"Between60_80K\", \"Between80_100K\", \"MoreThan100K\"),\n      ordered = TRUE\n    )\n  )\n\nincome_data\n\n# A tibble: 3,462 × 2\n   IncomeCategory Value\n   &lt;ord&gt;          &lt;dbl&gt;\n 1 Between20_40K    513\n 2 LessThan20K      462\n 3 Between40_60K    539\n 4 Between60_80K    550\n 5 Between80_100K   566\n 6 MoreThan100K     588\n 7 Between20_40K    492\n 8 LessThan20K      464\n 9 Between40_60K    517\n10 Between60_80K    513\n# ℹ 3,452 more rows\n\n\n\nWhat is pivot_longer()?\n\nWide Format: This format typically has multiple columns representing different variables. For example, each family income category might have its own column for math scores.\nLong Format: This format consolidates these variables into two columns: one for the variable names (e.g., income categories) and one for their corresponding values. This format is often easier to work with for data analysis and visualization.\n\n\n\n\nReplicating the graph:\n\nincome_data %&gt;%\n  gf_boxplot(reorder(IncomeCategory, Value, FUN = median) ~ Value,\n    fill = ~IncomeCategory,\n    alpha = 0.2\n  ) %&gt;%\n  gf_labs(title = \"Math Scores vs Family Income\",subtitle = \"Chart 2\" ) %&gt;%\n  gf_labs(\n    x = \"Scores in Math\",\n    y = \"Income Class\"\n  )\n\n\n\n\n\n\n\n\n\nObservations:\n\nThere is a positive correlation between family income and math scores. As family income increases, median math scores also increase, showing that students from higher-income families tend to score better in math.\nThere are several outliers in the lower-income categories (especially for “LessThan20K” and “Between20_40K”) with students scoring exceptionally higher than their peers.\n“LessThan20K” seems to have a couple of outliers at 0. Did a lot of students not show up for tests? It’s interesting to see that other than them, only “Between20_40K” and “MoreThan100K” have students with a score of 0.\nThe chart illustrates that higher family income is associated with better math scores, with less variation in performance as income increases. However, even in lower-income groups, some students perform as well as those from higher-income families."
  },
  {
    "objectID": "posts/cs2/index.html#my-story",
    "href": "posts/cs2/index.html#my-story",
    "title": "Case Study 2",
    "section": "My Story:",
    "text": "My Story:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Statistics and Data Analysis",
    "section": "",
    "text": "Day 6\n\n\n\n\n\n\nCentral Limit Theorem\n\n\nrandamoisation\n\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study 3\n\n\n\n\n\n\nA2\n\n\nAntarctic-Sea-ice\n\n\nbox-plot\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study 2\n\n\n\n\n\n\nA2\n\n\nschool-scores\n\n\nscatter-plot\n\n\nbox-plot\n\n\n\n\n\n\n\n\n\nOct 12, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study 1\n\n\n\n\n\n\nA2\n\n\nmovie_profit\n\n\nbar-plot\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5\n\n\n\n\n\n\ngroups\n\n\nbox-plot\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5 - Part 2\n\n\n\n\n\n\nchange\n\n\nscatter-plot\n\n\ncorrelation\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4\n\n\n\n\n\n\nquantities\n\n\nhistogram\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3\n\n\n\n\n\n\ncounts\n\n\nBar-graph\n\n\ntaxi\n\n\naddiction\n\n\nbanned-books\n\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n\nSneha Manu Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2\n\n\n\n\n\n\nsummaries\n\n\nmgp-data\n\n\nmath-anxiety-data\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\nSneha Jacob\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\n\n\n\nintro\n\n\nbabyname-analysis\n\n\nline-graph\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nSneha Jacob\n\n\n\n\n\n\n\n\n\n\n\n\n Facing the Abyss - WorkFLow\n\n\n\n\n\n\nEDA\n\n\nWorkflow\n\n\nDescriptive\n\n\n\nA complete EDA Workflow\n\n\n\n\n\nOct 21, 2023\n\n\nArvind V\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/CS1/index.html",
    "href": "posts/CS1/index.html",
    "title": "Case Study 1",
    "section": "",
    "text": "This is a dataset pertaining to movies and genres, modified for ease of analysis and plotting."
  },
  {
    "objectID": "posts/CS1/index.html#introduction",
    "href": "posts/CS1/index.html#introduction",
    "title": "Case Study 1",
    "section": "",
    "text": "This is a dataset pertaining to movies and genres, modified for ease of analysis and plotting."
  },
  {
    "objectID": "posts/CS1/index.html#playing-with-and-understanding-the-data",
    "href": "posts/CS1/index.html#playing-with-and-understanding-the-data",
    "title": "Case Study 1",
    "section": "Playing with and understanding the data",
    "text": "Playing with and understanding the data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\n\n\nAcquiring the data:\n\nmovie_profit &lt;- read_delim(\"../../data/movie_profit.csv\", delim=\";\")\n\nRows: 3310 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (4): movie, distributor, mpaa_rating, genre\ndbl  (4): production_budget, domestic_gross, worldwide_gross, decade\nnum  (1): profit_ratio\ndate (1): release_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmovie_profit\n\n# A tibble: 3,310 × 10\n   release_date movie           production_budget domestic_gross worldwide_gross\n   &lt;date&gt;       &lt;chr&gt;                       &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n 1 2005-07-22   November                   250000         191862          191862\n 2 1998-08-28   I Married a St…            250000         203134          203134\n 3 1997-03-28   Love and Other…            250000         212285          743216\n 4 2000-07-14   Chuck&Buck                 250000        1055671         1157672\n 5 2011-10-28   Like Crazy                 250000        3395391         3728400\n 6 2003-04-11   Better Luck To…            250000        3802390         3809226\n 7 2017-04-28   Sleight                    250000        3930990         3934450\n 8 2002-06-28   Lovely and Ama…            250000        4210379         4613482\n 9 2012-08-17   Compliance                 270000         319285          830700\n10 2005-05-06   Fighting Tommy…            300000          10514           10514\n# ℹ 3,300 more rows\n# ℹ 5 more variables: distributor &lt;chr&gt;, mpaa_rating &lt;chr&gt;, genre &lt;chr&gt;,\n#   profit_ratio &lt;dbl&gt;, decade &lt;dbl&gt;\n\n\n\nglimpse(movie_profit)\n\nRows: 3,310\nColumns: 10\n$ release_date      &lt;date&gt; 2005-07-22, 1998-08-28, 1997-03-28, 2000-07-14, 201…\n$ movie             &lt;chr&gt; \"November\", \"I Married a Strange Person\", \"Love and …\n$ production_budget &lt;dbl&gt; 250000, 250000, 250000, 250000, 250000, 250000, 2500…\n$ domestic_gross    &lt;dbl&gt; 191862, 203134, 212285, 1055671, 3395391, 3802390, 3…\n$ worldwide_gross   &lt;dbl&gt; 191862, 203134, 743216, 1157672, 3728400, 3809226, 3…\n$ distributor       &lt;chr&gt; \"Other\", \"Other\", \"Other\", \"Other\", \"Paramount Pictu…\n$ mpaa_rating       &lt;chr&gt; \"R\", NA, \"R\", \"R\", \"PG-13\", \"R\", \"R\", \"R\", \"R\", \"R\",…\n$ genre             &lt;chr&gt; \"Drama\", \"Comedy\", \"Comedy\", \"Drama\", \"Drama\", \"Dram…\n$ profit_ratio      &lt;dbl&gt; 7.674480e+13, 8.125360e+13, 2.972864e+14, 4.630688e+…\n$ decade            &lt;dbl&gt; 2000, 1990, 1990, 2000, 2010, 2000, 2010, 2000, 2010…\n\n\n\nskim(movie_profit)\n\n\nData summary\n\n\nName\nmovie_profit\n\n\nNumber of rows\n3310\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nDate\n1\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmovie\n0\n1.00\n1\n35\n0\n3310\n0\n\n\ndistributor\n42\n0.99\n5\n18\n0\n6\n0\n\n\nmpaa_rating\n130\n0.96\n1\n5\n0\n4\n0\n\n\ngenre\n0\n1.00\n5\n9\n0\n5\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nrelease_date\n0\n1\n1936-02-05\n2017-12-22\n2005-06-30\n1723\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nproduction_budget\n0\n1\n3.326794e+07\n3.460741e+07\n2.50e+05\n9.500000e+06\n2.000000e+07\n4.500000e+07\n1.750000e+08\n▇▂▁▁▁\n\n\ndomestic_gross\n0\n1\n4.551509e+07\n5.852794e+07\n0.00e+00\n6.530094e+06\n2.558731e+07\n6.046695e+07\n4.745447e+08\n▇▁▁▁▁\n\n\nworldwide_gross\n0\n1\n9.384123e+07\n1.389514e+08\n4.23e+02\n1.086144e+07\n4.040903e+07\n1.184703e+08\n1.162782e+09\n▇▁▁▁▁\n\n\nprofit_ratio\n0\n1\n4.319388e+14\n1.501736e+15\n1.38e+10\n7.861269e+13\n1.962499e+14\n3.942158e+14\n4.315179e+16\n▇▁▁▁▁\n\n\ndecade\n0\n1\n1.998790e+03\n1.061000e+01\n1.93e+03\n1.990000e+03\n2.000000e+03\n2.010000e+03\n2.010000e+03\n▁▁▁▃▇\n\n\n\n\n\n\n\nData Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nmovie\nThe title of the movie\nQualitative\n\n\ndistributor\nThe company responsible for distributing the movie (e.g., Warner Bros., Disney)\nQualitative\n\n\nmpaa_rating\nThe movie’s MPAA rating (e.g., G, PG, PG-13, R), which indicates the age-appropriateness of the content.\nQualitative\n\n\ngenre\nThe type or category of the movie (e.g., Action, Comedy, Drama)\nQualitative\n\n\nrelease_date\nThe date when the movie was first released in theaters\nQualitative\n\n\nproduction_budget\nThe amount of money spent on producing the movie\nQuantitative\n\n\ndomestic_gross\nThe total earnings of the movie from domestic (usually U.S.) markets\nQuantitative\n\n\nworldwide_gross\nThe total earnings of the movie from all markets worldwide\nQuantitative\n\n\nprofit_ratio\nThe ratio of profit earned relative to the production budget\nQuantitative\n\n\ndecade\nThe decade during which the movie was released\nQualitative\n\n\n\n\nObservations:\n\nThe variables mpaa_rating, genre, and distributor are qualitative (categorical) data with a low enough number of unique levels that they could be treated as factors.\nDecade is quantitative data but could also be treated as a factor.\nThere are missing values in the dataset for mpaa_rating and distributor.\n\n\n\n\nTransforming the data\n\nmovie_modified &lt;- movie_profit %&gt;%\n  dplyr::mutate(\n    mpaa_rating = as_factor(mpaa_rating),\n    genre = as_factor(genre),\n    distributor = as_factor(distributor),\n    decade = as_factor(decade)\n  )\nmovie_modified\n\n# A tibble: 3,310 × 10\n   release_date movie           production_budget domestic_gross worldwide_gross\n   &lt;date&gt;       &lt;chr&gt;                       &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n 1 2005-07-22   November                   250000         191862          191862\n 2 1998-08-28   I Married a St…            250000         203134          203134\n 3 1997-03-28   Love and Other…            250000         212285          743216\n 4 2000-07-14   Chuck&Buck                 250000        1055671         1157672\n 5 2011-10-28   Like Crazy                 250000        3395391         3728400\n 6 2003-04-11   Better Luck To…            250000        3802390         3809226\n 7 2017-04-28   Sleight                    250000        3930990         3934450\n 8 2002-06-28   Lovely and Ama…            250000        4210379         4613482\n 9 2012-08-17   Compliance                 270000         319285          830700\n10 2005-05-06   Fighting Tommy…            300000          10514           10514\n# ℹ 3,300 more rows\n# ℹ 5 more variables: distributor &lt;fct&gt;, mpaa_rating &lt;fct&gt;, genre &lt;fct&gt;,\n#   profit_ratio &lt;dbl&gt;, decade &lt;fct&gt;\n\n\n\nglimpse(movie_modified)\n\nRows: 3,310\nColumns: 10\n$ release_date      &lt;date&gt; 2005-07-22, 1998-08-28, 1997-03-28, 2000-07-14, 201…\n$ movie             &lt;chr&gt; \"November\", \"I Married a Strange Person\", \"Love and …\n$ production_budget &lt;dbl&gt; 250000, 250000, 250000, 250000, 250000, 250000, 2500…\n$ domestic_gross    &lt;dbl&gt; 191862, 203134, 212285, 1055671, 3395391, 3802390, 3…\n$ worldwide_gross   &lt;dbl&gt; 191862, 203134, 743216, 1157672, 3728400, 3809226, 3…\n$ distributor       &lt;fct&gt; Other, Other, Other, Other, Paramount Pictures, Para…\n$ mpaa_rating       &lt;fct&gt; R, NA, R, R, PG-13, R, R, R, R, R, R, R, PG-13, NA, …\n$ genre             &lt;fct&gt; Drama, Comedy, Comedy, Drama, Drama, Drama, Action, …\n$ profit_ratio      &lt;dbl&gt; 7.674480e+13, 8.125360e+13, 2.972864e+14, 4.630688e+…\n$ decade            &lt;fct&gt; 2000, 1990, 1990, 2000, 2010, 2000, 2010, 2000, 2010…\n\n\n\nskim(movie_modified)\n\n\nData summary\n\n\nName\nmovie_modified\n\n\nNumber of rows\n3310\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nfactor\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmovie\n0\n1\n1\n35\n0\n3310\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nrelease_date\n0\n1\n1936-02-05\n2017-12-22\n2005-06-30\n1723\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndistributor\n42\n0.99\nFALSE\n6\nOth: 1737, War: 360, Son: 332, Uni: 299\n\n\nmpaa_rating\n130\n0.96\nFALSE\n4\nR: 1477, PG-: 1066, PG: 552, G: 85\n\n\ngenre\n0\n1.00\nFALSE\n5\nDra: 1209, Com: 798, Act: 547, Adv: 467\n\n\ndecade\n0\n1.00\nFALSE\n9\n200: 1387, 201: 994, 199: 607, 198: 228\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nproduction_budget\n0\n1\n3.326794e+07\n3.460741e+07\n2.50e+05\n9.500000e+06\n2.000000e+07\n4.500000e+07\n1.750000e+08\n▇▂▁▁▁\n\n\ndomestic_gross\n0\n1\n4.551509e+07\n5.852794e+07\n0.00e+00\n6.530094e+06\n2.558731e+07\n6.046695e+07\n4.745447e+08\n▇▁▁▁▁\n\n\nworldwide_gross\n0\n1\n9.384123e+07\n1.389514e+08\n4.23e+02\n1.086144e+07\n4.040903e+07\n1.184703e+08\n1.162782e+09\n▇▁▁▁▁\n\n\nprofit_ratio\n0\n1\n4.319388e+14\n1.501736e+15\n1.38e+10\n7.861269e+13\n1.962499e+14\n3.942158e+14\n4.315179e+16\n▇▁▁▁▁\n\n\n\n\n\n\n\nTarget and Predictor Variables:\nIn the context of your movie_profit data-set, a potential target variable could be profit_ratio (the name of the data-set does suggest so anyway). Leaving the rest of the variables- distributor, mpaa_rating, genre, decade, release_date, production_budget, domestic_gross and worldwide_gross to be predictor variables."
  },
  {
    "objectID": "posts/CS1/index.html#defining-the-research-experiment",
    "href": "posts/CS1/index.html#defining-the-research-experiment",
    "title": "Case Study 1",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nTo analyze factors that might influence the profitability of movies. Using statistical techniques, researchers might examine the relationships between the predictor variables and the target variable (profit ratio) to determine what factors contribute to higher profitability in films.\nUse: The study’s findings could inform studios, distributors, and investors about what factors are most likely to lead to a movie being profitable, potentially influencing future decisions in movie production.\n\nAnalyzing the Predictor Variables:\n\nDistributors:\n\nmovie_modified %&gt;% count(distributor)\n\n# A tibble: 7 × 2\n  distributor            n\n  &lt;fct&gt;              &lt;int&gt;\n1 Other               1737\n2 Paramount Pictures   261\n3 Universal            299\n4 20th Century Fox     279\n5 Sony Pictures        332\n6 Warner Bros.         360\n7 &lt;NA&gt;                  42\n\n\n\nObservations:\n\n“Other” Distributors: Largest group with 1,737 entries, suggesting many smaller or less-known distributors.\nTop Distributors include Warner Bros. (360), Sony Pictures (332), Universal (299), 20th Century Fox (279), and Paramount Pictures (261).\n\n\n\n\nMpaa Rating:\n\nmovie_modified %&gt;% count(mpaa_rating)\n\n# A tibble: 5 × 2\n  mpaa_rating     n\n  &lt;fct&gt;       &lt;int&gt;\n1 R            1477\n2 PG-13        1066\n3 PG            552\n4 G              85\n5 &lt;NA&gt;          130\n\n\n\nObservations:\nThis indicates a higher prevalence of R-rated and PG-13 movies in the data-set, with fewer G-rated films.\n\n\n\nGenre:\n\nmovie_modified %&gt;% count(genre)\n\n# A tibble: 5 × 2\n  genre         n\n  &lt;fct&gt;     &lt;int&gt;\n1 Drama      1209\n2 Comedy      798\n3 Action      547\n4 Horror      289\n5 Adventure   467\n\n\n\nObservations:\nThis distribution shows a preference for Drama and Comedy in the dataset, while Horror appears less often.\n\n\n\nDecade:\n\nmovie_modified %&gt;% count(decade)\n\n# A tibble: 9 × 2\n  decade     n\n  &lt;fct&gt;  &lt;int&gt;\n1 1930       2\n2 1940       4\n3 1950       6\n4 1960      19\n5 1970      63\n6 1980     228\n7 1990     607\n8 2000    1387\n9 2010     994\n\n\n\nObservations:\nThe distribution indicates a clear trend of increasing movie releases from the 1930s to the 2000s, with a noticeable decline in earlier decades.\n\n\n\nProduction Budget:\n\nmovie_modified %&gt;%\n  gf_histogram(~production_budget)\n\n\n\n\n\n\n\n\n\n\nDomestic Gross:\n\nmovie_modified %&gt;%\n  gf_histogram(~domestic_gross)\n\n\n\n\n\n\n\n\n\n\nWorldwide Gross:\n\nmovie_modified %&gt;%\n  gf_histogram(~worldwide_gross)\n\n\n\n\n\n\n\n\n\nObservations:\nThe histograms of all the quantitative predictor variables are similarly skewed to the right. A rightward skew indicates that a majority of the data points are concentrated on the lower end of the scale, with fewer high values. This could suggest that most movies in my data-set have lower production budgets or grosses, while only a few have significantly higher values.The uniform skewness across multiple variables suggests a consistent pattern in the data-set, which can indicate underlying trends or characteristics shared among the movies"
  },
  {
    "objectID": "posts/CS1/index.html#define-the-question",
    "href": "posts/CS1/index.html#define-the-question",
    "title": "Case Study 1",
    "section": "Define the Question:",
    "text": "Define the Question:\nGraph:\n\nIdentifying the type of plot: A bar plot where a quantitative variable is plotted rather than a qualitative one.\nQuestions:\n\nHow do the distributor and genre affect a movie’s profitability?\nWhich genre is most likely to being the most profit for a distributor.\nIf i have a horror movie i want to bring to the theaters, which distributor would be my best bet?\n\n\nGraph Replication:\nI examined the variables plotted- We are plotting a quantitative variable against 2 qualitative variables. Has the profit ratio been transformed in any way to be used as a bar graph- one where the count of qualitative variables are plotted? A histogram is used to plot continuous quantitative data that could be faceted by groups that are defined by levels of a qualitative data. But is it possible to plot it against 2 qulaitative variables? No, You cannot meaningfully divide categories into bins, which is the core concept behind a histogram.\nMaybe i could use groups to understand this data? The x axis of the graph does say median profit ratio. Using box-plots, i do find the different medians.\n\nmovie_modified_again &lt;- movie_modified %&gt;%\n  dplyr::mutate(\n    genre = factor(genre,\n      levels = c(\"Action\", \"Adventure\", \"Comedy\", \"Drama\", \"Horror\" ),\n      labels = c(\"Action\", \"Adventure\", \"Comedy\", \"Drama\", \"Horror\"),\n      ordered = TRUE\n    )\n  )\n\n\nmovie_modified_again %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(genre, profit_ratio) ~ profit_ratio,\n    fill = ~genre,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(distributor)) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Profits made by film distributors\",\n    subtitle = \"Ratio of profits to budgets\",\n    x = \"Median profit ratio\",\n    y = \"genre\"\n  )\n\n\n\n\n\n\n\n\n\nmovie_modified_again %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(genre, profit_ratio) ~ log10(profit_ratio),\n    fill = ~genre,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(distributor)) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Profits made by film distributors\",\n    subtitle = \"Ratio of profits to budgets\",\n    x = \"Median profit ratio\",\n    y = \"genre\"\n  )\n\n\n\n\n\n\n\n\nAlthough bar graphs are commonly used to represent the count or frequency of qualitative (categorical) data. They can also be used to represent quantitative data if you summarize the data in some way (e.g., using means, sums, or other aggregates).\n\nggplot(movie_modified_again, aes(x = genre, y = profit_ratio)) +\n  facet_wrap(vars(distributor))+\n  stat_summary(fun = \"median\", geom = \"bar\") +\n  labs(title = \"Profits made by film distributors\", subtitle = \"Ratio of profits to budgets\", x = \"Genre\", y = \"Median Profit Ratio\")+  \n  coord_flip()\n\n\n\n\n\n\n\n\nI tried doing this with ggformula using stat = “summary” and fun = “median” but it didn’t work for some reason to i tried it out with ggplot2 and it did work.\nI know that I am only required to make this graph, but I’m curious about how to graph would look if i plot the mean instead of the median. Would it be easier to pull out inferences from the mean plot?\nfacet_wrap is used to obtain separate plots for each distributor.\n\nggplot(movie_modified_again, aes(x = genre, y = profit_ratio)) +\n  facet_wrap(vars(distributor))+\n  stat_summary(fun = \"mean\", geom = \"bar\") +\n  labs(title = \"Profits made by film distributors\", subtitle = \"Ratio of profits to budgets\", x = \"Genre\", y = \"Mean Profit Ratio\")+  \n  coord_flip()\n\n\n\n\n\n\n\n\nSo after a little bit of research, I realise, mean is more sensitive to outliers and extreme values, so it works well when the data is normally distributed and without significant skewness while Median is more robust in the presence of skewed data or outliers because it represents the middle value.\nWhen i observe the box plot, there are couple of outliers present of both sides making the data skewed, in which case, it could mislead the interpretation.Therefore, the median often provides a better sense of the “typical” value, making it easier to draw meaningful inferences.\n\n\nInferences:\n\nParamount pictures, Warner Bros and Sony Pictures seem to most likely gain the highest profits through horror movies.\nAlmost all distributors seem to make the lest amount of profits through drama movies (it’s a close second otherwise). This is interesting because if we looks at the count of all genres in the data-set, drama is the most. Is this a peculiarity of the data-set? If not, if it is most likely to bring in the least profit, why do they make it so much?\n\nThis makes me curious about the count of genre by distributors.\n\ngf_bar(~ genre | distributor, fill = ~genre, data = movie_modified) %&gt;%\n  gf_labs(title = \"Counts of genre by distributors\")\n\n\n\n\n\n\n\n\n\nThis is interesting because even though Paramount pictures, Warner Bros and Sony Pictures are most likely to gain highest profits through horror movies, these movies are of the least count in their data-set.\nFor Universal Studio and 20th Century Fox, adventure is the genre that’s most likely to bring in a high profit. Supporting this, 20th Century Fox does have a high count of adventure films, but Universal studio that has a higher median profit ratio for Adventure even compared to 20th Century fox, has a very low count (the second lowest they have) when it comes to adventure movies.\nWhile comedy brings in a good profit ratio for Universal Studio and and 20th Century Fox, not so much for any of the other distributors. What might that be about? The count of comedy films seem to be similar for all distributors despite this.\nIf I were to invest in movie production ventures, which are the two best genres that you might decide to invest in? It would be either horror or adventure. I’m most likely to gain profits with these 2 options."
  },
  {
    "objectID": "posts/CS1/index.html#my-story",
    "href": "posts/CS1/index.html#my-story",
    "title": "Case Study 1",
    "section": "My Story:",
    "text": "My Story:\nI have gone through the Case Studies provided in Arvind’s websites before the specifications of A2 came through. At that point we were two classes down and I remember feeling like everything was way out of my ballpark. I never looked at it again. So when we did get our A2 assignment, I panicked a little. When i did open case studies again, as i glimpsed through the preview images within each document, i jumped at clicking this movies profits one cause it looks like a bar plot. How hard could it possibly be?"
  },
  {
    "objectID": "posts/cs3/index.html",
    "href": "posts/cs3/index.html",
    "title": "Case Study 3",
    "section": "",
    "text": "This data-set pertains to the extent of Antarctic Sea Ice over time that is monitored by the National Snow and Ice Data Center\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/cs3/index.html#introduction",
    "href": "posts/cs3/index.html#introduction",
    "title": "Case Study 3",
    "section": "",
    "text": "This data-set pertains to the extent of Antarctic Sea Ice over time that is monitored by the National Snow and Ice Data Center\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/cs3/index.html#playing-with-and-understanding-the-data",
    "href": "posts/cs3/index.html#playing-with-and-understanding-the-data",
    "title": "Case Study 3",
    "section": "Playing with and understanding the data",
    "text": "Playing with and understanding the data\n\nAcquiring the data:\n\nice &lt;- read_excel(\"../../data/ice.xlsx\", , sheet = \"SH-Daily-Extent\")\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...50`\n\nice\n\n# A tibble: 366 × 52\n   ...1     ...2 `1978` `1979` `1980` `1981` `1982` `1983` `1984` `1985` `1986`\n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 January     1     NA  NA      5.97   6.32  NA      6.51  NA     NA      7.72\n 2 &lt;NA&gt;        2     NA   6.94  NA     NA      7.04  NA      6.94   6.53  NA   \n 3 &lt;NA&gt;        3     NA  NA      5.67   5.79  NA      6.17  NA     NA      7.57\n 4 &lt;NA&gt;        4     NA   6.84  NA     NA      6.69  NA      6.65   6.06  NA   \n 5 &lt;NA&gt;        5     NA  NA      5.58   5.35  NA      5.87  NA     NA      7.24\n 6 &lt;NA&gt;        6     NA   6.64  NA     NA      6.39  NA      6.30   5.66  NA   \n 7 &lt;NA&gt;        7     NA  NA      5.33   5.19  NA      5.66  NA     NA      6.81\n 8 &lt;NA&gt;        8     NA   6.27  NA     NA      6.08  NA      5.94   5.31  NA   \n 9 &lt;NA&gt;        9     NA  NA      5      4.78  NA      5.30  NA     NA      6.28\n10 &lt;NA&gt;       10     NA   6.14  NA     NA      5.86  NA      5.63   4.93  NA   \n# ℹ 356 more rows\n# ℹ 41 more variables: `1987` &lt;dbl&gt;, `1988` &lt;dbl&gt;, `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;,\n#   `1991` &lt;dbl&gt;, `1992` &lt;dbl&gt;, `1993` &lt;dbl&gt;, `1994` &lt;dbl&gt;, `1995` &lt;dbl&gt;,\n#   `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;, `1999` &lt;dbl&gt;, `2000` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2005` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, …\n\n\n\nObservations (list of imperfections):\n\nThere is no data collected for 1978. For the next 9 years (till 1987), data has been entered every alternate day. Fom the mid of 1987, data has been entered every day but there by end ( from december 3rd) there is missing data till the end of that year.\nOn 1988- there is no data for the first few days of the week- first 12 days of January.\nWhat am i supposed to do with the data on Feb 29 only every 4 years?\nThe month was only entered for the 1st day of every month, Making the value of every other data entry of each month NA\n\n\n\n\nData Dictionary:\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\n..1\nRefers to the month the data was collected in\nQualitative\n\n\n..2\nRefers to date of the month the data was collected in\nQualitative\n\n\nYear (1978, 1979 … 2027)\nSea Ice Extent - the total area covered by sea ice, often measured in square kilometers based on the year, month and day\nQuantitative\n\n\n1981-2010 mean\nThis variable represents the average value for sea ice extent calculated over the 30-year period from 1981 to 2010.\nQuantitative\n\n\n1981-2010 median\nThis variable represents the median value for sea ice extent calculated over the 30-year period from 1981 to 2010.\nQuantitative\n\n\n\n\n\nTarget and Predictor Variables:\nTarget variable could be daily sea ice levels across multiple years.\nPredictor Variables:\n\nYear: Trends over time can have a major impact on sea ice due to climate change.\nMonth: Sea ice varies seasonally, so the month is a key predictor.\nDay: In daily data, day-to-day variations can be tracked."
  },
  {
    "objectID": "posts/cs3/index.html#defining-the-research-experiment",
    "href": "posts/cs3/index.html#defining-the-research-experiment",
    "title": "Case Study 3",
    "section": "Defining the Research Experiment:",
    "text": "Defining the Research Experiment:\nIt could have been aimed at monitoring, measuring, and understanding changes in Antarctic sea ice over time. In order to do this, extensive data on sea ice coverage and its fluctuations have been collected over the span of many years.This would be used to identify trends in sea ice extent over the long-term (year-to-year trends) and short-term (daily or seasonal fluctuations).\nUse: Understanding the trend in Antarctic sea ice can provide critical insight into global climate change and its effects on the polar regions.\nQuestions:\n\nHow has the Antarctic sea ice extent changed from 1978 to the present day?\nAre there significant seasonal differences in sea ice over decades?\nHas the extent of Antarctic sea ice significantly reduced over the past 40 years due to rising global temperatures?"
  },
  {
    "objectID": "posts/cs3/index.html#graph",
    "href": "posts/cs3/index.html#graph",
    "title": "Case Study 3",
    "section": "Graph:",
    "text": "Graph:\n\nType of graph: A box plot.\n\nDefining the question:\n\nWhat happened to Antarctic sea ice extent in 2023 compared to historical data from 1980 to 2022?\nHow did the Antarctic ice extent in 2023 differ from the historical norms?\n\n\n\nAnalyzing the data:\nTypes of Variables: quantitative variables representing the ice extent that are stored unter mutiple columns seperated by the year the time they were collected in. Similar to my last case study, i need all the quantiative data i should plot in one column and so i creat a new data-set where a new row called series is created where the year is stored and the ice extent value under is stored in well… values.\n\nice %&gt;%\n  select(\"month\" = ...1, \"day\" = ...2, c(4:49)) %&gt;%\n  tidyr::fill(month) %&gt;%\n  pivot_longer(\n    cols = -c(month, day),\n    names_to = \"series\",\n    values_to = \"values\"\n  ) %&gt;%\n  mutate(\n    series = as.integer(series),\n    month = factor(month,\n      levels = month.name,\n      labels = month.name,\n      ordered = TRUE\n    ),\n  ) -&gt; ice_prepared\n\nice_prepared\n\n# A tibble: 16,836 × 4\n   month     day series values\n   &lt;ord&gt;   &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;\n 1 January     1   1979  NA   \n 2 January     1   1980   5.97\n 3 January     1   1981   6.32\n 4 January     1   1982  NA   \n 5 January     1   1983   6.51\n 6 January     1   1984  NA   \n 7 January     1   1985  NA   \n 8 January     1   1986   7.72\n 9 January     1   1987  NA   \n10 January     1   1988  NA   \n# ℹ 16,826 more rows\n\n\n\n\nReplicating the graph:\n\nice_prepared %&gt;%\n  filter(is.finite(values)) %&gt;%\n  gf_boxplot(values ~ month, color = \"darkgrey\") %&gt;%  \n  gf_point(values ~ month, data = subset(ice_prepared, series == 2023), color = \"salmon\", size = 3, shape = 15) %&gt;%  \n  gf_labs(title = \"Antarctic Ice Area over the years 1980 to 2023\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, all values of the year 2023 are being plotted across the month they are inputted in. Can i put only the average value of ice extent each month?\n\nice_avg_2023 &lt;- ice_prepared %&gt;%\n  filter(series == 2023) %&gt;%\n  group_by(month) %&gt;%                    \n  summarise(avg_value = mean(values, na.rm = TRUE))  \nice_avg_2023\n\n# A tibble: 12 × 2\n   month     avg_value\n   &lt;ord&gt;         &lt;dbl&gt;\n 1 January        3.23\n 2 February       1.91\n 3 March          2.80\n 4 April          5.49\n 5 May            8.36\n 6 June          11.0 \n 7 July          13.5 \n 8 August        15.5 \n 9 September     16.8 \n10 October       16.2 \n11 November      14.3 \n12 December       8.67\n\n\n\nice_prepared %&gt;%\n  filter(is.finite(values)) %&gt;%\n  gf_boxplot(values ~ month, color = \"darkgrey\") %&gt;%  \n  gf_point(data = ice_avg_2023, avg_value ~ month, color = \"salmon\", size = 3, shape = 15) %&gt;%  \n  gf_labs(title = \"Antarctic Ice Area over the years 1980 to 2023\", subtitle = \"What happend in 2023?\", x = \"Month\", y = \"Ice Evtent in million square km\", caption = \"Data: National Snow and Ice data center\")\n\n\n\n\n\n\n\n\n\nObservations:\n\nThere is a seasonal pattern in the Antarctic ice extent, with ice reaching its maximum around September and its minimum around February . This pattern seems to be consistent with the natural annual cycle of ice melting and formation in the region.\nFor all months, the ice extent in 2023 is lower than the historical median. June through October seems to have it being far more lower than the median compared to the other months."
  },
  {
    "objectID": "posts/cs3/index.html#my-story",
    "href": "posts/cs3/index.html#my-story",
    "title": "Case Study 3",
    "section": "My Story:",
    "text": "My Story:"
  },
  {
    "objectID": "posts/day-3(2)/index.html",
    "href": "posts/day-3(2)/index.html",
    "title": "Day 3",
    "section": "",
    "text": "Today we worked on count of qualitative data. We also looked in creation of bar graph - data in lengths.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing"
  },
  {
    "objectID": "posts/day-3(2)/index.html#introduction",
    "href": "posts/day-3(2)/index.html#introduction",
    "title": "Day 3",
    "section": "",
    "text": "Today we worked on count of qualitative data. We also looked in creation of bar graph - data in lengths.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing"
  },
  {
    "objectID": "posts/day-3(2)/index.html#taxi-data-set",
    "href": "posts/day-3(2)/index.html#taxi-data-set",
    "title": "Day 3",
    "section": "Taxi data-set",
    "text": "Taxi data-set\n\ntaxi &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/taxi.csv\")\n\nRows: 10000 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): tip, company, local, dow, month\ndbl (3): rownames, distance, hour\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntaxi\n\n# A tibble: 10,000 × 8\n   rownames tip   distance company                      local dow   month  hour\n      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1        1 yes      17.2  Chicago Independents         no    Thu   Feb      16\n 2        2 yes       0.88 City Service                 yes   Thu   Mar       8\n 3        3 yes      18.1  other                        no    Mon   Feb      18\n 4        4 yes      20.7  Chicago Independents         no    Mon   Apr       8\n 5        5 yes      12.2  Chicago Independents         no    Sun   Mar      21\n 6        6 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n 7        7 yes      17.5  Flash Cab                    no    Fri   Mar      12\n 8        8 yes      17.7  other                        no    Sun   Jan       6\n 9        9 yes       1.85 Taxicab Insurance Agency Llc no    Fri   Apr      12\n10       10 yes       1.47 City Service                 no    Tue   Mar      14\n# ℹ 9,990 more rows\n\n\nWe first glimpse it to identify what variables need to be converted to factors.\n\ntaxi %&gt;% dplyr::glimpse()\n\nRows: 10,000\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ tip      &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\"…\n$ distance &lt;dbl&gt; 17.19, 0.88, 18.11, 20.70, 12.23, 0.94, 17.47, 17.67, 1.85, 1…\n$ company  &lt;chr&gt; \"Chicago Independents\", \"City Service\", \"other\", \"Chicago Ind…\n$ local    &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\",…\n$ dow      &lt;chr&gt; \"Thu\", \"Thu\", \"Mon\", \"Mon\", \"Sun\", \"Sat\", \"Fri\", \"Sun\", \"Fri\"…\n$ month    &lt;chr&gt; \"Feb\", \"Mar\", \"Feb\", \"Apr\", \"Mar\", \"Apr\", \"Mar\", \"Jan\", \"Apr\"…\n$ hour     &lt;dbl&gt; 16, 8, 18, 8, 21, 23, 12, 6, 12, 14, 18, 11, 12, 19, 17, 13, …\n\n\nBefore we do mutate stuff into factors, it helps to know the number of levels of each variable, just to make sure. For do inspect and skim.\n\ntaxi %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n     name     class levels     n missing\n1     tip character      2 10000       0\n2 company character      7 10000       0\n3   local character      2 10000       0\n4     dow character      7 10000       0\n5   month character      4 10000       0\n                                   distribution\n1 yes (92.1%), no (7.9%)                       \n2 other (27.1%) ...                            \n3 no (81.2%), yes (18.8%)                      \n4 Thu (19.6%), Wed (17.5%), Tue (16.3%) ...    \n5 Apr (31.8%), Mar (31.4%), Feb (20.4%) ...    \n\nquantitative variables:  \n      name   class min      Q1  median        Q3     max        mean\n1 rownames numeric   1 2500.75 5000.50 7500.2500 10000.0 5000.500000\n2 distance numeric   0    0.94    1.78   15.5625    42.3    6.224144\n3     hour numeric   0   11.00   15.00   18.0000    23.0   14.177300\n           sd     n missing\n1 2886.895680 10000       0\n2    7.381397 10000       0\n3    4.359904 10000       0\n\n\n\ntaxi %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntip\n0\n1\n2\n3\n0\n2\n0\n\n\ncompany\n0\n1\n5\n28\n0\n7\n0\n\n\nlocal\n0\n1\n2\n3\n0\n2\n0\n\n\ndow\n0\n1\n3\n3\n0\n7\n0\n\n\nmonth\n0\n1\n3\n3\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrownames\n0\n1\n5000.50\n2886.90\n1\n2500.75\n5000.50\n7500.25\n10000.0\n▇▇▇▇▇\n\n\ndistance\n0\n1\n6.22\n7.38\n0\n0.94\n1.78\n15.56\n42.3\n▇▁▂▁▁\n\n\nhour\n0\n1\n14.18\n4.36\n0\n11.00\n15.00\n18.00\n23.0\n▁▃▅▇▃\n\n\n\n\n\n\nTable of Variables\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\ntip\nRepresents if the tip was given or not\nQualitative\n\n\ncompany\nThis variable indicates the taxi company or vendor that operates the vehicle.\nQualitative\n\n\nlocal\nThis variable may refer to whether the trip occurred within a local area or not\nQualitative\n\n\ndow\nhis variable represents the day of the week when the taxi ride took place\nQualitative\n\n\nmonth\nThis variable indicates the month during which the taxi ride occurred.\nQualitative\n\n\ndistance\nThis variable measures the total distance traveled during the taxi ride, usually expressed in miles\nQuantitative\n\n\nhour\nThis variable indicates the hour of the day when the trip started\nQuantitative\n\n\n\nTarget Variable: tip\nPredictor Variables: company, dow, local, month, hour\n\nAnd then we mutate. I feel like i scientist when i use the word mutate. We glimpse it again, cause why not.\n\n\ntaxi_modified &lt;- taxi %&gt;%\n  dplyr::mutate(\n    tip = factor(tip,\n      levels = c(\"no\", \"yes\"),\n      labels = c(\"no\", \"yes\"),\n      ordered = TRUE\n    ),\n    company = as_factor(company),\n    ##\n    dow = factor(dow,\n      levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"),\n      labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"),\n      ordered = TRUE\n    ),\n    ##\n    local = factor(local,\n      levels = c(\"no\", \"yes\"),\n      labels = c(\"no\", \"yes\"),\n      ordered = TRUE\n    ),\n    ##\n    month = factor(month,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n      labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n      ordered = TRUE\n    ),\n    hour = as_factor(hour)\n  )\nglimpse(taxi_modified)\n\nRows: 10,000\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ tip      &lt;ord&gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, y…\n$ distance &lt;dbl&gt; 17.19, 0.88, 18.11, 20.70, 12.23, 0.94, 17.47, 17.67, 1.85, 1…\n$ company  &lt;fct&gt; Chicago Independents, City Service, other, Chicago Independen…\n$ local    &lt;ord&gt; no, yes, no, no, no, yes, no, no, no, no, no, no, no, yes, no…\n$ dow      &lt;ord&gt; Thu, Thu, Mon, Mon, Sun, Sat, Fri, Sun, Fri, Tue, Tue, Sun, W…\n$ month    &lt;ord&gt; Feb, Mar, Feb, Apr, Mar, Apr, Mar, Jan, Apr, Mar, Mar, Apr, A…\n$ hour     &lt;fct&gt; 16, 8, 18, 8, 21, 23, 12, 6, 12, 14, 18, 11, 12, 19, 17, 13, …\n\n\n\nq1. Do more people tip than not?\n\ngf_bar(~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips\")\n\n\n\n\n\n\n\n\nLook that that! A vast majority of the lot, are people who tip!\n\n\nq2. Does the tip depend upon whether the trip is local or not?\nIn a dodged bar chart one variable is in the x- axis, but the y axis has the count of another variable. In this way we acquire counts of different levels of a qualitative data broken down by levels of another qualitative variable side by side, making it easy to compare.\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"dodge\"\n  ) %&gt;%\n  gf_labs(title = \"Dodged Bar Chart\" ,\n    subtitle = \"Proof that i know how to write subtitles\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nA stacked bar looks very similar to a dodged chart, but counts of both levels are on top of each other. I like this one better, it looks neater, but in my opinion if the counts of multiple levels of one paramter are closeby values, it would be more practical to use a dodged bar.\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Stacked Bar Chart\",\n    subtitle = \"A sandwiched version of the last one!\"\n  )%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nLooking at these, while we can spot the number of tips and lack of tips the locals and non-locals contributed- there are a lot more non local tips than local ones and in quantity, they do get most of their tips from non-locals.\n\nBut we can’t definitely say that non-locals tend to tip more certainly, it could only appear so beacuse the non-local trips are so much more. To find this, we will have to find the difference in ratios by using position = fill.\n\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"fill\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  )%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nWe can now observe that locals tend to tip lesser than the non-locals.\n\n\nq3. Do some cab companies get more tips than others?\n\ntaxi_modified %&gt;%\n  gf_bar(~company, fill = ~tip, position = \"dodge\") %&gt;%\n  gf_labs(title = \"Dodged Bar Chart\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1)))%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nOther gets the most count of trips but do they get the most number of tips in ratio?\n\ntaxi_modified %&gt;%\n  gf_props(~company, fill = ~tip, position = \"fill\") %&gt;%\n  gf_labs(\n    title = \"Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1)))%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nIt appears so that flash cabs is the company that gets the least amount of tips. While Chicago gets the most number of tips per-group proportion.\n\n\nq4. Does a tip depend upon the hour of the day? At which are are the tips highest and lowest?\n\ngf_bar(~hour, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Hour\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nThe count of tips are highest are the 17th hour while lowest is at the 4th hour.\n\ngf_bar(~hour, fill = ~tip, position = \"fill\", data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Hour in proportion\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nPeople are most likely to tip at 2nd and 4th hour and least at 8th, 15th and 23rd hour.\n\n\nq5. Does a tip depend upon the day of the week? At which day are the tips highest and lowest?\n\ngf_bar(~dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Day of Week\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nThe highest count of tips are on Thursdays and lowest is on Sundays. The count of tips thend to be the highest in weekdays and lesser in weekends\n\ngf_bar(~dow, fill = ~tip, data = taxi_modified, position = \"fill\") %&gt;%\n  gf_labs(title = \"Counts of Tips by Day of Week in proportion\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nPeople are most likely to tip on Sundays and least likely to tip on Fridays.\n\n\nq6. At which month are the tips highest and lowest among the 4 recorded?\n\ngf_bar(~month, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Month\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nThe highest count of tips is on April and lowest is on January.\n\ngf_bar(~month, fill = ~tip, data = taxi_modified, position = \"fill\") %&gt;%\n  gf_labs(title = \"Counts of Tips by Month in proportion\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nPeople are least likely to tip in April and most likely to tip in January.\n\n\nCounts of Tips by Day of Week and Month\n\nHere, we are obtaining a total 7 graphs, each for a day week. Each graph has 4 bars each, with month as the x axis indicating- for each day, in what month, the count of tips is observed.\n\n\ngf_bar(~ month | dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Counts of Tips by Day of Week and Month\")%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\n\n\nCounts of Tips by Hour and Day of Week\n\ngf_bar(~ hour | dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(\n    title = \"Counts of Tips by Hour and Day of Week\"\n  )%&gt;%\n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))"
  },
  {
    "objectID": "posts/day-3(2)/index.html#addiction-healthcare-data-set",
    "href": "posts/day-3(2)/index.html#addiction-healthcare-data-set",
    "title": "Day 3",
    "section": "Addiction healthcare data-set",
    "text": "Addiction healthcare data-set\n\ndata(\"HELPrct\")\n\n\nObtaining the count of of each substance\n\nHELPrct %&gt;% gf_bar(~substance)\n\n\n\n\n\n\n\n\nBased on this data, more people are addicted to alcohol than cocaine or heroin.\n\n\nPlotting the count of substance based on gender\n\nHELPrct %&gt;% gf_bar(~substance,fill = ~sex)%&gt;% \n  gf_refine(scale_fill_manual(values = c(\"turquoise\", \"salmon\")))\n\n\n\n\n\n\n\n\nFor all 3 substances, more men in count seem to be addicted than women."
  },
  {
    "objectID": "posts/day-3(2)/index.html#banned-books-data-set-trying-it-myself",
    "href": "posts/day-3(2)/index.html#banned-books-data-set-trying-it-myself",
    "title": "Day 3",
    "section": "Banned Books data-set (Trying it myself)",
    "text": "Banned Books data-set (Trying it myself)\n\nbanned &lt;- read_csv(\"../../data/banned-author-title.csv\")\n\nRows: 1586 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): Author, Title, Type of Ban, State, District, Date of Challenge/Remo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbanned\n\n# A tibble: 1,586 × 7\n   Author              Title `Type of Ban` State District Date of Challenge/Re…¹\n   &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;                 \n 1 Àbíké-Íyímídé, Far… Ace … Banned from … Flor… Indian … Nov-21                \n 2 Acevedo, Elizabeth  Clap… Banned from … Penn… Central… Sep-21                \n 3 Acevedo, Elizabeth  The … Banned from … Flor… Indian … Nov-21                \n 4 Acevedo, Elizabeth  The … Banned from … New … Marlbor… Feb-22                \n 5 Acevedo, Elizabeth  The … Banned Pendi… Texas Frederi… Mar-22                \n 6 Acevedo, Elizabeth  The … Banned Pendi… Virg… New Ken… Oct-21                \n 7 Aciman, André       Call… Banned Pendi… Virg… Spotsly… Nov-21                \n 8 Acito, Marc         How … Banned Pendi… Flor… Indian … Nov-21                \n 9 Adeyoha, Koja       47,0… Banned from … Penn… Central… Sep-21                \n10 Adichie, Chimamand… Half… Banned from … Mich… Hudsonv… Jan-22                \n# ℹ 1,576 more rows\n# ℹ abbreviated name: ¹​`Date of Challenge/Removal`\n# ℹ 1 more variable: `Origin of Challenge` &lt;chr&gt;\n\n\n\nbanned %&gt;% dplyr::glimpse()\n\nRows: 1,586\nColumns: 7\n$ Author                      &lt;chr&gt; \"Àbíké-Íyímídé, Faridah\", \"Acevedo, Elizab…\n$ Title                       &lt;chr&gt; \"Ace of Spades\", \"Clap When You Land\", \"Th…\n$ `Type of Ban`               &lt;chr&gt; \"Banned from Libraries and Classrooms\", \"B…\n$ State                       &lt;chr&gt; \"Florida\", \"Pennsylvania\", \"Florida\", \"New…\n$ District                    &lt;chr&gt; \"Indian River County School\", \"Central Yor…\n$ `Date of Challenge/Removal` &lt;chr&gt; \"Nov-21\", \"Sep-21\", \"Nov-21\", \"Feb-22\", \"M…\n$ `Origin of Challenge`       &lt;chr&gt; \"Administrator\", \"Administrator\", \"Adminis…\n\n\n\n banned %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n                       name     class levels    n missing\n1                    Author character    797 1586       0\n2                     Title character   1145 1586       0\n3               Type of Ban character      4 1586       0\n4                     State character     26 1586       0\n5                  District character     86 1586       0\n6 Date of Challenge/Removal character     12 1586       0\n7       Origin of Challenge character      2 1586       0\n                                   distribution\n1 Kobabe, Maia (1.9%) ...                      \n2 Gender Queer: A Memoir (1.9%) ...            \n3 Banned Pending Investigation (46.1%) ...     \n4 Texas (45%), Pennsylvania (28.8%) ...        \n5 Central York (27.8%) ...                     \n6 Sep-21 (28.8%), Dec-21 (28.3%) ...           \n7 Administrator (95.6%) ...                    \n\n\n\nTable of Variables\n\n\n\n\n\n\n\n\nVariable name\nDescription\nType of variable\n\n\n\n\nAuthor\nRepresents the name of the author of the book that has been challenged or banned\nQualitative\n\n\nTitle\nThis variable indicates the title of the book that has faced challenges or bans.\nQualitative\n\n\nType of Ban\nDescribes the context in which the book is banned\nQualitative\n\n\nState\nIndicates the U.S. state where the challenge or ban occurred.\nQualitative\n\n\nDistrict\nThis variable specifies the school district or library district involved in the challenge or ban.\nQualitative\n\n\nDate of Challenge/Removal\nThis variable records the date when the challenge was made or when the book was removed from circulation.\nQualitative\n\n\nOrigin of Challenge\nIndicates who initiated the challenge\nQualitative\n\n\n\n\nType of ban, State, District, Date of challenge and origin of challenge can be declared as factors.\n\n\nWhat could the target variable be?\nI guess it could be type of ban - understanding it could help predict if a book will be fully banned, temporarily banned, or restricted or with State/district, we could understand which states or districts are more likely to challenge books.\nWhen it comes to predictor variables, everything except title, author and district (there way too many levels) could be predictor variables.\n\nbanned_modified &lt;- banned %&gt;%\n  dplyr::mutate(\n    `Type of Ban` = as_factor(`Type of Ban`),\n    State = as_factor(State),\n    District = as_factor(District),\n    `Date of Challenge/Removal` = as_factor(`Date of Challenge/Removal`),\n    `Origin of Challenge` = as_factor(`Origin of Challenge`),\n  )\nglimpse(banned_modified)\n\nRows: 1,586\nColumns: 7\n$ Author                      &lt;chr&gt; \"Àbíké-Íyímídé, Faridah\", \"Acevedo, Elizab…\n$ Title                       &lt;chr&gt; \"Ace of Spades\", \"Clap When You Land\", \"Th…\n$ `Type of Ban`               &lt;fct&gt; Banned from Libraries and Classrooms, Bann…\n$ State                       &lt;fct&gt; Florida, Pennsylvania, Florida, New York, …\n$ District                    &lt;fct&gt; Indian River County School, Central York, …\n$ `Date of Challenge/Removal` &lt;fct&gt; Nov-21, Sep-21, Nov-21, Feb-22, Mar-22, Oc…\n$ `Origin of Challenge`       &lt;fct&gt; Administrator, Administrator, Administrato…\n\n\n\nbanned_modified %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n                       name     class levels    n missing\n1                    Author character    797 1586       0\n2                     Title character   1145 1586       0\n3               Type of Ban    factor      4 1586       0\n4                     State    factor     26 1586       0\n5                  District    factor     86 1586       0\n6 Date of Challenge/Removal    factor     12 1586       0\n7       Origin of Challenge    factor      2 1586       0\n                                   distribution\n1 Kobabe, Maia (1.9%) ...                      \n2 Gender Queer: A Memoir (1.9%) ...            \n3 Banned Pending Investigation (46.1%) ...     \n4 Texas (45%), Pennsylvania (28.8%) ...        \n5 Central York (27.8%) ...                     \n6 Sep-21 (28.8%), Dec-21 (28.3%) ...           \n7 Administrator (95.6%) ...                    \n\n\n\nbanned_modified %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nAuthor\n0\n1\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1\n2\n155\n0\n1145\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nType of Ban\n0\n1\nFALSE\n4\nBan: 731, Ban: 474, Ban: 197, Ban: 184\n\n\nState\n0\n1\nFALSE\n26\nTex: 713, Pen: 456, Flo: 204, Okl: 43\n\n\nDistrict\n0\n1\nFALSE\n86\nCen: 441, Nor: 435, Ind: 161, Gra: 131\n\n\nDate of Challenge/Removal\n0\n1\nFALSE\n12\nSep: 456, Dec: 449, Nov: 227, Jan: 178\n\n\nOrigin of Challenge\n0\n1\nFALSE\n2\nAdm: 1517, For: 69\n\n\n\n\n\n\n\n\nTARGET VARIABLE: State\n\nq1. Which state has the most amount of banned books?\n\ngf_bar(~State, data = banned_modified) %&gt;%\n  gf_labs(title = \"Counts of States\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nAs expected, Texas leads the nation in the number of banned books, with Pennsylvania following in second place.\n\nSince there is data on so many states, like we did on day 4, i want to filter it to be only 10 states with the most amount of banned books.\n\n\ntop_states &lt;- banned_modified %&gt;%\n  group_by(State) %&gt;%                   # Group by state\n  summarize(n = n()) %&gt;%                # Count banned books per state\n  slice_max(n, n = 10) %&gt;%                # Select the top 5 states\n  pull(State)                           # Extract the state names\n\n\ntop_10_states_data &lt;- banned_modified %&gt;%\n  filter(State %in% top_states)\n\n\ntop_10_states_data\n\n# A tibble: 1,524 × 7\n   Author              Title `Type of Ban` State District Date of Challenge/Re…¹\n   &lt;chr&gt;               &lt;chr&gt; &lt;fct&gt;         &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt;                 \n 1 Àbíké-Íyímídé, Far… Ace … Banned from … Flor… Indian … Nov-21                \n 2 Acevedo, Elizabeth  Clap… Banned from … Penn… Central… Sep-21                \n 3 Acevedo, Elizabeth  The … Banned from … Flor… Indian … Nov-21                \n 4 Acevedo, Elizabeth  The … Banned Pendi… Texas Frederi… Mar-22                \n 5 Acevedo, Elizabeth  The … Banned Pendi… Virg… New Ken… Oct-21                \n 6 Aciman, André       Call… Banned Pendi… Virg… Spotsly… Nov-21                \n 7 Acito, Marc         How … Banned Pendi… Flor… Indian … Nov-21                \n 8 Adeyoha, Koja       47,0… Banned from … Penn… Central… Sep-21                \n 9 Agell, Charlotte    The … Banned from … Texas North E… Dec-21                \n10 Ahmadi, Arvin       How … Banned Pendi… Texas North E… Dec-21                \n# ℹ 1,514 more rows\n# ℹ abbreviated name: ¹​`Date of Challenge/Removal`\n# ℹ 1 more variable: `Origin of Challenge` &lt;fct&gt;\n\n\nTrying kskin again with this updated data-set to see if the amount of districts are small enough to be a facor\n\ntop_10_states_data %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1524\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nAuthor\n0\n1\n7\n29\n0\n789\n0\n\n\nTitle\n0\n1\n2\n155\n0\n1133\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nType of Ban\n0\n1\nFALSE\n4\nBan: 701, Ban: 464, Ban: 189, Ban: 170\n\n\nState\n0\n1\nFALSE\n10\nTex: 713, Pen: 456, Flo: 204, Okl: 43\n\n\nDistrict\n0\n1\nFALSE\n57\nCen: 441, Nor: 435, Ind: 161, Gra: 131\n\n\nDate of Challenge/Removal\n0\n1\nFALSE\n12\nSep: 451, Dec: 447, Nov: 224, Jan: 161\n\n\nOrigin of Challenge\n0\n1\nFALSE\n2\nAdm: 1479, For: 45\n\n\n\n\n\n\nNope, still too much. i guess i could try it tho.\n\n\n\nq2. What is the correlation between type of ban and the state it is challenged in?\n\ntop_10_states_data %&gt;%\n  gf_bar(~State,\n    fill = ~`Type of Ban`,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(title = \"Type of ban by state\" ,\n    subtitle = \"Stacked Bar Chart\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nIt seems that almost all books banned in Pennsylvania are banned in Classrooms and in Texas, while most of them are banned from a pending investigation, there is a significant section of it banned from libraries and classrooms\n\n\nq3. Does the ban depend on the Origin of Challenge?\n\ntop_10_states_data %&gt;%\n  gf_bar(~State,\n    fill = ~`Origin of Challenge`,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(title = \"Origin of Challenge by state\" ,\n    subtitle = \"Stacked Bar Chart\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nMost bans everywhere were raised by the administration. Interestingly, all the books banned in Pennsylvania have been challenged by the administrator. In Texas, a large majority has been challenged by Administrator but there is still a portion of it challenged formally. All banned books in Missouri were challenged formally, how surprising!\n\n\nq4. Is there a correlation between the type of ban and the Origin of challenge?\n\ngf_bar(~`Origin of Challenge` | `Type of Ban`, fill = ~State, data = top_10_states_data) %&gt;%\n  gf_labs(title = \"Counts of Ban by type of ban and origin in states\")\n\n\n\n\n\n\n\n\n\ngf_bar(~`Origin of Challenge` | State, fill = ~`Type of Ban`, data = top_10_states_data) %&gt;%\n  gf_labs(title = \"Counts of Types of Ban by State and Origin\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nYou would expect that only all books whose origin was formal would be the ones banned in classrooms (over protective parents exist all over the world) but surprising, From what i can see, in Texas, most books formally banned are banned from libraries (a few also banned from classrooms and libraries) and the ones banned in Missouri are mostly banned pending investigation.\n\n\nq5. In which month of which year were most bans challenged?\n\ngf_bar(~`Date of Challenge/Removal`, data = banned_modified) %&gt;%\n  gf_labs(title = \"Count of bans every month\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nMost books were banned during September and December.\n\ntop_10_states_data %&gt;%\n  gf_bar(~State,\n    fill = ~`Date of Challenge/Removal`,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(title = \"Date of Challenge/Removal by State\" ,\n    subtitle = \"Stacked bar chart\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nFrom this bar, it is clear that every state has one month where most of there bans are imposed. For Texas it’s mostly in December, some in January, fewer in March, fall and November. For Pennsylvania, it’s moslty all in September. For Florida it’s mostly in November.\n\n\nq6. Is there a correlation between the type of ban and when it was challenged?\n\ngf_bar(~`Type of Ban` | `Date of Challenge/Removal`, fill = ~State, data = top_10_states_data) %&gt;%\n  gf_labs(title = \"Counts of Types of Ban by State and Date of Challenge\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nTo no surprise, there does not seem to be any correlation between type of ban and the month it was challenged in.\n\n\nq7. Is there a correlation between the origin of ban and when it was challenged?\nI know it’s a long shot.\n\ngf_bar(~`Origin of Challenge` | State, fill = ~`Date of Challenge/Removal`, data = top_10_states_data) %&gt;%\n  gf_labs(title = \"Origin of challenge of State and Date of Challenge/Removal\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nThere doesn’t seem to be any clear correlation between origin of ban and when it was challenged except in Tennesse where most if not all administrator origins came at Winter 2021 and all formal challenges came of September 2021.\n\n\nq8. Which author has the honor of being the most banned?\nI know this doesn’t really come under the bar graph theme we were going with in this post but i really wanted to know and when i did yry and make it a bar graph, i wasn;t getting one with all the same counts.\n\ntop_author &lt;- banned_modified %&gt;%\n  count(Author) %&gt;%\n  slice_max(n, n = 10)  \n\ntop_author\n\n# A tibble: 10 × 2\n   Author                 n\n   &lt;chr&gt;              &lt;int&gt;\n 1 Kobabe, Maia          30\n 2 Hopkins, Ellen        27\n 3 Johnson, George M.    21\n 4 Do, Anh               17\n 5 Evison, Jonathan      16\n 6 Faruqi, Saadia        16\n 7 Jules, Jacqueline     16\n 8 Morrison, Toni        16\n 9 Myracle, Lauren       16\n10 Pérez, Ashley Hope    16\n\n\nKobabe, Maia is the author with the most number of banned books, so proud of him or her!\n\n\nq9. Which District of Texas has raised the most challenges?\n\ntop_1_state &lt;- banned_modified %&gt;%\n  group_by(State) %&gt;%                   \n  summarize(n = n()) %&gt;%                \n  slice_max(n, n = 1) %&gt;%                \n  pull(State)\n\ntop_1_states_data &lt;- banned_modified %&gt;%\n  filter(State %in% top_1_state)\n\n\ngf_bar(~District, data = top_1_states_data, fill = \"lightblue\") %&gt;%\n  gf_labs(title = \"Counts of Districts in Texas\")%&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\nThe North East District of Texas has the most number of challenges/ bans\n\n\n\nThe End of my A1."
  },
  {
    "objectID": "posts/day-5/index.html",
    "href": "posts/day-5/index.html",
    "title": "Day 5",
    "section": "",
    "text": "Variables1: Quantitative, Variable 2: Qualitative\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(palmerpenguins)\n\n\nwages &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/stevedata/gss_wages.csv\")\n\nRows: 61697 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): occrecode, wrkstat, gender, educcat, maritalcat\ndbl (7): rownames, year, realrinc, age, occ10, prestg10, childs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwages\n\n# A tibble: 61,697 × 12\n   rownames  year realrinc   age occ10 occrecode  prestg10 childs wrkstat gender\n      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; \n 1        1  1974     4935    21  5620 Office an…       25      0 School  Male  \n 2        2  1974    43178    41  2040 Professio…       66      3 Full-T… Male  \n 3        3  1974       NA    83    NA &lt;NA&gt;             NA      2 Housek… Female\n 4        4  1974       NA    69    NA &lt;NA&gt;             NA      2 Housek… Female\n 5        5  1974    18505    58  5820 Office an…       37      0 Full-T… Female\n 6        6  1974    22206    30   910 Business/…       45      0 School  Male  \n 7        7  1974    55515    48   230 Business/…       59      2 Full-T… Male  \n 8        8  1974       NA    67  6355 Construct…       49      1 Retired Male  \n 9        9  1974       NA    51  4720 Sales            28      2 Housek… Female\n10       10  1974     4935    54  3940 Service          38      2 Full-T… Female\n# ℹ 61,687 more rows\n# ℹ 2 more variables: educcat &lt;chr&gt;, maritalcat &lt;chr&gt;\n\n\n\nglimpse(wages)\n\nRows: 61,697\nColumns: 12\n$ rownames   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ year       &lt;dbl&gt; 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974,…\n$ realrinc   &lt;dbl&gt; 4935, 43178, NA, NA, 18505, 22206, 55515, NA, NA, 4935, NA,…\n$ age        &lt;dbl&gt; 21, 41, 83, 69, 58, 30, 48, 67, 51, 54, 89, 71, 27, 30, 22,…\n$ occ10      &lt;dbl&gt; 5620, 2040, NA, NA, 5820, 910, 230, 6355, 4720, 3940, 4810,…\n$ occrecode  &lt;chr&gt; \"Office and Administrative Support\", \"Professional\", NA, NA…\n$ prestg10   &lt;dbl&gt; 25, 66, NA, NA, 37, 45, 59, 49, 28, 38, 47, 45, 50, 29, 33,…\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 0, 2, 1, 2, 2, 3, 1, 4, 3, 0, 1, 2, 3, 4, 8,…\n$ wrkstat    &lt;chr&gt; \"School\", \"Full-Time\", \"Housekeeper\", \"Housekeeper\", \"Full-…\n$ gender     &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male…\n$ educcat    &lt;chr&gt; \"High School\", \"Bachelor\", \"Less Than High School\", \"Less T…\n$ maritalcat &lt;chr&gt; \"Married\", \"Married\", \"Widowed\", \"Widowed\", \"Never Married\"…\n\n\n\nwages %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n61697\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\noccrecode\n3561\n0.94\n5\n37\n0\n11\n0\n\n\nwrkstat\n21\n1.00\n5\n23\n0\n8\n0\n\n\ngender\n0\n1.00\n4\n6\n0\n2\n0\n\n\neduccat\n135\n1.00\n8\n21\n0\n5\n0\n\n\nmaritalcat\n27\n1.00\n7\n13\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrownames\n0\n1.00\n30849.00\n17810.53\n1\n15425\n30849\n46273\n61697.0\n▇▇▇▇▇\n\n\nyear\n0\n1.00\n1996.07\n12.79\n1974\n1985\n1996\n2006\n2018.0\n▆▇▇▇▇\n\n\nrealrinc\n23810\n0.61\n22326.36\n28581.79\n227\n8156\n16563\n27171\n480144.5\n▇▁▁▁▁\n\n\nage\n219\n1.00\n46.18\n17.56\n18\n32\n44\n59\n89.0\n▇▇▆▅▂\n\n\nocc10\n3561\n0.94\n4695.77\n2627.72\n10\n2710\n4720\n6230\n9997.0\n▃▅▇▂▃\n\n\nprestg10\n4186\n0.93\n43.06\n12.99\n16\n33\n42\n50\n80.0\n▃▇▇▃▁\n\n\nchilds\n189\n1.00\n1.92\n1.76\n0\n0\n2\n3\n8.0\n▇▇▂▁▁\n\n\n\n\n\n\ninspect(wages)\n\n\ncategorical variables:  \n        name     class levels     n missing\n1  occrecode character     11 58136    3561\n2    wrkstat character      8 61676      21\n3     gender character      2 61697       0\n4    educcat character      5 61562     135\n5 maritalcat character      5 61670      27\n                                   distribution\n1 Professional (19%), Service (16.9%) ...      \n2 Full-Time (49.4%), Housekeeper (15.1%) ...   \n3 Female (56.1%), Male (43.9%)                 \n4 High School (51.5%) ...                      \n5 Married (51.7%), Never Married (21.8%) ...   \n\nquantitative variables:  \n      name   class  min    Q1 median    Q3      max         mean           sd\n1 rownames numeric    1 15425  30849 46273  61697.0 30849.000000 17810.534116\n2     year numeric 1974  1985   1996  2006   2018.0  1996.073715    12.794470\n3 realrinc numeric  227  8156  16563 27171 480144.5 22326.359234 28581.794499\n4      age numeric   18    32     44    59     89.0    46.176177    17.561065\n5    occ10 numeric   10  2710   4720  6230   9997.0  4695.774081  2627.724076\n6 prestg10 numeric   16    33     42    50     80.0    43.060701    12.987526\n7   childs numeric    0     0      2     3      8.0     1.923457     1.763569\n      n missing\n1 61697       0\n2 61697       0\n3 37887   23810\n4 61478     219\n5 58136    3561\n6 57511    4186\n7 61508     189\n\n\n\nyear (&lt;dbl&gt;):\n\nThe year in which the survey was conducted. This is a numeric (double) variable representing the survey’s year (e.g., 2010, 2015, etc.).\n\nrealrinc (&lt;dbl&gt;):\n\nThis likely stands for real income. It’s the respondent’s income adjusted for inflation, typically measured in constant dollars from a specific base year. This allows comparisons of income across different years by accounting for changes in the cost of living.\n\nage (&lt;dbl&gt;):\n\nThe age of the respondent in years. It’s a numeric value.\n\nocc10 (&lt;dbl&gt;):\n\nThis is likely a standard occupation code, using the 2010 Occupational Classification System. It’s a numeric code representing the respondent’s occupation according to a standardized system used by labor statistics agencies.\n\noccrecode (&lt;chr&gt;):\n\nThis likely refers to a recoded occupation variable. The occupation might have been simplified or recoded into broader categories to facilitate analysis. This is stored as a character string (text).\n\nprestg10 (&lt;dbl&gt;):\n\nThis likely stands for occupational prestige score (2010). It is a numeric value that represents the societal prestige or status of the respondent’s occupation, based on surveys or expert ratings of different jobs.\n\nchilds (&lt;dbl&gt;):\n\nThe number of children the respondent has. This is a numeric variable representing the count of children.\n\nwrkstat (&lt;chr&gt;):\n\nLikely short for work status. It indicates the respondent’s current employment situation (e.g., “Employed full-time,” “Unemployed,” “Retired”). This is a categorical variable stored as a character string.\n\ngender (&lt;chr&gt;):\n\nThe respondent’s gender, typically recorded as “Male” or “Female.” This is a categorical (character) variable.\n\neduccat (&lt;chr&gt;):\n\n\nThis refers to education category, which classifies respondents into broad educational attainment groups (e.g., “Less than High School,” “High School Graduate,” “Some College,” “College Graduate”). This is a categorical variable.\n\n\nmaritalcat (&lt;chr&gt;):\n\n\nThis is a marital status category, which categorizes respondents into groups based on their marital status (e.g., “Married,” “Single,” “Divorced,” “Widowed”). It’s a categorical variable stored as a character string.\n\nSince there are so many missing data in the target variable realinc and there is still enough data leftover, let us remove the rows containing missing data in that variable.\n\nwages_clean &lt;-\n  wages %&gt;%\n  tidyr::drop_na(realrinc) # choose column or leave blank to choose all\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(realrinc ~ \"Income\") %&gt;% ## There is nothing really to put on the x. \n  gf_labs(\n    title = \"Plot 1A: Income has a skewed distribution\",\n    subtitle = \"Many outliers on the high side\"\n  )\n\n\n\n\n\n\n\n\nIncome is a very skewed distribution. There is a large population of people with low incomes and we observe a couple of outliers in higher incomes.\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 2A: Income by Gender\")\n\n\n\n\n\n\n\n\nTo understand this better, we take the log10 of the Realinc and we add colour to the 2 groups we are seperating it by\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc, fill = ~gender) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(title = \"Plot 2C: Income filled by Gender, log scale\")\n\n\n\n\n\n\n\n\nThe IQR for males is smaller than the IQR for females. There is less variation in the middle ranges of realrinc for men. log10 transformation helps to view and understand the regions of low realrinc. There median of both genders suggests that there may be a disparity in pay. We can only confirm this by looking at the mean value.\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 3A: Income by Education Category\")\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ log10(realrinc)) %&gt;%\n  gf_labs(title = \"Plot 3B: Log(Income) by Education Category\")\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(\n    reorder(educcat, realrinc, FUN = median) ~ log(realrinc),\n    fill = ~educcat,\n    alpha = 0.3\n  ) %&gt;%\n  gf_labs(title = \"Plot 3C: Log(Income) by Education Category, sorted\") %&gt;%\n  gf_labs(\n    x = \"Log Income\",\n    y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\nreorder(educcat, realrinc, FUN = median): Reorders the educcat (education category) based on the median of realrinc (income). This means the education categories will be ordered in the plot from lowest to highest median income.\n\nwages_clean %&gt;%\n  gf_boxplot(reorder(educcat, realrinc, FUN = median) ~ realrinc,\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Income by Education Category, sorted\",\n    subtitle = \"Log Income\"\n  ) %&gt;%\n  gf_labs(\n    x = \"Income\",\n    y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\nThe second code uses gf_refine(scale_x_log10()), which transforms the x-axis scale into a logarithmic scale (base 10) after the plot is created. This means the log transformation is purely for the axis labels and the scaling of the plot, not for the underlying data in the plot.\nConclutions?\nThere an increase in the median of income as the qualifications increase as expected. There are people with very low and very high income in all categories of education.\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(educcat, realrinc) ~ log10(realrinc),\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(childs)) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Log Income by Education Category and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\nFamily size does not seem to play a role in income while the education does.\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(childs = as_factor(childs)) %&gt;%\n  gf_boxplot(childs ~ log10(realrinc),\n    group = ~childs,\n    fill = ~childs,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~gender) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  gf_boxplot(gender ~ log10(realrinc),\n    group = ~gender,\n    fill = ~gender,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~childs) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  gf_boxplot(gender ~ log10(realrinc),\n    group = ~gender,\n    fill = ~gender,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~educcat) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Qualification\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )"
  },
  {
    "objectID": "posts/day-5/index.html#introduction",
    "href": "posts/day-5/index.html#introduction",
    "title": "Day 5",
    "section": "",
    "text": "Variables1: Quantitative, Variable 2: Qualitative\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggformula)\nlibrary(palmerpenguins)\n\n\nwages &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/stevedata/gss_wages.csv\")\n\nRows: 61697 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): occrecode, wrkstat, gender, educcat, maritalcat\ndbl (7): rownames, year, realrinc, age, occ10, prestg10, childs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwages\n\n# A tibble: 61,697 × 12\n   rownames  year realrinc   age occ10 occrecode  prestg10 childs wrkstat gender\n      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; \n 1        1  1974     4935    21  5620 Office an…       25      0 School  Male  \n 2        2  1974    43178    41  2040 Professio…       66      3 Full-T… Male  \n 3        3  1974       NA    83    NA &lt;NA&gt;             NA      2 Housek… Female\n 4        4  1974       NA    69    NA &lt;NA&gt;             NA      2 Housek… Female\n 5        5  1974    18505    58  5820 Office an…       37      0 Full-T… Female\n 6        6  1974    22206    30   910 Business/…       45      0 School  Male  \n 7        7  1974    55515    48   230 Business/…       59      2 Full-T… Male  \n 8        8  1974       NA    67  6355 Construct…       49      1 Retired Male  \n 9        9  1974       NA    51  4720 Sales            28      2 Housek… Female\n10       10  1974     4935    54  3940 Service          38      2 Full-T… Female\n# ℹ 61,687 more rows\n# ℹ 2 more variables: educcat &lt;chr&gt;, maritalcat &lt;chr&gt;\n\n\n\nglimpse(wages)\n\nRows: 61,697\nColumns: 12\n$ rownames   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ year       &lt;dbl&gt; 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974,…\n$ realrinc   &lt;dbl&gt; 4935, 43178, NA, NA, 18505, 22206, 55515, NA, NA, 4935, NA,…\n$ age        &lt;dbl&gt; 21, 41, 83, 69, 58, 30, 48, 67, 51, 54, 89, 71, 27, 30, 22,…\n$ occ10      &lt;dbl&gt; 5620, 2040, NA, NA, 5820, 910, 230, 6355, 4720, 3940, 4810,…\n$ occrecode  &lt;chr&gt; \"Office and Administrative Support\", \"Professional\", NA, NA…\n$ prestg10   &lt;dbl&gt; 25, 66, NA, NA, 37, 45, 59, 49, 28, 38, 47, 45, 50, 29, 33,…\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 0, 2, 1, 2, 2, 3, 1, 4, 3, 0, 1, 2, 3, 4, 8,…\n$ wrkstat    &lt;chr&gt; \"School\", \"Full-Time\", \"Housekeeper\", \"Housekeeper\", \"Full-…\n$ gender     &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male…\n$ educcat    &lt;chr&gt; \"High School\", \"Bachelor\", \"Less Than High School\", \"Less T…\n$ maritalcat &lt;chr&gt; \"Married\", \"Married\", \"Widowed\", \"Widowed\", \"Never Married\"…\n\n\n\nwages %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n61697\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\noccrecode\n3561\n0.94\n5\n37\n0\n11\n0\n\n\nwrkstat\n21\n1.00\n5\n23\n0\n8\n0\n\n\ngender\n0\n1.00\n4\n6\n0\n2\n0\n\n\neduccat\n135\n1.00\n8\n21\n0\n5\n0\n\n\nmaritalcat\n27\n1.00\n7\n13\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrownames\n0\n1.00\n30849.00\n17810.53\n1\n15425\n30849\n46273\n61697.0\n▇▇▇▇▇\n\n\nyear\n0\n1.00\n1996.07\n12.79\n1974\n1985\n1996\n2006\n2018.0\n▆▇▇▇▇\n\n\nrealrinc\n23810\n0.61\n22326.36\n28581.79\n227\n8156\n16563\n27171\n480144.5\n▇▁▁▁▁\n\n\nage\n219\n1.00\n46.18\n17.56\n18\n32\n44\n59\n89.0\n▇▇▆▅▂\n\n\nocc10\n3561\n0.94\n4695.77\n2627.72\n10\n2710\n4720\n6230\n9997.0\n▃▅▇▂▃\n\n\nprestg10\n4186\n0.93\n43.06\n12.99\n16\n33\n42\n50\n80.0\n▃▇▇▃▁\n\n\nchilds\n189\n1.00\n1.92\n1.76\n0\n0\n2\n3\n8.0\n▇▇▂▁▁\n\n\n\n\n\n\ninspect(wages)\n\n\ncategorical variables:  \n        name     class levels     n missing\n1  occrecode character     11 58136    3561\n2    wrkstat character      8 61676      21\n3     gender character      2 61697       0\n4    educcat character      5 61562     135\n5 maritalcat character      5 61670      27\n                                   distribution\n1 Professional (19%), Service (16.9%) ...      \n2 Full-Time (49.4%), Housekeeper (15.1%) ...   \n3 Female (56.1%), Male (43.9%)                 \n4 High School (51.5%) ...                      \n5 Married (51.7%), Never Married (21.8%) ...   \n\nquantitative variables:  \n      name   class  min    Q1 median    Q3      max         mean           sd\n1 rownames numeric    1 15425  30849 46273  61697.0 30849.000000 17810.534116\n2     year numeric 1974  1985   1996  2006   2018.0  1996.073715    12.794470\n3 realrinc numeric  227  8156  16563 27171 480144.5 22326.359234 28581.794499\n4      age numeric   18    32     44    59     89.0    46.176177    17.561065\n5    occ10 numeric   10  2710   4720  6230   9997.0  4695.774081  2627.724076\n6 prestg10 numeric   16    33     42    50     80.0    43.060701    12.987526\n7   childs numeric    0     0      2     3      8.0     1.923457     1.763569\n      n missing\n1 61697       0\n2 61697       0\n3 37887   23810\n4 61478     219\n5 58136    3561\n6 57511    4186\n7 61508     189\n\n\n\nyear (&lt;dbl&gt;):\n\nThe year in which the survey was conducted. This is a numeric (double) variable representing the survey’s year (e.g., 2010, 2015, etc.).\n\nrealrinc (&lt;dbl&gt;):\n\nThis likely stands for real income. It’s the respondent’s income adjusted for inflation, typically measured in constant dollars from a specific base year. This allows comparisons of income across different years by accounting for changes in the cost of living.\n\nage (&lt;dbl&gt;):\n\nThe age of the respondent in years. It’s a numeric value.\n\nocc10 (&lt;dbl&gt;):\n\nThis is likely a standard occupation code, using the 2010 Occupational Classification System. It’s a numeric code representing the respondent’s occupation according to a standardized system used by labor statistics agencies.\n\noccrecode (&lt;chr&gt;):\n\nThis likely refers to a recoded occupation variable. The occupation might have been simplified or recoded into broader categories to facilitate analysis. This is stored as a character string (text).\n\nprestg10 (&lt;dbl&gt;):\n\nThis likely stands for occupational prestige score (2010). It is a numeric value that represents the societal prestige or status of the respondent’s occupation, based on surveys or expert ratings of different jobs.\n\nchilds (&lt;dbl&gt;):\n\nThe number of children the respondent has. This is a numeric variable representing the count of children.\n\nwrkstat (&lt;chr&gt;):\n\nLikely short for work status. It indicates the respondent’s current employment situation (e.g., “Employed full-time,” “Unemployed,” “Retired”). This is a categorical variable stored as a character string.\n\ngender (&lt;chr&gt;):\n\nThe respondent’s gender, typically recorded as “Male” or “Female.” This is a categorical (character) variable.\n\neduccat (&lt;chr&gt;):\n\n\nThis refers to education category, which classifies respondents into broad educational attainment groups (e.g., “Less than High School,” “High School Graduate,” “Some College,” “College Graduate”). This is a categorical variable.\n\n\nmaritalcat (&lt;chr&gt;):\n\n\nThis is a marital status category, which categorizes respondents into groups based on their marital status (e.g., “Married,” “Single,” “Divorced,” “Widowed”). It’s a categorical variable stored as a character string.\n\nSince there are so many missing data in the target variable realinc and there is still enough data leftover, let us remove the rows containing missing data in that variable.\n\nwages_clean &lt;-\n  wages %&gt;%\n  tidyr::drop_na(realrinc) # choose column or leave blank to choose all\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(realrinc ~ \"Income\") %&gt;% ## There is nothing really to put on the x. \n  gf_labs(\n    title = \"Plot 1A: Income has a skewed distribution\",\n    subtitle = \"Many outliers on the high side\"\n  )\n\n\n\n\n\n\n\n\nIncome is a very skewed distribution. There is a large population of people with low incomes and we observe a couple of outliers in higher incomes.\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 2A: Income by Gender\")\n\n\n\n\n\n\n\n\nTo understand this better, we take the log10 of the Realinc and we add colour to the 2 groups we are seperating it by\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc, fill = ~gender) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(title = \"Plot 2C: Income filled by Gender, log scale\")\n\n\n\n\n\n\n\n\nThe IQR for males is smaller than the IQR for females. There is less variation in the middle ranges of realrinc for men. log10 transformation helps to view and understand the regions of low realrinc. There median of both genders suggests that there may be a disparity in pay. We can only confirm this by looking at the mean value.\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 3A: Income by Education Category\")\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ log10(realrinc)) %&gt;%\n  gf_labs(title = \"Plot 3B: Log(Income) by Education Category\")\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(\n    reorder(educcat, realrinc, FUN = median) ~ log(realrinc),\n    fill = ~educcat,\n    alpha = 0.3\n  ) %&gt;%\n  gf_labs(title = \"Plot 3C: Log(Income) by Education Category, sorted\") %&gt;%\n  gf_labs(\n    x = \"Log Income\",\n    y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\nreorder(educcat, realrinc, FUN = median): Reorders the educcat (education category) based on the median of realrinc (income). This means the education categories will be ordered in the plot from lowest to highest median income.\n\nwages_clean %&gt;%\n  gf_boxplot(reorder(educcat, realrinc, FUN = median) ~ realrinc,\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Income by Education Category, sorted\",\n    subtitle = \"Log Income\"\n  ) %&gt;%\n  gf_labs(\n    x = \"Income\",\n    y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\nThe second code uses gf_refine(scale_x_log10()), which transforms the x-axis scale into a logarithmic scale (base 10) after the plot is created. This means the log transformation is purely for the axis labels and the scaling of the plot, not for the underlying data in the plot.\nConclutions?\nThere an increase in the median of income as the qualifications increase as expected. There are people with very low and very high income in all categories of education.\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(educcat, realrinc) ~ log10(realrinc),\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(childs)) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Log Income by Education Category and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\nFamily size does not seem to play a role in income while the education does.\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(childs = as_factor(childs)) %&gt;%\n  gf_boxplot(childs ~ log10(realrinc),\n    group = ~childs,\n    fill = ~childs,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~gender) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  gf_boxplot(gender ~ log10(realrinc),\n    group = ~gender,\n    fill = ~gender,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~childs) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  gf_boxplot(gender ~ log10(realrinc),\n    group = ~gender,\n    fill = ~gender,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~educcat) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Qualification\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )"
  },
  {
    "objectID": "posts/My-name-is-Immortal/Index.html",
    "href": "posts/My-name-is-Immortal/Index.html",
    "title": "Day 1",
    "section": "",
    "text": "I’m a designer in training and like most people in the creatives, my interests cover a broad spectrum ranging from 3D Modelling and Lettering to Website Designs. I’m now working to add data visualisation and into my skill set, I hope I don’t totally suck at it!"
  },
  {
    "objectID": "posts/My-name-is-Immortal/Index.html#introduction",
    "href": "posts/My-name-is-Immortal/Index.html#introduction",
    "title": "Day 1",
    "section": "",
    "text": "I’m a designer in training and like most people in the creatives, my interests cover a broad spectrum ranging from 3D Modelling and Lettering to Website Designs. I’m now working to add data visualisation and into my skill set, I hope I don’t totally suck at it!"
  },
  {
    "objectID": "posts/My-name-is-Immortal/Index.html#my-first-piece-of-r-code",
    "href": "posts/My-name-is-Immortal/Index.html#my-first-piece-of-r-code",
    "title": "Day 1",
    "section": "My First Piece of R-code",
    "text": "My First Piece of R-code\nI’m doing this!!!!!\n\nlibrary (tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary (ggformula)\n\nLoading required package: scales\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nLoading required package: ggridges\n\nNew to ggformula?  Try the tutorials: \n    learnr::run_tutorial(\"introduction\", package = \"ggformula\")\n    learnr::run_tutorial(\"refining\", package = \"ggformula\")\n\nlibrary (babynames)\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\nGetting a list of all baby names in the USA from the year 1880\n\nbabynames\n\n# A tibble: 1,924,665 × 5\n    year sex   name          n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n 1  1880 F     Mary       7065 0.0724\n 2  1880 F     Anna       2604 0.0267\n 3  1880 F     Emma       2003 0.0205\n 4  1880 F     Elizabeth  1939 0.0199\n 5  1880 F     Minnie     1746 0.0179\n 6  1880 F     Margaret   1578 0.0162\n 7  1880 F     Ida        1472 0.0151\n 8  1880 F     Alice      1414 0.0145\n 9  1880 F     Bertha     1320 0.0135\n10  1880 F     Sarah      1288 0.0132\n# ℹ 1,924,655 more rows\n\n\n\n\n\nFiltering the presence of my name i.e. Sneha and creating a line graph with this data\n\nbabynames %&gt;% filter (name==\"Sneha\")\n\n# A tibble: 43 × 5\n    year sex   name      n       prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;\n 1  1975 F     Sneha     9 0.00000577\n 2  1976 F     Sneha     7 0.00000445\n 3  1977 F     Sneha     9 0.00000547\n 4  1978 F     Sneha     9 0.00000548\n 5  1979 F     Sneha     6 0.00000348\n 6  1980 F     Sneha     7 0.00000393\n 7  1981 F     Sneha     6 0.00000336\n 8  1982 F     Sneha     8 0.00000441\n 9  1983 F     Sneha     9 0.00000503\n10  1984 F     Sneha    14 0.00000777\n# ℹ 33 more rows\n\n\n\nbabynames %&gt;% filter(name==\"Sneha\") %&gt;% gf_line(n~year)\n\n\n\n\n\n\n\n\n\n\nFiltering the presence of the name “Trisha” and creating a line graph with this data\n\nbabynames %&gt;% filter (name==\"Trisha\")\n\n# A tibble: 84 × 5\n    year sex   name       n       prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n 1  1934 F     Trisha     5 0.00000462\n 2  1943 F     Trisha     5 0.00000348\n 3  1944 F     Trisha    33 0.0000242 \n 4  1945 F     Trisha    46 0.0000342 \n 5  1946 F     Trisha    56 0.0000347 \n 6  1947 F     Trisha    32 0.0000176 \n 7  1948 F     Trisha    29 0.0000166 \n 8  1949 F     Trisha    28 0.0000160 \n 9  1950 F     Trisha    36 0.0000205 \n10  1951 F     Trisha    50 0.0000271 \n# ℹ 74 more rows\n\n\n\nbabynames %&gt;% filter(name==\"Trisha\") %&gt;% gf_line(n~year)\n\n\n\n\n\n\n\n\n\n\nFiltering the presence of the name “Sarah” and creating a line graph with this data\n\nbabynames %&gt;% filter (name==\"Sarah\" | name==\"Sara\") \n\n# A tibble: 487 × 5\n    year sex   name      n      prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt;\n 1  1880 F     Sarah  1288 0.0132   \n 2  1880 F     Sara    165 0.00169  \n 3  1881 F     Sarah  1226 0.0124   \n 4  1881 F     Sara    147 0.00149  \n 5  1882 F     Sarah  1410 0.0122   \n 6  1882 F     Sara    180 0.00156  \n 7  1883 F     Sarah  1359 0.0113   \n 8  1883 F     Sara    183 0.00152  \n 9  1883 M     Sarah     7 0.0000622\n10  1884 F     Sarah  1518 0.0110   \n# ℹ 477 more rows\n\n\n\nbabynames %&gt;% filter(name==\"Sarah\" | name==\"Sara\") %&gt;% gf_line(n~year)\n\n\n\n\n\n\n\n\n\n\nGlimpse:\n\nbabynames %&gt;% dplyr::glimpse()\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\n\n\nbabynames_modified &lt;- babynames %&gt;%\n  dplyr::mutate(\n    sex = as_factor(sex),\n  )\nglimpse(babynames_modified)\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;fct&gt; F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F,…\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\n\n\nbabynames_modified %&gt;% mosaic::inspect()\n\n\ncategorical variables:  \n  name     class levels       n missing\n1  sex    factor      2 1924665       0\n2 name character  97310 1924665       0\n                                   distribution\n1 F (59.1%), M (40.9%)                         \n2 Francis (0%), James (0%) ...                 \n\nquantitative variables:  \n  name   class      min        Q1    median        Q3          max         mean\n1 year numeric 1.88e+03 1.951e+03 1.985e+03 2.003e+03 2.017000e+03 1.974851e+03\n2    n integer 5.00e+00 7.000e+00 1.200e+01 3.200e+01 9.968600e+04 1.808733e+02\n3 prop numeric 2.26e-06 3.870e-06 7.300e-06 2.288e-05 8.154561e-02 1.362963e-04\n            sd       n missing\n1 3.402948e+01 1924665       0\n2 1.533337e+03 1924665       0\n3 1.151693e-03 1924665       0\n\n\n\nbabynames_modified %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1924665\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n2\n15\n0\n97310\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1\nFALSE\n2\nF: 1138293, M: 786372\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1974.85\n34.03\n1880\n1951\n1985\n2003\n2017.00\n▁▂▃▅▇\n\n\nn\n0\n1\n180.87\n1533.34\n5\n7\n12\n32\n99686.00\n▇▁▁▁▁\n\n\nprop\n0\n1\n0.00\n0.00\n0\n0\n0\n0\n0.08\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/day-6/index.html",
    "href": "posts/day-6/index.html",
    "title": "Day 6",
    "section": "",
    "text": "library(tidyverse) # Data Processing in R\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic) # Our workhorse for stats, sampling\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(skimr) # Good to Examine data\n\n\nAttaching package: 'skimr'\n\nThe following object is masked from 'package:mosaic':\n\n    n_missing\n\nlibrary(ggformula) # Formula interface for graphs\n\n# load the NHANES data library\nlibrary(NHANES)\nlibrary(infer)\n\n\nAttaching package: 'infer'\n\nThe following objects are masked from 'package:mosaic':\n\n    prop_test, t_test\n\n\n\ndata(\"NHANES\")\nglimpse(NHANES)\n\nRows: 10,000\nColumns: 76\n$ ID               &lt;int&gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 5164…\n$ SurveyYr         &lt;fct&gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,…\n$ Gender           &lt;fct&gt; male, male, male, male, female, male, male, female, f…\n$ Age              &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, …\n$ AgeDecade        &lt;fct&gt;  30-39,  30-39,  30-39,  0-9,  40-49,  0-9,  0-9,  40…\n$ AgeMonths        &lt;int&gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795,…\n$ Race1            &lt;fct&gt; White, White, White, Other, White, White, White, Whit…\n$ Race3            &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education        &lt;fct&gt; High School, High School, High School, NA, Some Colle…\n$ MaritalStatus    &lt;fct&gt; Married, Married, Married, NA, LivePartner, NA, NA, M…\n$ HHIncome         &lt;fct&gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, 3…\n$ HHIncomeMid      &lt;int&gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 8750…\n$ Poverty          &lt;dbl&gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00,…\n$ HomeRooms        &lt;int&gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3,…\n$ HomeOwn          &lt;fct&gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, O…\n$ Work             &lt;fct&gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, N…\n$ Weight           &lt;dbl&gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7,…\n$ Length           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HeadCirc         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Height           &lt;dbl&gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.…\n$ BMI              &lt;dbl&gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.2…\n$ BMICatUnder20yrs &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BMI_WHO          &lt;fct&gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plus…\n$ Pulse            &lt;int&gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 8…\n$ BPSysAve         &lt;int&gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111, …\n$ BPDiaAve         &lt;int&gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, 6…\n$ BPSys1           &lt;int&gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124, …\n$ BPDia1           &lt;int&gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, 6…\n$ BPSys2           &lt;int&gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108, …\n$ BPDia2           &lt;int&gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, 6…\n$ BPSys3           &lt;int&gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114, …\n$ BPDia3           &lt;int&gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, 7…\n$ Testosterone     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DirectChol       &lt;dbl&gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, 2…\n$ TotChol          &lt;dbl&gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, 5…\n$ UrineVol1        &lt;int&gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113, …\n$ UrineFlow1       &lt;dbl&gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1.…\n$ UrineVol2        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ UrineFlow2       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Diabetes         &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ DiabetesAge      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HealthGen        &lt;fct&gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vgo…\n$ DaysPhysHlthBad  &lt;int&gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0,…\n$ DaysMentHlthBad  &lt;int&gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA,…\n$ LittleInterest   &lt;fct&gt; Most, Most, Most, NA, Several, NA, NA, None, None, No…\n$ Depressed        &lt;fct&gt; Several, Several, Several, NA, Several, NA, NA, None,…\n$ nPregnancies     &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, N…\n$ nBabies          &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Age1stBaby       &lt;int&gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SleepHrsNight    &lt;int&gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, N…\n$ SleepTrouble     &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, Y…\n$ PhysActive       &lt;fct&gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes, …\n$ PhysActiveDays   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2, …\n$ TVHrsDay         &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CompHrsDay       &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ TVHrsDayChild    &lt;int&gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, N…\n$ CompHrsDayChild  &lt;int&gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, N…\n$ Alcohol12PlusYr  &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, Y…\n$ AlcoholDay       &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA, …\n$ AlcoholYear      &lt;int&gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, N…\n$ SmokeNow         &lt;fct&gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA, …\n$ Smoke100         &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No, …\n$ Smoke100n        &lt;fct&gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smoke…\n$ SmokeAge         &lt;int&gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, N…\n$ Marijuana        &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Ye…\n$ AgeFirstMarij    &lt;int&gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, N…\n$ RegularMarij     &lt;fct&gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes,…\n$ AgeRegMarij      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, N…\n$ HardDrugs        &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes, …\n$ SexEver          &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, Y…\n$ SexAge           &lt;int&gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, N…\n$ SexNumPartnLife  &lt;int&gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA, …\n$ SexNumPartYear   &lt;int&gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1,…\n$ SameSex          &lt;fct&gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, N…\n$ SexOrientation   &lt;fct&gt; Heterosexual, Heterosexual, Heterosexual, NA, Heteros…\n$ PregnantNow      &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\nNHANES_adult &lt;-\n  NHANES %&gt;%\n  distinct(ID, .keep_all = TRUE) %&gt;%\n  filter(Age &gt;= 18) %&gt;%\n  select(Height) %&gt;%\n  drop_na(Height)\nNHANES_adult\n\n# A tibble: 4,790 × 1\n   Height\n    &lt;dbl&gt;\n 1   165.\n 2   168.\n 3   167.\n 4   170.\n 5   182.\n 6   169.\n 7   148.\n 8   178.\n 9   181.\n10   170.\n# ℹ 4,780 more rows\n\n\n\npop_mean &lt;- mosaic::mean(~Height, data = NHANES_adult)\n\npop_sd &lt;- mosaic::sd(~Height, data = NHANES_adult)\n\npop_mean\n\n[1] 168.3497\n\npop_sd \n\n[1] 10.15705\n\n\n\nsample_50 &lt;- mosaic::sample(NHANES_adult, size = 50) %&gt;%\n  select(Height)\nsample_50\n\n# A tibble: 50 × 1\n   Height\n    &lt;dbl&gt;\n 1   144.\n 2   158.\n 3   176.\n 4   155.\n 5   163 \n 6   167.\n 7   153.\n 8   151.\n 9   167.\n10   156.\n# ℹ 40 more rows\n\nsample_mean_50 &lt;- mean(~Height, data = sample_50)\nsample_mean_50\n\n[1] 166.84\n\n\n\nsample_50 %&gt;%\n  gf_histogram(~Height, bins = 10) %&gt;%\n  gf_vline(\n    xintercept = ~sample_mean_50,\n    color = \"purple\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_mean,\n    colour = \"black\"\n  ) %&gt;%\n  gf_label(7 ~ (pop_mean + 8),\n    label = \"Population Mean\",\n    color = \"black\"\n  ) %&gt;%\n  gf_label(7 ~ (sample_mean_50 - 8),\n    label = \"Sample Mean\", color = \"purple\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Distribution and Mean of a Single Sample\",\n    subtitle = \"Sample Size = 50\"\n  )\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 50 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 50 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\nsample_50_500 &lt;- do(500) * {\n  sample(NHANES_adult, size = 50) %&gt;%\n    select(Height) %&gt;% # drop sampling related column \"orig.id\"\n    summarise(\n      sample_mean = mean(Height),\n      sample_sd = sd(Height),\n      sample_min = min(Height),\n      sample_max = max(Height)\n    )\n}\nsample_50_500\n\n# A tibble: 500 × 6\n   sample_mean sample_sd sample_min sample_max  .row .index\n         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1        168.      9.73       151.       193.     1      1\n 2        167.      9.95       149        189.     1      2\n 3        168.      9.86       152.       190.     1      3\n 4        169.      9.85       145        196.     1      4\n 5        167.      9.64       149.       191.     1      5\n 6        170.     10.3        146.       193.     1      6\n 7        167.      9.58       152.       189.     1      7\n 8        168.     10.2        146.       186.     1      8\n 9        169.     10.2        149.       190.     1      9\n10        168.     11.3        146.       191.     1     10\n# ℹ 490 more rows\n\ndim(sample_50_500)\n\n[1] 500   6\n\n\n\nsample_50_500 %&gt;%\n  gf_point(.index ~ sample_mean,\n    color = \"purple\",\n    title = \"Sample Means are close to the Population Mean\",\n    subtitle = \"Sample Means are Random!\",\n    caption = \"Grey lines represent our 500 samples\"\n  ) %&gt;%\n  gf_segment(\n    .index + .index ~ sample_min + sample_max,\n    color = \"grey\",\n    linewidth = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_mean,\n    color = \"black\"\n  ) %&gt;%\n  gf_label(-25 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"black\"\n  )\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nIt can be more of less, therefore fore it is fair. i can trust the mean. Therefore a sample mean is a good point to estimate the data.\n\nsample_50_500 %&gt;%\n  gf_point(.index ~ sample_sd,\n    color = \"purple\",\n    title = \"Sample SDs are close to the Population Sd\",\n    subtitle = \"Sample SDs are Random!\",\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_sd,\n    color = \"black\"\n  ) %&gt;%\n  gf_label(-25 ~ pop_sd,\n    label = \"Population SD\",\n    color = \"black\"\n  ) %&gt;%\n  gf_refine(lims(x = c(4, 16)))\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\nsample_50_500 %&gt;%\n  gf_dhistogram(~sample_mean, bins = 30, xlab = \"Height\") %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(0.01 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Sampling Mean Distribution\",\n    subtitle = \"500 means\"\n  )\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n# How does this **distribution of sample-means** compare with the\n# overall distribution of the population?\n#\nsample_50_500 %&gt;%\n  gf_dhistogram(~sample_mean, bins = 30, xlab = \"Height\") %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(0.01 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  ## Add the population histogram\n  gf_histogram(~Height,\n    data = NHANES_adult,\n    alpha = 0.2, fill = \"blue\",\n    bins = 30\n  ) %&gt;%\n  gf_label(0.025 ~ (pop_mean + 20),\n    label = \"Population Distribution\", color = \"blue\"\n  ) %&gt;%\n  gf_labs(title = \"Sampling Mean Distribution\", subtitle = \"Original Population overlay\")\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 500 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\nDeriving the Central Limit Theorem (CLT)\nAs sample lenght increases, the density graph becomes narrower.\nlet’s test this out:\n\nsamples_08_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 08))\n\nsamples_16_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 16))\n\nsamples_32_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\n\nsamples_64_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\n\n# samples_128_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\nI will receive 1000 values for each sample size\nThe plotting of the 1000 values each seperately\n\n# Let us overlay their individual histograms to compare them:\np5 &lt;- gf_dhistogram(~mean,\n  data = samples_08_1000,\n  color = \"grey\",\n  fill = \"dodgerblue\", title = \"N = 8\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean, inherit = FALSE,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-0.025 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n##\np6 &lt;- gf_dhistogram(~mean,\n  data = samples_16_1000,\n  color = \"grey\",\n  fill = \"sienna\", title = \"N = 16\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n##\np7 &lt;- gf_dhistogram(~mean,\n  data = samples_32_1000,\n  na.rm = TRUE,\n  color = \"grey\",\n  fill = \"palegreen\", title = \"N = 32\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\", color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n\np8 &lt;- gf_dhistogram(~mean,\n  data = samples_64_1000,\n  na.rm = TRUE,\n  color = \"grey\",\n  fill = \"violetred\", title = \"N = 64\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\", color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n\n# patchwork::wrap_plots(p5,p6,p7,p8)\np5\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\np6\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\np7\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\np8\n\nWarning in (function (mapping = NULL, data = NULL, stat = \"identity\", position = \"identity\", : All aesthetics have length 1, but the data has 1000 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThe range in the x axis, reduces as the sample size reduces."
  }
]